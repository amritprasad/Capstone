{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree## \n",
    "## Capstone Project: Predicting sign of SPX daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPX Historical Data has 1260 rows and 10 features.\n",
      "A few sample rows are displayed below-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012/12/06</td>\n",
       "      <td>1409.430054</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1405.930054</td>\n",
       "      <td>1413.939941</td>\n",
       "      <td>3229700000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>16.309999</td>\n",
       "      <td>16.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012/12/07</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1420.339966</td>\n",
       "      <td>1410.900024</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>3125160000</td>\n",
       "      <td>16.120001</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>15.730000</td>\n",
       "      <td>15.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012/12/10</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>1421.640015</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>2999430000</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>16.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012/12/11</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1434.270020</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>3650230000</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>15.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012/12/12</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>1438.589966</td>\n",
       "      <td>1426.760010</td>\n",
       "      <td>1428.479980</td>\n",
       "      <td>3709050000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>15.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0  2012/12/06  1409.430054  1413.949951  1405.930054  1413.939941  3229700000   \n",
       "1  2012/12/07  1413.949951  1420.339966  1410.900024  1418.069946  3125160000   \n",
       "2  2012/12/10  1418.069946  1421.640015  1415.640015  1418.550049  2999430000   \n",
       "3  2012/12/11  1418.550049  1434.270020  1418.550049  1427.839966  3650230000   \n",
       "4  2012/12/12  1427.839966  1438.589966  1426.760010  1428.479980  3709050000   \n",
       "\n",
       "    Open_VIX   High_VIX    Low_VIX  Close_VIX  \n",
       "0  16.590000  16.850000  16.309999  16.580000  \n",
       "1  16.120001  16.650000  15.730000  15.900000  \n",
       "2  16.469999  16.469999  15.960000  16.049999  \n",
       "3  15.940000  16.010000  15.420000  15.570000  \n",
       "4  15.600000  16.090000  15.410000  15.950000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SPX Historical data\n",
    "SPX_data_dump = pd.read_csv('SPX Data.csv')\n",
    "print (\"SPX Historical Data has {} rows and {} features.\".format(*SPX_data_dump.shape))\n",
    "print (\"A few sample rows are displayed below-\")\n",
    "display(SPX_data_dump.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data values are valid.\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid values, and correct them\n",
    "colnames_SPX = SPX_data_dump.columns\n",
    "\n",
    "if SPX_data_dump.isnull().values.any():\n",
    "#     If date is invalid, drop the row\n",
    "    SPX_data_dump = SPX_data_dump[SPX_data_dump['Date'].notnull()]\n",
    "    rows, cols = SPX_data_dump.shape\n",
    "#     Fill the average of the past 5 trading days in case value is invalid.\n",
    "    for col in range(2,cols+1):\n",
    "        ndx_invalid = SPX_data_dump.iloc[:,col].isnull()\n",
    "#         If any of the 1st 5 values is invalid, drop them\n",
    "        if ndx_invalid[0:5].any():\n",
    "            SPX_data_dump = SPX_data_dump[np.logical_not(ndx_invalid[0:5])]\n",
    "        elif ndx_invalid.any():\n",
    "            index_list = [i for i,x in enumerate(ndx_invalid) if x]\n",
    "            index_list_prev5 = [i-5 for i,x in enumerate(ndx_invalid) if x]\n",
    "            SPX_data_dump.iloc[index_list, col] = sum(SPX_data_dump.iloc[index_list_prev5, col])/5\n",
    "    print('Invalid data values have been corrected.')\n",
    "else:\n",
    "    print('All the data values are valid.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260 10\n"
     ]
    }
   ],
   "source": [
    "rows, cols = SPX_data_dump.shape\n",
    "print(rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>2012-12-06</td>\n",
       "      <td>1409.430054</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1405.930054</td>\n",
       "      <td>1413.939941</td>\n",
       "      <td>3229700000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>16.309999</td>\n",
       "      <td>16.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2012-12-07</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1420.339966</td>\n",
       "      <td>1410.900024</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>3125160000</td>\n",
       "      <td>16.120001</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>15.730000</td>\n",
       "      <td>15.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>1421.640015</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>2999430000</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>16.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1434.270020</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>3650230000</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>15.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2012-12-12</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>1438.589966</td>\n",
       "      <td>1426.760010</td>\n",
       "      <td>1428.479980</td>\n",
       "      <td>3709050000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>15.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  \\\n",
       "1259 2012-12-06  1409.430054  1413.949951  1405.930054  1413.939941   \n",
       "1258 2012-12-07  1413.949951  1420.339966  1410.900024  1418.069946   \n",
       "1257 2012-12-10  1418.069946  1421.640015  1415.640015  1418.550049   \n",
       "1256 2012-12-11  1418.550049  1434.270020  1418.550049  1427.839966   \n",
       "1255 2012-12-12  1427.839966  1438.589966  1426.760010  1428.479980   \n",
       "\n",
       "      Volume_SPX   Open_VIX   High_VIX    Low_VIX  Close_VIX  \n",
       "1259  3229700000  16.590000  16.850000  16.309999  16.580000  \n",
       "1258  3125160000  16.120001  16.650000  15.730000  15.900000  \n",
       "1257  2999430000  16.469999  16.469999  15.960000  16.049999  \n",
       "1256  3650230000  15.940000  16.010000  15.420000  15.570000  \n",
       "1255  3709050000  15.600000  16.090000  15.410000  15.950000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the dataframe by Date\n",
    "SPX_data_dump['Date'] = pd.to_datetime(SPX_data_dump.Date, format='%Y/%m/%d')\n",
    "SPX_data_dump = SPX_data_dump.sort_values(by='Date', ascending=1)\n",
    "SPX_data_dump.index = np.arange(rows)[::-1]\n",
    "display(SPX_data_dump.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f7ad6d1a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph S&P 500 for all the data present\n",
    "# 0 represents the latest data point, while 1259 is the earliest\n",
    "f, axes = plt.subplots(2, 1)\n",
    "\n",
    "axes[0].axis([rows,0,min(SPX_data_dump['Close_SPX']),max(SPX_data_dump['Close_SPX'])])\n",
    "axes[0].plot(SPX_data_dump['Close_SPX'])\n",
    "axes[0].set_ylabel('SPX Close')\n",
    "axes[0].set_yticks([1200,2000,2800])\n",
    "\n",
    "axes[1].axis([rows,0,min(SPX_data_dump['Close_VIX']),max(SPX_data_dump['Close_VIX'])])\n",
    "axes[1].plot(SPX_data_dump['Close_VIX'])\n",
    "axes[1].set_ylabel('VIX Close')\n",
    "axes[1].set_yticks([0,15,30,45])\n",
    "\n",
    "axes[1].set_xlabel('Trading Days')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>2626.239990</td>\n",
       "      <td>2634.409912</td>\n",
       "      <td>2624.750000</td>\n",
       "      <td>2633.459961</td>\n",
       "      <td>1281124551</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2639.780029</td>\n",
       "      <td>2648.719971</td>\n",
       "      <td>2627.729980</td>\n",
       "      <td>2629.570068</td>\n",
       "      <td>3539040000</td>\n",
       "      <td>11.38</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2639.439941</td>\n",
       "      <td>4023150000</td>\n",
       "      <td>11.05</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.26</td>\n",
       "      <td>11.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2645.100098</td>\n",
       "      <td>2650.620117</td>\n",
       "      <td>2605.520020</td>\n",
       "      <td>2642.219971</td>\n",
       "      <td>3942320000</td>\n",
       "      <td>11.19</td>\n",
       "      <td>14.58</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2657.739990</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>4938490000</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.05</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0 2017-12-06  2626.239990  2634.409912  2624.750000  2633.459961  1281124551   \n",
       "1 2017-12-05  2639.780029  2648.719971  2627.729980  2629.570068  3539040000   \n",
       "2 2017-12-04  2657.189941  2665.189941  2639.030029  2639.439941  4023150000   \n",
       "3 2017-12-01  2645.100098  2650.620117  2605.520020  2642.219971  3942320000   \n",
       "4 2017-11-30  2633.929932  2657.739990  2633.929932  2647.580078  4938490000   \n",
       "\n",
       "   Open_VIX  High_VIX  Low_VIX  Close_VIX  \n",
       "0     11.63     11.68    10.86      11.08  \n",
       "1     11.38     11.67    10.65      11.33  \n",
       "2     11.05     11.86    10.26      11.68  \n",
       "3     11.19     14.58    10.54      11.43  \n",
       "4     10.49     12.05    10.25      11.28  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SPX_data_dump = SPX_data_dump.sort_index()\n",
    "display(SPX_data_dump.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date' 'Open_SPX' 'High_SPX' 'Low_SPX' 'Close_SPX' 'Volume_SPX' 'Open_VIX'\n",
      " 'High_VIX' 'Low_VIX' 'Close_VIX']\n"
     ]
    }
   ],
   "source": [
    "print(SPX_data_dump.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 rows have been dropped due to at least one engineered feature having invalid data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing_10d_Return</th>\n",
       "      <th>Trailing_22d_Return</th>\n",
       "      <th>Trailing_63d_Return</th>\n",
       "      <th>Trailing_252d_Return</th>\n",
       "      <th>Trailing_1d_Return_VIX</th>\n",
       "      <th>Trailing_1d_Max_Move_VIX</th>\n",
       "      <th>Trailing_5d_Return_VIX</th>\n",
       "      <th>Is_MA5_above_MA20</th>\n",
       "      <th>Is_Trailing_1d_Vol_above_MA10_Vol</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>2626.239990</td>\n",
       "      <td>2634.409912</td>\n",
       "      <td>2624.750000</td>\n",
       "      <td>2633.459961</td>\n",
       "      <td>1281124551</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>-0.029966</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.129611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2639.780029</td>\n",
       "      <td>2648.719971</td>\n",
       "      <td>2627.729980</td>\n",
       "      <td>2629.570068</td>\n",
       "      <td>3539040000</td>\n",
       "      <td>11.38</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023495</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.073882</td>\n",
       "      <td>0.204152</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.183384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2639.439941</td>\n",
       "      <td>4023150000</td>\n",
       "      <td>11.05</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.26</td>\n",
       "      <td>11.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.026001</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>0.205898</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2645.100098</td>\n",
       "      <td>2650.620117</td>\n",
       "      <td>2605.520020</td>\n",
       "      <td>2642.219971</td>\n",
       "      <td>3942320000</td>\n",
       "      <td>11.19</td>\n",
       "      <td>14.58</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032348</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>0.071179</td>\n",
       "      <td>0.204097</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.175610</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2657.739990</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>4938490000</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.05</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.068555</td>\n",
       "      <td>0.191145</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.114169</td>\n",
       "      <td>0.099692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0 2017-12-06  2626.239990  2634.409912  2624.750000  2633.459961  1281124551   \n",
       "1 2017-12-05  2639.780029  2648.719971  2627.729980  2629.570068  3539040000   \n",
       "2 2017-12-04  2657.189941  2665.189941  2639.030029  2639.439941  4023150000   \n",
       "3 2017-12-01  2645.100098  2650.620117  2605.520020  2642.219971  3942320000   \n",
       "4 2017-11-30  2633.929932  2657.739990  2633.929932  2647.580078  4938490000   \n",
       "\n",
       "   Open_VIX  High_VIX  Low_VIX  Close_VIX  ...    Trailing_10d_Return  \\\n",
       "0     11.63     11.68    10.86      11.08  ...               0.018369   \n",
       "1     11.38     11.67    10.65      11.33  ...               0.023495   \n",
       "2     11.05     11.86    10.26      11.68  ...               0.021882   \n",
       "3     11.19     14.58    10.54      11.43  ...               0.032348   \n",
       "4     10.49     12.05    10.25      11.28  ...               0.018303   \n",
       "\n",
       "   Trailing_22d_Return  Trailing_63d_Return  Trailing_252d_Return  \\\n",
       "0             0.019272             0.066529              0.192706   \n",
       "1             0.023293             0.073882              0.204152   \n",
       "2             0.026001             0.066895              0.205898   \n",
       "3             0.029054             0.071179              0.204097   \n",
       "4             0.017435             0.068555              0.191145   \n",
       "\n",
       "   Trailing_1d_Return_VIX  Trailing_1d_Max_Move_VIX  Trailing_5d_Return_VIX  \\\n",
       "0               -0.029966                  0.095775                0.129611   \n",
       "1                0.021872                  0.155945                0.183384   \n",
       "2                0.013298                  0.383302                0.182006   \n",
       "3                0.054206                  0.175610                0.141700   \n",
       "4                0.066800                  0.114169                0.099692   \n",
       "\n",
       "   Is_MA5_above_MA20  Is_Trailing_1d_Vol_above_MA10_Vol  Label  \n",
       "0                1.0                                0.0    1.0  \n",
       "1                1.0                                0.0    0.0  \n",
       "2                1.0                                0.0    0.0  \n",
       "3                1.0                                0.0    0.0  \n",
       "4                1.0                                0.0    1.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new dataframe with all the new features\n",
    "\n",
    "# Engineer the returns\n",
    "SPX_data_modified = pd.DataFrame(data=SPX_data_dump)\n",
    "SPX_data_modified['Trailing_1d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-2) - 1\n",
    "SPX_data_modified['Trailing_1d_Max_Move'] = SPX_data_dump['High_SPX'].shift(-1)/SPX_data_dump['Low_SPX'].shift(-1) - 1\n",
    "SPX_data_modified['Trailing_1d_Gap_Return'] = SPX_data_dump['Open_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-2) - 1\n",
    "SPX_data_modified['Trailing_2d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-3) - 1\n",
    "SPX_data_modified['Trailing_3d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-4) - 1\n",
    "SPX_data_modified['Trailing_4d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-5) - 1\n",
    "SPX_data_modified['Trailing_5d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-6) - 1\n",
    "SPX_data_modified['Trailing_10d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-11) - 1\n",
    "SPX_data_modified['Trailing_22d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-23) - 1\n",
    "SPX_data_modified['Trailing_63d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-64) - 1\n",
    "SPX_data_modified['Trailing_252d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-253) - 1\n",
    "SPX_data_modified['Trailing_1d_Return_VIX'] = SPX_data_dump['Close_VIX'].shift(-1)/SPX_data_dump['Close_VIX'].shift(-2) - 1\n",
    "SPX_data_modified['Trailing_1d_Max_Move_VIX'] = SPX_data_dump['High_VIX'].shift(-1)/SPX_data_dump['Low_VIX'].shift(-1) - 1\n",
    "SPX_data_modified['Trailing_5d_Return_VIX'] = SPX_data_dump['Close_VIX'].shift(-1)/SPX_data_dump['Close_VIX'].shift(-6) - 1\n",
    "\n",
    "# Engineer the trend indicators\n",
    "close_index = np.where(SPX_data_dump.columns.values == 'Close_SPX')\n",
    "volume_index = np.where(SPX_data_dump.columns.values == 'Volume_SPX')\n",
    "temp_close_indicator = np.zeros(rows)\n",
    "temp_volume_indicator = np.zeros(rows)\n",
    "\n",
    "for i in np.arange(rows):\n",
    "    if i <= rows - 21:\n",
    "        close_5d_avg = sum(SPX_data_dump.iloc[(i+1):(i+6),int(close_index[0])])/5\n",
    "        close_20d_avg = sum(SPX_data_dump.iloc[(i+1):(i+21),int(close_index[0])])/20\n",
    "        if close_5d_avg > close_20d_avg:\n",
    "            temp_close_indicator[i] = 1\n",
    "        else:\n",
    "            temp_close_indicator[i] = 0\n",
    "    else:\n",
    "        temp_close_indicator[i] = np.nan\n",
    "    \n",
    "    if i <= rows - 11:\n",
    "        vol_10d_avg = sum(SPX_data_dump.iloc[(i+1):(i+11),int(volume_index[0])])/10\n",
    "        if vol_10d_avg > SPX_data_dump.iloc[i+1,int(volume_index[0])]:\n",
    "            temp_volume_indicator[i] = 1\n",
    "        else:\n",
    "            temp_volume_indicator[i] = 0\n",
    "    else:\n",
    "        temp_volume_indicator[i] = np.nan\n",
    "        \n",
    "SPX_data_modified['Is_MA5_above_MA20'] = temp_close_indicator\n",
    "SPX_data_modified['Is_Trailing_1d_Vol_above_MA10_Vol'] = temp_volume_indicator\n",
    "\n",
    "# Create label\n",
    "open_index = np.where(SPX_data_dump.columns.values == 'Open_SPX')\n",
    "a = np.zeros(rows)\n",
    "for i in np.arange(rows):\n",
    "    if SPX_data_dump.iloc[i,int(close_index[0])] > SPX_data_dump.iloc[i,int(open_index[0])]:\n",
    "        a[i] = 1\n",
    "    elif SPX_data_dump.iloc[i,int(close_index[0])] < SPX_data_dump.iloc[i,int(open_index[0])]:\n",
    "        a[i] = 0    \n",
    "    else:\n",
    "        a[i] = np.nan\n",
    "\n",
    "if any(np.isnan(a)):\n",
    "    a_check_ind = [i for i,x in enumerate(np.isnan(a)) if x]\n",
    "    for i in a_check_ind:\n",
    "#         The 5 labels corresponding to the oldest dates don't have enough data points to take an average\n",
    "        if i in np.arange(rows-5,rows,1):\n",
    "            a[i] = np.nan\n",
    "        else:\n",
    "            a[i] = round(sum(a[(i+1):(i+6)])/5)\n",
    "\n",
    "SPX_data_modified['Label'] = a\n",
    "\n",
    "# Drop rows if any of the engineered features is invalid\n",
    "if SPX_data_modified.isnull().values.any():\n",
    "    SPX_data_modified = SPX_data_modified.dropna()\n",
    "    print('{} rows have been dropped due to at least one engineered feature having invalid data.'.format(rows - SPX_data_modified.shape[0]))\n",
    "\n",
    "display(SPX_data_modified.head(n=5))\n",
    "rows_new, cols_new = SPX_data_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>Trailing_1d_Return</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing_10d_Return</th>\n",
       "      <th>Trailing_22d_Return</th>\n",
       "      <th>Trailing_63d_Return</th>\n",
       "      <th>Trailing_252d_Return</th>\n",
       "      <th>Trailing_1d_Return_VIX</th>\n",
       "      <th>Trailing_1d_Max_Move_VIX</th>\n",
       "      <th>Trailing_5d_Return_VIX</th>\n",
       "      <th>Is_MA5_above_MA20</th>\n",
       "      <th>Is_Trailing_1d_Vol_above_MA10_Vol</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1.007000e+03</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.00000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2119.581836</td>\n",
       "      <td>2128.078819</td>\n",
       "      <td>2110.290448</td>\n",
       "      <td>2120.067844</td>\n",
       "      <td>3.578009e+09</td>\n",
       "      <td>14.591470</td>\n",
       "      <td>15.398352</td>\n",
       "      <td>13.88146</td>\n",
       "      <td>14.509851</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.025246</td>\n",
       "      <td>0.109754</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.104367</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.540218</td>\n",
       "      <td>0.528302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>203.564221</td>\n",
       "      <td>201.960388</td>\n",
       "      <td>205.402709</td>\n",
       "      <td>203.788063</td>\n",
       "      <td>7.006715e+08</td>\n",
       "      <td>3.818355</td>\n",
       "      <td>4.362816</td>\n",
       "      <td>3.45737</td>\n",
       "      <td>3.878825</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021532</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.040427</td>\n",
       "      <td>0.085228</td>\n",
       "      <td>0.079916</td>\n",
       "      <td>0.065659</td>\n",
       "      <td>0.182587</td>\n",
       "      <td>0.479795</td>\n",
       "      <td>0.498627</td>\n",
       "      <td>0.499446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1743.819946</td>\n",
       "      <td>1755.790039</td>\n",
       "      <td>1737.920044</td>\n",
       "      <td>1741.890015</td>\n",
       "      <td>1.281125e+09</td>\n",
       "      <td>9.230000</td>\n",
       "      <td>9.520000</td>\n",
       "      <td>8.56000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>-0.039414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103864</td>\n",
       "      <td>-0.103103</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.115759</td>\n",
       "      <td>-0.259057</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>-0.426630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1977.690002</td>\n",
       "      <td>1985.454956</td>\n",
       "      <td>1968.880005</td>\n",
       "      <td>1978.279968</td>\n",
       "      <td>3.164345e+09</td>\n",
       "      <td>12.030000</td>\n",
       "      <td>12.580000</td>\n",
       "      <td>11.55500</td>\n",
       "      <td>11.990000</td>\n",
       "      <td>-0.002768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006465</td>\n",
       "      <td>-0.006519</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>0.046023</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.064586</td>\n",
       "      <td>-0.089208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2083.100098</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2073.649902</td>\n",
       "      <td>2082.419922</td>\n",
       "      <td>3.486910e+09</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>14.350000</td>\n",
       "      <td>13.09000</td>\n",
       "      <td>13.630000</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>0.127883</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>-0.011140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2210.464966</td>\n",
       "      <td>2227.864990</td>\n",
       "      <td>2207.599976</td>\n",
       "      <td>2226.090088</td>\n",
       "      <td>3.896030e+09</td>\n",
       "      <td>15.905000</td>\n",
       "      <td>16.675000</td>\n",
       "      <td>15.12000</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.025598</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.173678</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.121102</td>\n",
       "      <td>0.078661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>7.597450e+09</td>\n",
       "      <td>31.910000</td>\n",
       "      <td>53.290001</td>\n",
       "      <td>29.91000</td>\n",
       "      <td>40.740002</td>\n",
       "      <td>0.039034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075779</td>\n",
       "      <td>0.110842</td>\n",
       "      <td>0.130730</td>\n",
       "      <td>0.312771</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.901177</td>\n",
       "      <td>2.129032</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open_SPX     High_SPX      Low_SPX    Close_SPX    Volume_SPX  \\\n",
       "count  1007.000000  1007.000000  1007.000000  1007.000000  1.007000e+03   \n",
       "mean   2119.581836  2128.078819  2110.290448  2120.067844  3.578009e+09   \n",
       "std     203.564221   201.960388   205.402709   203.788063  7.006715e+08   \n",
       "min    1743.819946  1755.790039  1737.920044  1741.890015  1.281125e+09   \n",
       "25%    1977.690002  1985.454956  1968.880005  1978.279968  3.164345e+09   \n",
       "50%    2083.100098  2093.000000  2073.649902  2082.419922  3.486910e+09   \n",
       "75%    2210.464966  2227.864990  2207.599976  2226.090088  3.896030e+09   \n",
       "max    2657.189941  2665.189941  2639.030029  2647.580078  7.597450e+09   \n",
       "\n",
       "          Open_VIX     High_VIX     Low_VIX    Close_VIX  Trailing_1d_Return  \\\n",
       "count  1007.000000  1007.000000  1007.00000  1007.000000         1007.000000   \n",
       "mean     14.591470    15.398352    13.88146    14.509851            0.000414   \n",
       "std       3.818355     4.362816     3.45737     3.878825            0.007647   \n",
       "min       9.230000     9.520000     8.56000     9.140000           -0.039414   \n",
       "25%      12.030000    12.580000    11.55500    11.990000           -0.002768   \n",
       "50%      13.700000    14.350000    13.09000    13.630000            0.000367   \n",
       "75%      15.905000    16.675000    15.12000    15.770000            0.004441   \n",
       "max      31.910000    53.290001    29.91000    40.740002            0.039034   \n",
       "\n",
       "          ...       Trailing_10d_Return  Trailing_22d_Return  \\\n",
       "count     ...               1007.000000          1007.000000   \n",
       "mean      ...                  0.003976             0.008668   \n",
       "std       ...                  0.021532             0.029552   \n",
       "min       ...                 -0.103864            -0.103103   \n",
       "25%       ...                 -0.006465            -0.006519   \n",
       "50%       ...                  0.004842             0.010867   \n",
       "75%       ...                  0.016340             0.025598   \n",
       "max       ...                  0.075779             0.110842   \n",
       "\n",
       "       Trailing_63d_Return  Trailing_252d_Return  Trailing_1d_Return_VIX  \\\n",
       "count          1007.000000           1007.000000             1007.000000   \n",
       "mean              0.025246              0.109754                0.002730   \n",
       "std               0.040427              0.085228                0.079916   \n",
       "min              -0.121361             -0.115759               -0.259057   \n",
       "25%               0.007828              0.046023               -0.040619   \n",
       "50%               0.030593              0.127883               -0.004528   \n",
       "75%               0.050070              0.173678                0.038286   \n",
       "max               0.130730              0.312771                0.493333   \n",
       "\n",
       "       Trailing_1d_Max_Move_VIX  Trailing_5d_Return_VIX  Is_MA5_above_MA20  \\\n",
       "count               1007.000000             1007.000000        1007.000000   \n",
       "mean                   0.104367                0.012358           0.641509   \n",
       "std                    0.065659                0.182587           0.479795   \n",
       "min                    0.027692               -0.426630           0.000000   \n",
       "25%                    0.064586               -0.089208           0.000000   \n",
       "50%                    0.088867               -0.011140           1.000000   \n",
       "75%                    0.121102                0.078661           1.000000   \n",
       "max                    0.901177                2.129032           1.000000   \n",
       "\n",
       "       Is_Trailing_1d_Vol_above_MA10_Vol        Label  \n",
       "count                        1007.000000  1007.000000  \n",
       "mean                            0.540218     0.528302  \n",
       "std                             0.498627     0.499446  \n",
       "min                             0.000000     0.000000  \n",
       "25%                             0.000000     0.000000  \n",
       "50%                             1.000000     1.000000  \n",
       "75%                             1.000000     1.000000  \n",
       "max                             1.000000     1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe the dataset that would be used for training of models\n",
    "display(SPX_data_modified.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data description shows that SPX closed above its open 52.8% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the dataset with the engineered features\n",
    "SPX_data_modified.to_csv('SPX_Data_New_Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataset with the engineered features needs to be separated clearly into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features are-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing_5d_Return</th>\n",
       "      <th>Trailing_10d_Return</th>\n",
       "      <th>Trailing_22d_Return</th>\n",
       "      <th>Trailing_63d_Return</th>\n",
       "      <th>Trailing_252d_Return</th>\n",
       "      <th>Trailing_1d_Return_VIX</th>\n",
       "      <th>Trailing_1d_Max_Move_VIX</th>\n",
       "      <th>Trailing_5d_Return_VIX</th>\n",
       "      <th>Is_MA5_above_MA20</th>\n",
       "      <th>Is_Trailing_1d_Vol_above_MA10_Vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>2626.239990</td>\n",
       "      <td>2634.409912</td>\n",
       "      <td>2624.750000</td>\n",
       "      <td>2633.459961</td>\n",
       "      <td>1281124551</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>-0.029966</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.129611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2639.780029</td>\n",
       "      <td>2648.719971</td>\n",
       "      <td>2627.729980</td>\n",
       "      <td>2629.570068</td>\n",
       "      <td>3539040000</td>\n",
       "      <td>11.38</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.023495</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.073882</td>\n",
       "      <td>0.204152</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.183384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2639.439941</td>\n",
       "      <td>4023150000</td>\n",
       "      <td>11.05</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.26</td>\n",
       "      <td>11.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015293</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.026001</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>0.205898</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2645.100098</td>\n",
       "      <td>2650.620117</td>\n",
       "      <td>2605.520020</td>\n",
       "      <td>2642.219971</td>\n",
       "      <td>3942320000</td>\n",
       "      <td>11.19</td>\n",
       "      <td>14.58</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.032348</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>0.071179</td>\n",
       "      <td>0.204097</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.175610</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2657.739990</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>4938490000</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.05</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.068555</td>\n",
       "      <td>0.191145</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.114169</td>\n",
       "      <td>0.099692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0 2017-12-06  2626.239990  2634.409912  2624.750000  2633.459961  1281124551   \n",
       "1 2017-12-05  2639.780029  2648.719971  2627.729980  2629.570068  3539040000   \n",
       "2 2017-12-04  2657.189941  2665.189941  2639.030029  2639.439941  4023150000   \n",
       "3 2017-12-01  2645.100098  2650.620117  2605.520020  2642.219971  3942320000   \n",
       "4 2017-11-30  2633.929932  2657.739990  2633.929932  2647.580078  4938490000   \n",
       "\n",
       "   Open_VIX  High_VIX  Low_VIX  Close_VIX                ...                  \\\n",
       "0     11.63     11.68    10.86      11.08                ...                   \n",
       "1     11.38     11.67    10.65      11.33                ...                   \n",
       "2     11.05     11.86    10.26      11.68                ...                   \n",
       "3     11.19     14.58    10.54      11.43                ...                   \n",
       "4     10.49     12.05    10.25      11.28                ...                   \n",
       "\n",
       "   Trailing_5d_Return  Trailing_10d_Return  Trailing_22d_Return  \\\n",
       "0            0.000963             0.018369             0.019272   \n",
       "1            0.014615             0.023495             0.023293   \n",
       "2            0.015293             0.021882             0.026001   \n",
       "3            0.019445             0.032348             0.029054   \n",
       "4            0.010404             0.018303             0.017435   \n",
       "\n",
       "   Trailing_63d_Return  Trailing_252d_Return  Trailing_1d_Return_VIX  \\\n",
       "0             0.066529              0.192706               -0.029966   \n",
       "1             0.073882              0.204152                0.021872   \n",
       "2             0.066895              0.205898                0.013298   \n",
       "3             0.071179              0.204097                0.054206   \n",
       "4             0.068555              0.191145                0.066800   \n",
       "\n",
       "   Trailing_1d_Max_Move_VIX  Trailing_5d_Return_VIX  Is_MA5_above_MA20  \\\n",
       "0                  0.095775                0.129611                1.0   \n",
       "1                  0.155945                0.183384                1.0   \n",
       "2                  0.383302                0.182006                1.0   \n",
       "3                  0.175610                0.141700                1.0   \n",
       "4                  0.114169                0.099692                1.0   \n",
       "\n",
       "   Is_Trailing_1d_Vol_above_MA10_Vol  \n",
       "0                                0.0  \n",
       "1                                0.0  \n",
       "2                                0.0  \n",
       "3                                0.0  \n",
       "4                                0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected labels are-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    1.0\n",
       "Name: Label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = SPX_data_modified.iloc[:,0:(cols_new-1)]\n",
    "Y = SPX_data_modified.iloc[:,(cols_new-1)]\n",
    "print('The features are-')\n",
    "display(X.head(n=5))\n",
    "print('The expected labels are-')\n",
    "display(Y.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The splitting of the labels has been done correctly.\n"
     ]
    }
   ],
   "source": [
    "# Check if the splitting is done correctly\n",
    "a = pd.concat([X,Y], axis=1)\n",
    "if pd.DataFrame.equals(a,SPX_data_modified):\n",
    "    print('The splitting of the labels has been done correctly.')\n",
    "else:\n",
    "    print('The splitting of the labels has NOT been done correctly. Please check again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset needs to be split into training, cross-validation and test sets. Care needs to be taken that look-ahead bias does not creep in while doing this. To ensure this, sequential splitting will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 202 604\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, cross-validation and testing sets\n",
    "\n",
    "# The split is supposed to be done sequentially into training, cross-validation and test sets in the ratio 60%-20%-20%\n",
    "n = round(0.2*rows_new)\n",
    "X_test = X.iloc[0:n,:]\n",
    "Y_test = Y[0:n]\n",
    "\n",
    "X_cv = X.iloc[n:(2*n+1),:]\n",
    "Y_cv = Y[n:(2*n+1)]\n",
    "\n",
    "X_train = X.iloc[(2*n+1):,:]\n",
    "Y_train = Y[(2*n+1):]\n",
    "\n",
    "print(X_test.shape[0], X_cv.shape[0], X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The splitting of the labels has been done correctly.\n",
      "The splitting of the features has been done correctly.\n"
     ]
    }
   ],
   "source": [
    "# Check if the splitting has been done correctly\n",
    "a = pd.concat([Y_test,Y_cv,Y_train])\n",
    "if pd.DataFrame.equals(a,Y):\n",
    "    print('The splitting of the labels has been done correctly.')\n",
    "else:\n",
    "    print('The splitting of the labels has NOT been done correctly. Please check again!')\n",
    "\n",
    "b = pd.concat([X_test,X_cv,X_train])\n",
    "if pd.DataFrame.equals(b,X):\n",
    "    print('The splitting of the features has been done correctly.')\n",
    "else:\n",
    "    print('The splitting of the features has NOT been done correctly. Please check again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark chosen is the buy and hold strategy. That translates to predicting 1 on all the days. The below snippet will copute the accuracy & F1 scores, and the PnL associated with the benchmark. The PnL would be calculated as buying at the open of the oldest date, and selling at the close of the newest date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An XIRR function should be implemented to allow for the comparison of benchmark PnLs against that of the models. The following was taken from https://github.com/peliot/XIRR-and-XNPV/blob/master/financial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000002\n"
     ]
    }
   ],
   "source": [
    "# XIRR function in python\n",
    "import datetime\n",
    "from scipy import optimize\n",
    "\n",
    "def xnpv(rate,cashflows):\n",
    "    chron_order = sorted(cashflows, key = lambda x: x[0])\n",
    "    t0 = pd.to_datetime(chron_order[0][0], format='%Y/%m/%d') #t0 is the date of the first cash flow\n",
    "    return sum([cf/(1+rate)**((pd.to_datetime(t, format='%Y/%m/%d')-t0).days/365.0) for (t,cf) in chron_order])\n",
    "\n",
    "def xirr(cashflows,guess=0.1):\n",
    "    return optimize.newton(lambda r: xnpv(r,cashflows),guess)\n",
    "\n",
    "# Dummy calculations to test the function\n",
    "cashflows = ('2016/12/26',-5),('2016/12/26',-5),('2017/12/26',16)\n",
    "print(xirr(cashflows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all the benchmark calculations below to allow for ease of comparison later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the benchmark on the training set is 0.5265\n",
      "The F1 score of the benchmark on the training set is 0.6898\n",
      "The XIRR of the benchmark on the training set is 5.70%\n",
      "\n",
      "The accuracy score of the benchmark on the cross-validation set is 0.5347\n",
      "The F1 score of the benchmark on the cross-validation set is 0.6968\n",
      "The XIRR of the benchmark on the cross-validation set is 18.77%\n",
      "\n",
      "The accuracy score of the benchmark on the test set is 0.5274\n",
      "The F1 score of the benchmark on the test set is 0.6906\n",
      "The XIRR of the benchmark on the test set is 14.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX')\n",
    "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX')\n",
    "# Show benchmark scores on the different sets against which the models would be compared\n",
    "benchmark_Y_pred_train = np.empty(Y_train.shape[0])\n",
    "benchmark_Y_pred_train.fill(1)\n",
    "cashflows_train = (X_train.iloc[0,0],X_train.iloc[0,int(close_index[0])]),\\\n",
    "                  (X_train.iloc[X_train.shape[0]-1,0],-X_train.iloc[X_train.shape[0]-1,int(open_index[0])])\n",
    "print('The accuracy score of the benchmark on the training set is {:,.4f}'.\\\n",
    "      format(accuracy_score(Y_train, benchmark_Y_pred_train)))\n",
    "print('The F1 score of the benchmark on the training set is {:,.4f}'.format(f1_score(Y_train, benchmark_Y_pred_train)))\n",
    "print('The XIRR of the benchmark on the training set is {:,.2f}%'.format(100*xirr(cashflows_train)))\n",
    "\n",
    "benchmark_Y_pred_cv = np.empty(Y_cv.shape[0])\n",
    "benchmark_Y_pred_cv.fill(1)\n",
    "cashflows_cv = (X_cv.iloc[0,0],X_cv.iloc[0,int(close_index[0])]),\\\n",
    "               (X_cv.iloc[X_cv.shape[0]-1,0],-X_cv.iloc[X_cv.shape[0]-1,int(open_index[0])])\n",
    "print('\\nThe accuracy score of the benchmark on the cross-validation set is {:,.4f}'.\\\n",
    "      format(accuracy_score(Y_cv, benchmark_Y_pred_cv)))\n",
    "print('The F1 score of the benchmark on the cross-validation set is {:,.4f}'.format(f1_score(Y_cv, benchmark_Y_pred_cv)))\n",
    "print('The XIRR of the benchmark on the cross-validation set is {:,.2f}%'.format(100*xirr(cashflows_cv)))\n",
    "\n",
    "benchmark_Y_pred_test = np.empty(Y_test.shape[0])\n",
    "benchmark_Y_pred_test.fill(1)\n",
    "cashflows_test = (X_test.iloc[0,0],X_test.iloc[0,int(close_index[0])]),\\\n",
    "                 (X_test.iloc[X_test.shape[0]-1,0],-X_test.iloc[X_test.shape[0]-1,int(open_index[0])])\n",
    "print('\\nThe accuracy score of the benchmark on the test set is {:,.4f}'.format(accuracy_score(Y_test, benchmark_Y_pred_test)))\n",
    "print('The F1 score of the benchmark on the test set is {:,.4f}'.format(f1_score(Y_test, benchmark_Y_pred_test)))\n",
    "print('The XIRR of the benchmark on the test set is {:,.2f}%'.format(100*xirr(cashflows_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_classifier(clf, X_train, Y_train):\n",
    "    start = time.clock()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    end = time.clock()\n",
    "    print('The classifier took {:,.4f} sec to train'.format(end - start))\n",
    "\n",
    "def classifier_predict(clf, X_train):\n",
    "    start = time.clock()\n",
    "    Y_train_pred = clf.predict(X_train)\n",
    "    end = time.clock()\n",
    "    print('The classifier took {:,.4f} sec to predict'.format(end - start))\n",
    "    \n",
    "    return(Y_train_pred)\n",
    "\n",
    "def score_classifier(clf, X_train, Y_train):\n",
    "    Y_pred = classifier_predict(clf, X_train)\n",
    "    print('The accuracy score is {:,.4f}'.format(accuracy_score(Y_train, Y_pred)))\n",
    "    print('The F1 score is {:,.4f}'.format(f1_score(Y_train, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing to note is that only the engineered features should be used for predicting the labels because the raw features haven't been balanced, and exhibit look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "engineered_features_start_index = int(np.where(SPX_data_modified.columns.values == 'Trailing_1d_Return')[0])\n",
    "print(engineered_features_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "The classifier took 0.0081 sec to train\n",
      "The classifier took 0.0004 sec to predict\n",
      "The accuracy score is 1.0000\n",
      "The F1 score is 1.0000\n",
      "\n",
      "SVM:\n",
      "The classifier took 0.0155 sec to train\n",
      "The classifier took 0.0125 sec to predict\n",
      "The accuracy score is 0.5265\n",
      "The F1 score is 0.6898\n",
      "\n",
      "Random Forest:\n",
      "The classifier took 0.0304 sec to train\n",
      "The classifier took 0.0038 sec to predict\n",
      "The accuracy score is 0.9785\n",
      "The F1 score is 0.9793\n",
      "\n",
      "Extreme Gradient Boosting:\n",
      "The classifier took 0.1081 sec to train\n",
      "The classifier took 0.0027 sec to predict\n",
      "The accuracy score is 0.9255\n",
      "The F1 score is 0.9296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as x_gb\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "xgb = x_gb.XGBClassifier()\n",
    "\n",
    "print('Decision Tree:')\n",
    "train_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "print('\\nSVM:')\n",
    "train_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "train_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "print('\\nExtreme Gradient Boosting:')\n",
    "train_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning of the learners has been handled below one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Depth: 2, Min_Samples_Leaf: 1, Criterion: gini, Splitter: best, F1: 0.6178861788617886\n",
      "Max_Depth: 4, Min_Samples_Leaf: 9, Criterion: gini, Splitter: best, F1: 0.6428571428571429\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=9, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Tune Decision Tree\n",
    "max_depth_list = np.arange(1,11)\n",
    "min_samples_leaf_list = np.arange(1,11)\n",
    "criterion_list = ('gini','entropy')\n",
    "splitter_list = ('best','random')\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "best_dt = dt\n",
    "best_dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_dt_Y_cv_pred = best_dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_dt_F1 = f1_score(Y_cv, best_dt_Y_cv_pred)\n",
    "for max_depth in max_depth_list:\n",
    "    for min_samples_leaf in min_samples_leaf_list:\n",
    "        for criterion in criterion_list:\n",
    "            for splitter in splitter_list:\n",
    "                dt = DecisionTreeClassifier(random_state=42, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "                dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                dt_Y_cv_pred = dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                dt_F1 = f1_score(Y_cv, dt_Y_cv_pred)\n",
    "                if dt_F1 > best_dt_F1:\n",
    "                    best_dt = dt\n",
    "                    best_dt_F1 = dt_F1\n",
    "                    print('Max_Depth: {}, Min_Samples_Leaf: {}, Criterion: {}, Splitter: {}, F1: {}'.\\\n",
    "                          format(max_depth, min_samples_leaf, criterion, splitter, dt_F1))\n",
    "\n",
    "print(best_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the importance of the features used in the best Decision Tree is shown. The most important feature is the Trailing 1d Gap Return, while the least is the Trailing 4d Return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trailing_1d_Gap_Return               0.164906\n",
      "Trailing_1d_Max_Move_VIX             0.156178\n",
      "Trailing_1d_Max_Move                 0.090408\n",
      "Trailing_22d_Return                  0.087375\n",
      "Trailing_1d_Return                   0.071424\n",
      "Trailing_3d_Return                   0.070282\n",
      "Trailing_2d_Return                   0.066062\n",
      "Trailing_63d_Return                  0.065560\n",
      "Trailing_1d_Return_VIX               0.049266\n",
      "Trailing_5d_Return                   0.047273\n",
      "Trailing_10d_Return                  0.044033\n",
      "Trailing_252d_Return                 0.040808\n",
      "Is_MA5_above_MA20                    0.026193\n",
      "Trailing_5d_Return_VIX               0.009177\n",
      "Is_Trailing_1d_Vol_above_MA10_Vol    0.007959\n",
      "Trailing_4d_Return                   0.003097\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_dt = dt\n",
    "best_dt_feature_importance = pd.Series(best_dt.feature_importances_, index=\\\n",
    "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
    "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
    "print(best_dt_feature_importance.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# params = {'max_depth':np.arange(1,11), 'min_samples_leaf':np.arange(1,5), 'criterion':('gini','entropy'), \\\n",
    "#           'splitter':('best','random')}\n",
    "\n",
    "# dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# grid = GridSearchCV(dt, params)\n",
    "# grid.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "# best_dt = grid.best_estimator_\n",
    "# print(best_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM has been tuned below. Care should be taken to not increase the gamma parameter too much, else it'll lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Tune SVM\n",
    "kernel_list = ('linear', 'rbf', 'sigmoid', 'poly')\n",
    "C_list = np.arange(1,16)\n",
    "gamma_list = [1e-7*10**i for i in np.arange(1,7)]\n",
    "degree_list = np.arange(2,6)\n",
    "\n",
    "svm = SVC()\n",
    "best_svm = svm\n",
    "best_svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_svm_Y_cv_pred = best_svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_svm_F1 = f1_score(Y_cv, best_svm_Y_cv_pred)\n",
    "for kernel in kernel_list:\n",
    "    for C in C_list:\n",
    "        for gamma in gamma_list:\n",
    "            for degree in degree_list:\n",
    "                svm = SVC(C=C, gamma=gamma, kernel=kernel, degree=degree)\n",
    "                svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                svm_Y_cv_pred = svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                svm_F1 = f1_score(Y_cv, svm_Y_cv_pred)\n",
    "                if svm_F1 > best_svm_F1:\n",
    "                    best_svm = svm\n",
    "                    best_svm_F1 = svm_F1\n",
    "                    print('Kernel: {}, C: {}, Gamma: {}, Degree: {}, F1: {}'.format(kernel, C, gamma, degree, svm_F1))\n",
    "\n",
    "print(best_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 1, Max_Features: sqrt, F1: 0.6285714285714286\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 20, Max_Features: sqrt, F1: 0.6302521008403361\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 40, Max_Features: sqrt, F1: 0.6475409836065574\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 50, Max_Features: sqrt, F1: 0.6502057613168724\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 90, Max_Features: sqrt, F1: 0.6535433070866142\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: entropy, N_Estimators: 40, Max_Features: None, F1: 0.6639676113360323\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: entropy, N_Estimators: 60, Max_Features: None, F1: 0.6666666666666666\n",
      "Max_Depth: 1, Min_Samples_Leaf: 7, Criterion: entropy, N_Estimators: 60, Max_Features: None, F1: 0.6720647773279352\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=7, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Tune Random Forest\n",
    "n_estimators_list = [1] + [i*10 for i in np.arange(1,11)]\n",
    "criterion_list = ('gini','entropy')\n",
    "max_depth_list = np.arange(1,7)\n",
    "min_samples_leaf_list = np.arange(1, 10, 3)\n",
    "max_features_list = ('sqrt','auto',None)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "best_rf = rf\n",
    "best_rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
    "for max_depth in max_depth_list:\n",
    "    for min_samples_leaf in min_samples_leaf_list:\n",
    "        for criterion in criterion_list:\n",
    "            for n_estimators in n_estimators_list:\n",
    "                for max_features in max_features_list:\n",
    "                    rf = RandomForestClassifier(random_state=42,max_depth=max_depth, min_samples_leaf=min_samples_leaf,\\\n",
    "                                                criterion=criterion, n_estimators=n_estimators, max_features=max_features)\n",
    "                    rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                    rf_Y_cv_pred = rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                    rf_F1 = f1_score(Y_cv, rf_Y_cv_pred)\n",
    "                    if rf_F1 > best_rf_F1:\n",
    "                        best_rf = rf\n",
    "                        best_rf_F1 = rf_F1\n",
    "                        print('Max_Depth: {}, Min_Samples_Leaf: {}, Criterion: {}, N_Estimators: {}, Max_Features: {}, F1: {}'.\\\n",
    "                              format(max_depth, min_samples_leaf, criterion, n_estimators, max_features, rf_F1))\n",
    "\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the importance of the features used in the best Random Forest is shown. The most important feature is the Trailing 1d Return, while the least are 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trailing_1d_Return                   0.183333\n",
      "Trailing_1d_Max_Move                 0.166667\n",
      "Trailing_10d_Return                  0.150000\n",
      "Trailing_63d_Return                  0.100000\n",
      "Trailing_22d_Return                  0.100000\n",
      "Trailing_252d_Return                 0.083333\n",
      "Trailing_1d_Gap_Return               0.083333\n",
      "Trailing_1d_Max_Move_VIX             0.050000\n",
      "Trailing_1d_Return_VIX               0.033333\n",
      "Trailing_2d_Return                   0.033333\n",
      "Trailing_5d_Return                   0.016667\n",
      "Is_Trailing_1d_Vol_above_MA10_Vol    0.000000\n",
      "Is_MA5_above_MA20                    0.000000\n",
      "Trailing_5d_Return_VIX               0.000000\n",
      "Trailing_4d_Return                   0.000000\n",
      "Trailing_3d_Return                   0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_rf_feature_importance = pd.Series(best_rf.feature_importances_, index=\\\n",
    "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
    "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
    "print(best_rf_feature_importance.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 1, N_Estimators: 1, L1 Factor: 0, Gamma: 0, F1: 0.6967741935483871\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=1, missing=None, n_estimators=1, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=0,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "n_estimators_list = [1, 10, 30, 50 ,100] + [i*250 for i in np.arange(1, 5)]\n",
    "max_depth_list = np.arange(1,11)\n",
    "reg_lambda_list = [0, 0.1, 0.5, 1, 2, 5]\n",
    "gamma_list = [0] + [10**i for i in np.arange(0,4)]\n",
    "\n",
    "xgb = x_gb.XGBClassifier()\n",
    "best_xgb = xgb\n",
    "best_xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_xgb_Y_cv_pred = xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_xgb_F1 = f1_score(Y_cv, best_xgb_Y_cv_pred)\n",
    "for max_depth in max_depth_list:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for reg_lambda in reg_lambda_list:\n",
    "            for gamma in gamma_list:\n",
    "                xgb = x_gb.XGBClassifier(max_depth=max_depth, n_estimators = n_estimators, reg_lambda=reg_lambda, gamma=gamma)\n",
    "                xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                xgb_Y_cv_pred = xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                xgb_F1 = f1_score(Y_cv, xgb_Y_cv_pred)\n",
    "                if xgb_F1 > best_xgb_F1:\n",
    "                    best_xgb = xgb\n",
    "                    best_xgb_F1 = xgb_F1\n",
    "                    print('Max Depth: {}, N_Estimators: {}, L1 Factor: {}, Gamma: {}, F1: {}'.\\\n",
    "                           format(max_depth, n_estimators, reg_lambda, gamma, xgb_F1))\n",
    "\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune Gradient Boosting\n",
    "# loss_list = ('deviance', 'exponential')\n",
    "# n_estimators_list = [1, 10, 30, 50 ,100] + [i*250 for i in np.arange(1, 5)]\n",
    "# criterion_list = ('friedman_mse','mae')\n",
    "# max_depth_list = np.arange(1,6)\n",
    "# min_samples_leaf_list = np.arange(1,4)\n",
    "\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "# best_gb = gb\n",
    "# best_gb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "# best_gb_Y_cv_pred = best_gb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "# best_gb_F1 = f1_score(Y_cv, best_gb_Y_cv_pred)\n",
    "# for max_depth in max_depth_list:\n",
    "#     for min_samples_leaf in min_samples_leaf_list:\n",
    "#         for criterion in criterion_list:\n",
    "#             for n_estimators in n_estimators_list:\n",
    "#                 for loss in loss_list:\n",
    "#                     gb = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion=criterion,\\\n",
    "#                                                     n_estimators=n_estimators, loss=loss)\n",
    "#                     gb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "#                     gb_Y_cv_pred = gb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "#                     gb_F1 = f1_score(Y_cv, gb_Y_cv_pred)\n",
    "#                     if gb_F1 > best_gb_F1:\n",
    "#                         best_gb = gb\n",
    "#                         best_gb_F1 = gb_F1\n",
    "#                         print('Max_Depth: {}, Min_Samples_Leaf: {}, Criterion: {}, N_Estimators: {}, Loss: {}, F1: {}'.\\\n",
    "#                               format(max_depth, min_samples_leaf, criterion, n_estimators, loss, gb_F1))\n",
    "\n",
    "# print(best_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the best out of the tuned models on the basis of the cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate XIRR of the best models\n",
    "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX')\n",
    "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX')\n",
    "date_index = np.where(SPX_data_modified.columns.values == 'Date')\n",
    "\n",
    "def convert_label_to_cash_sign(label):\n",
    "    if label == 1:\n",
    "        cash_sign = -1\n",
    "    else:\n",
    "        cash_sign = 1\n",
    "    \n",
    "    return(cash_sign)\n",
    "\n",
    "def get_cashflows(best_clf, X):\n",
    "    Y_pred = best_clf.predict(X.iloc[:,engineered_features_start_index:X.shape[1]])\n",
    "    # A prediction of 1 should signal buying that day, which can be done at the index open price. Similarly, 0 should signal\n",
    "    # selling at the open. The position should be kept until the prediction label changes. At the change, sell/buy accordingly.\n",
    "    # The net position should be either 1 unit long/short or squared off.\n",
    "    length = len(Y_pred)\n",
    "    prev_cash_sign = convert_label_to_cash_sign(Y_pred[length-1])\n",
    "    net_position = -prev_cash_sign\n",
    "    cashflows = ((X.iloc[length-1, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                  X.iloc[length-1, int(open_index[0])]*prev_cash_sign),)\n",
    "\n",
    "    for i in np.arange(length-2, 1, -1):\n",
    "        cur_cash_sign = convert_label_to_cash_sign(Y_pred[i])\n",
    "        if cur_cash_sign != prev_cash_sign or net_position == 0:\n",
    "            net_position = net_position - cur_cash_sign\n",
    "            cashflows = (cashflows) + ((X.iloc[i, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                         X.iloc[i, int(open_index[0])]*cur_cash_sign),)\n",
    "        prev_cash_sign = cur_cash_sign\n",
    "\n",
    "    cur_cash_sign = convert_label_to_cash_sign(Y_pred[0])\n",
    "    # On the last day, if net position is 0, execute at the open and square off on that day's close. If net position isn't 0,\n",
    "    # then check if the label is different from last day. If yes, then square off at open and stop, else square off at close and\n",
    "    # stop\n",
    "    if net_position == 0:\n",
    "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),) + \\\n",
    "                                ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 -X.iloc[0, int(close_index[0])]*cur_cash_sign),)\n",
    "    elif cur_cash_sign != prev_cash_sign:\n",
    "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),)\n",
    "        net_position = net_position - cur_cash_sign\n",
    "    else:\n",
    "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 -X.iloc[0, int(close_index[0])]*prev_cash_sign),)\n",
    "        net_position = net_position + prev_cash_sign\n",
    "    \n",
    "    return(cashflows)\n",
    "\n",
    "best_dt_cv_cashflows = get_cashflows(best_dt, X_cv)\n",
    "best_svm_cv_cashflows = get_cashflows(best_svm, X_cv)\n",
    "best_rf_cv_cashflows = get_cashflows(best_rf, X_cv)\n",
    "best_xgb_cv_cashflows = get_cashflows(best_xgb, X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree:\n",
      "The classifier took 0.0048 sec to train\n",
      "The classifier took 0.0003 sec to predict\n",
      "The accuracy score is 0.5545\n",
      "The F1 score is 0.6429\n",
      "The XIRR is 1.10%\n",
      "\n",
      "Best SVM:\n",
      "The classifier took 0.0222 sec to train\n",
      "The classifier took 0.0068 sec to predict\n",
      "The accuracy score is 0.5347\n",
      "The F1 score is 0.6968\n",
      "The XIRR is 18.77%\n",
      "\n",
      "Best Random Forest:\n",
      "The classifier took 0.1944 sec to train\n",
      "The classifier took 0.0078 sec to predict\n",
      "The accuracy score is 0.5990\n",
      "The F1 score is 0.6721\n",
      "The XIRR is 62.86%\n",
      "\n",
      "Best Extreme Gradient Boosting:\n",
      "The classifier took 0.0071 sec to train\n",
      "The classifier took 0.0018 sec to predict\n",
      "The accuracy score is 0.5347\n",
      "The F1 score is 0.6968\n",
      "The XIRR is 18.77%\n"
     ]
    }
   ],
   "source": [
    "print('Best Decision Tree:')\n",
    "train_classifier(best_dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_dt, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_dt_cv_cashflows)))\n",
    "\n",
    "print('\\nBest SVM:')\n",
    "train_classifier(best_svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_svm, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_svm_cv_cashflows)))\n",
    "\n",
    "print('\\nBest Random Forest:')\n",
    "train_classifier(best_rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_rf, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_rf_cv_cashflows)))\n",
    "\n",
    "print('\\nBest Extreme Gradient Boosting:')\n",
    "train_classifier(best_xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_xgb, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_xgb_cv_cashflows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the tuned models perform better than the benchmark on the F1 metric in the cross-validation set. On accuracy and XIRR, Decision Tree and Random Forest are better. Random Forest is the best out of the two. It would be good to see how it performs on the test set after being trained on the cross-validation + training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier took 0.2100 sec to train\n",
      "The classifier took 0.0076 sec to predict\n",
      "The accuracy score is 0.5323\n",
      "The F1 score is 0.5648\n",
      "The XIRR is 90.99%\n"
     ]
    }
   ],
   "source": [
    "X_train_new = pd.concat([X_cv, X_train])\n",
    "Y_train_new = pd.concat([Y_cv, Y_train])\n",
    "\n",
    "best_rf_test_cashflows = get_cashflows(best_rf, X_test)\n",
    "train_classifier(best_rf, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
    "score_classifier(best_rf, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_rf_test_cashflows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest again falls short on the F1 score! It's XIRR remains spectacular though. I would be very cautious while using this model to trade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
