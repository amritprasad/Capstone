{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree## \n",
    "## Capstone Project: Predicting sign of SPX daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPX Historical Data has 1260 rows and 10 features.\n",
      "A few sample rows are displayed below-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012/12/06</td>\n",
       "      <td>1409.430054</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1405.930054</td>\n",
       "      <td>1413.939941</td>\n",
       "      <td>3229700000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>16.309999</td>\n",
       "      <td>16.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012/12/07</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1420.339966</td>\n",
       "      <td>1410.900024</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>3125160000</td>\n",
       "      <td>16.120001</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>15.730000</td>\n",
       "      <td>15.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012/12/10</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>1421.640015</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>2999430000</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>16.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012/12/11</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1434.270020</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>3650230000</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>15.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012/12/12</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>1438.589966</td>\n",
       "      <td>1426.760010</td>\n",
       "      <td>1428.479980</td>\n",
       "      <td>3709050000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>15.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0  2012/12/06  1409.430054  1413.949951  1405.930054  1413.939941  3229700000   \n",
       "1  2012/12/07  1413.949951  1420.339966  1410.900024  1418.069946  3125160000   \n",
       "2  2012/12/10  1418.069946  1421.640015  1415.640015  1418.550049  2999430000   \n",
       "3  2012/12/11  1418.550049  1434.270020  1418.550049  1427.839966  3650230000   \n",
       "4  2012/12/12  1427.839966  1438.589966  1426.760010  1428.479980  3709050000   \n",
       "\n",
       "    Open_VIX   High_VIX    Low_VIX  Close_VIX  \n",
       "0  16.590000  16.850000  16.309999  16.580000  \n",
       "1  16.120001  16.650000  15.730000  15.900000  \n",
       "2  16.469999  16.469999  15.960000  16.049999  \n",
       "3  15.940000  16.010000  15.420000  15.570000  \n",
       "4  15.600000  16.090000  15.410000  15.950000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SPX Historical data\n",
    "SPX_data_dump = pd.read_csv('SPX Data.csv')\n",
    "print (\"SPX Historical Data has {} rows and {} features.\".format(*SPX_data_dump.shape))\n",
    "print (\"A few sample rows are displayed below-\")\n",
    "display(SPX_data_dump.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data values are valid.\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid values, and correct them\n",
    "colnames_SPX = SPX_data_dump.columns\n",
    "\n",
    "if SPX_data_dump.isnull().values.any():\n",
    "#     If date is invalid, drop the row\n",
    "    SPX_data_dump = SPX_data_dump[SPX_data_dump['Date'].notnull()]\n",
    "    rows, cols = SPX_data_dump.shape\n",
    "#     Fill the average of the past 5 trading days in case value is invalid.\n",
    "    for col in range(2,cols+1):\n",
    "        ndx_invalid = SPX_data_dump.iloc[:,col].isnull()\n",
    "#         If any of the 1st 5 values is invalid, drop them\n",
    "        if ndx_invalid[0:5].any():\n",
    "            SPX_data_dump = SPX_data_dump[np.logical_not(ndx_invalid[0:5])]\n",
    "        elif ndx_invalid.any():\n",
    "            index_list = [i for i,x in enumerate(ndx_invalid) if x]\n",
    "            index_list_prev5 = [i-5 for i,x in enumerate(ndx_invalid) if x]\n",
    "            SPX_data_dump.iloc[index_list, col] = sum(SPX_data_dump.iloc[index_list_prev5, col])/5\n",
    "    print('Invalid data values have been corrected.')\n",
    "else:\n",
    "    print('All the data values are valid.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260 10\n"
     ]
    }
   ],
   "source": [
    "rows, cols = SPX_data_dump.shape\n",
    "print(rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>2012-12-06</td>\n",
       "      <td>1409.430054</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1405.930054</td>\n",
       "      <td>1413.939941</td>\n",
       "      <td>3229700000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>16.309999</td>\n",
       "      <td>16.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2012-12-07</td>\n",
       "      <td>1413.949951</td>\n",
       "      <td>1420.339966</td>\n",
       "      <td>1410.900024</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>3125160000</td>\n",
       "      <td>16.120001</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>15.730000</td>\n",
       "      <td>15.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>1418.069946</td>\n",
       "      <td>1421.640015</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>2999430000</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>16.469999</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>16.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1434.270020</td>\n",
       "      <td>1418.550049</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>3650230000</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>15.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2012-12-12</td>\n",
       "      <td>1427.839966</td>\n",
       "      <td>1438.589966</td>\n",
       "      <td>1426.760010</td>\n",
       "      <td>1428.479980</td>\n",
       "      <td>3709050000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>15.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  \\\n",
       "1259 2012-12-06  1409.430054  1413.949951  1405.930054  1413.939941   \n",
       "1258 2012-12-07  1413.949951  1420.339966  1410.900024  1418.069946   \n",
       "1257 2012-12-10  1418.069946  1421.640015  1415.640015  1418.550049   \n",
       "1256 2012-12-11  1418.550049  1434.270020  1418.550049  1427.839966   \n",
       "1255 2012-12-12  1427.839966  1438.589966  1426.760010  1428.479980   \n",
       "\n",
       "      Volume_SPX   Open_VIX   High_VIX    Low_VIX  Close_VIX  \n",
       "1259  3229700000  16.590000  16.850000  16.309999  16.580000  \n",
       "1258  3125160000  16.120001  16.650000  15.730000  15.900000  \n",
       "1257  2999430000  16.469999  16.469999  15.960000  16.049999  \n",
       "1256  3650230000  15.940000  16.010000  15.420000  15.570000  \n",
       "1255  3709050000  15.600000  16.090000  15.410000  15.950000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the dataframe by Date\n",
    "SPX_data_dump['Date'] = pd.to_datetime(SPX_data_dump.Date, format='%Y/%m/%d')\n",
    "SPX_data_dump = SPX_data_dump.sort_values(by='Date', ascending=1)\n",
    "SPX_data_dump.index = np.arange(rows)[::-1]\n",
    "display(SPX_data_dump.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ddf0837e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph S&P 500 for all the data present\n",
    "# 0 represents the latest data point, while 1259 is the earliest\n",
    "f, axes = plt.subplots(2, 1)\n",
    "\n",
    "axes[0].axis([rows,0,min(SPX_data_dump['Close_SPX']),max(SPX_data_dump['Close_SPX'])])\n",
    "axes[0].plot(SPX_data_dump['Close_SPX'])\n",
    "axes[0].set_ylabel('SPX Close')\n",
    "axes[0].set_yticks([1200,2000,2800])\n",
    "\n",
    "axes[1].axis([rows,0,min(SPX_data_dump['Close_VIX']),max(SPX_data_dump['Close_VIX'])])\n",
    "axes[1].plot(SPX_data_dump['Close_VIX'])\n",
    "axes[1].set_ylabel('VIX Close')\n",
    "axes[1].set_yticks([0,15,30,45])\n",
    "\n",
    "axes[1].set_xlabel('Trading Days')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>2626.239990</td>\n",
       "      <td>2634.409912</td>\n",
       "      <td>2624.750000</td>\n",
       "      <td>2633.459961</td>\n",
       "      <td>1281124551</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2639.780029</td>\n",
       "      <td>2648.719971</td>\n",
       "      <td>2627.729980</td>\n",
       "      <td>2629.570068</td>\n",
       "      <td>3539040000</td>\n",
       "      <td>11.38</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2639.439941</td>\n",
       "      <td>4023150000</td>\n",
       "      <td>11.05</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.26</td>\n",
       "      <td>11.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2645.100098</td>\n",
       "      <td>2650.620117</td>\n",
       "      <td>2605.520020</td>\n",
       "      <td>2642.219971</td>\n",
       "      <td>3942320000</td>\n",
       "      <td>11.19</td>\n",
       "      <td>14.58</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2657.739990</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>4938490000</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.05</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0 2017-12-06  2626.239990  2634.409912  2624.750000  2633.459961  1281124551   \n",
       "1 2017-12-05  2639.780029  2648.719971  2627.729980  2629.570068  3539040000   \n",
       "2 2017-12-04  2657.189941  2665.189941  2639.030029  2639.439941  4023150000   \n",
       "3 2017-12-01  2645.100098  2650.620117  2605.520020  2642.219971  3942320000   \n",
       "4 2017-11-30  2633.929932  2657.739990  2633.929932  2647.580078  4938490000   \n",
       "\n",
       "   Open_VIX  High_VIX  Low_VIX  Close_VIX  \n",
       "0     11.63     11.68    10.86      11.08  \n",
       "1     11.38     11.67    10.65      11.33  \n",
       "2     11.05     11.86    10.26      11.68  \n",
       "3     11.19     14.58    10.54      11.43  \n",
       "4     10.49     12.05    10.25      11.28  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SPX_data_dump = SPX_data_dump.sort_index()\n",
    "display(SPX_data_dump.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date' 'Open_SPX' 'High_SPX' 'Low_SPX' 'Close_SPX' 'Volume_SPX' 'Open_VIX'\n",
      " 'High_VIX' 'Low_VIX' 'Close_VIX']\n"
     ]
    }
   ],
   "source": [
    "print(SPX_data_dump.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 rows have been dropped due to at least one engineered feature having invalid data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing_10d_Return</th>\n",
       "      <th>Trailing_22d_Return</th>\n",
       "      <th>Trailing_63d_Return</th>\n",
       "      <th>Trailing_252d_Return</th>\n",
       "      <th>Trailing_1d_Return_VIX</th>\n",
       "      <th>Trailing_1d_Max_Move_VIX</th>\n",
       "      <th>Trailing_5d_Return_VIX</th>\n",
       "      <th>Is_MA5_above_MA20</th>\n",
       "      <th>Is_Trailing_1d_Vol_above_MA10_Vol</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>2626.239990</td>\n",
       "      <td>2634.409912</td>\n",
       "      <td>2624.750000</td>\n",
       "      <td>2633.459961</td>\n",
       "      <td>1281124551</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>-0.029966</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.129611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2639.780029</td>\n",
       "      <td>2648.719971</td>\n",
       "      <td>2627.729980</td>\n",
       "      <td>2629.570068</td>\n",
       "      <td>3539040000</td>\n",
       "      <td>11.38</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023495</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.073882</td>\n",
       "      <td>0.204152</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.183384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2639.439941</td>\n",
       "      <td>4023150000</td>\n",
       "      <td>11.05</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.26</td>\n",
       "      <td>11.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.026001</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>0.205898</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2645.100098</td>\n",
       "      <td>2650.620117</td>\n",
       "      <td>2605.520020</td>\n",
       "      <td>2642.219971</td>\n",
       "      <td>3942320000</td>\n",
       "      <td>11.19</td>\n",
       "      <td>14.58</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032348</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>0.071179</td>\n",
       "      <td>0.204097</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.175610</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2657.739990</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>4938490000</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.05</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.068555</td>\n",
       "      <td>0.191145</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.114169</td>\n",
       "      <td>0.099692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0 2017-12-06  2626.239990  2634.409912  2624.750000  2633.459961  1281124551   \n",
       "1 2017-12-05  2639.780029  2648.719971  2627.729980  2629.570068  3539040000   \n",
       "2 2017-12-04  2657.189941  2665.189941  2639.030029  2639.439941  4023150000   \n",
       "3 2017-12-01  2645.100098  2650.620117  2605.520020  2642.219971  3942320000   \n",
       "4 2017-11-30  2633.929932  2657.739990  2633.929932  2647.580078  4938490000   \n",
       "\n",
       "   Open_VIX  High_VIX  Low_VIX  Close_VIX  ...    Trailing_10d_Return  \\\n",
       "0     11.63     11.68    10.86      11.08  ...               0.018369   \n",
       "1     11.38     11.67    10.65      11.33  ...               0.023495   \n",
       "2     11.05     11.86    10.26      11.68  ...               0.021882   \n",
       "3     11.19     14.58    10.54      11.43  ...               0.032348   \n",
       "4     10.49     12.05    10.25      11.28  ...               0.018303   \n",
       "\n",
       "   Trailing_22d_Return  Trailing_63d_Return  Trailing_252d_Return  \\\n",
       "0             0.019272             0.066529              0.192706   \n",
       "1             0.023293             0.073882              0.204152   \n",
       "2             0.026001             0.066895              0.205898   \n",
       "3             0.029054             0.071179              0.204097   \n",
       "4             0.017435             0.068555              0.191145   \n",
       "\n",
       "   Trailing_1d_Return_VIX  Trailing_1d_Max_Move_VIX  Trailing_5d_Return_VIX  \\\n",
       "0               -0.029966                  0.095775                0.129611   \n",
       "1                0.021872                  0.155945                0.183384   \n",
       "2                0.013298                  0.383302                0.182006   \n",
       "3                0.054206                  0.175610                0.141700   \n",
       "4                0.066800                  0.114169                0.099692   \n",
       "\n",
       "   Is_MA5_above_MA20  Is_Trailing_1d_Vol_above_MA10_Vol  Label  \n",
       "0                1.0                                0.0    1.0  \n",
       "1                1.0                                0.0    0.0  \n",
       "2                1.0                                0.0    0.0  \n",
       "3                1.0                                0.0    0.0  \n",
       "4                1.0                                0.0    1.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new dataframe with all the new features\n",
    "\n",
    "# Engineer the returns\n",
    "SPX_data_modified = pd.DataFrame(data=SPX_data_dump)\n",
    "SPX_data_modified['Trailing_1d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-2) - 1\n",
    "SPX_data_modified['Trailing_1d_Max_Move'] = SPX_data_dump['High_SPX'].shift(-1)/SPX_data_dump['Low_SPX'].shift(-1) - 1\n",
    "SPX_data_modified['Trailing_1d_Gap_Return'] = SPX_data_dump['Open_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-2) - 1\n",
    "SPX_data_modified['Trailing_2d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-3) - 1\n",
    "SPX_data_modified['Trailing_3d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-4) - 1\n",
    "SPX_data_modified['Trailing_4d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-5) - 1\n",
    "SPX_data_modified['Trailing_5d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-6) - 1\n",
    "SPX_data_modified['Trailing_10d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-11) - 1\n",
    "SPX_data_modified['Trailing_22d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-23) - 1\n",
    "SPX_data_modified['Trailing_63d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-64) - 1\n",
    "SPX_data_modified['Trailing_252d_Return'] = SPX_data_dump['Close_SPX'].shift(-1)/SPX_data_dump['Close_SPX'].shift(-253) - 1\n",
    "SPX_data_modified['Trailing_1d_Return_VIX'] = SPX_data_dump['Close_VIX'].shift(-1)/SPX_data_dump['Close_VIX'].shift(-2) - 1\n",
    "SPX_data_modified['Trailing_1d_Max_Move_VIX'] = SPX_data_dump['High_VIX'].shift(-1)/SPX_data_dump['Low_VIX'].shift(-1) - 1\n",
    "SPX_data_modified['Trailing_5d_Return_VIX'] = SPX_data_dump['Close_VIX'].shift(-1)/SPX_data_dump['Close_VIX'].shift(-6) - 1\n",
    "\n",
    "# Engineer the trend indicators\n",
    "close_index = np.where(SPX_data_dump.columns.values == 'Close_SPX')\n",
    "volume_index = np.where(SPX_data_dump.columns.values == 'Volume_SPX')\n",
    "temp_close_indicator = np.zeros(rows)\n",
    "temp_volume_indicator = np.zeros(rows)\n",
    "\n",
    "for i in np.arange(rows):\n",
    "    if i <= rows - 21:\n",
    "        close_5d_avg = sum(SPX_data_dump.iloc[(i+1):(i+6),int(close_index[0])])/5\n",
    "        close_20d_avg = sum(SPX_data_dump.iloc[(i+1):(i+21),int(close_index[0])])/20\n",
    "        if close_5d_avg > close_20d_avg:\n",
    "            temp_close_indicator[i] = 1\n",
    "        else:\n",
    "            temp_close_indicator[i] = 0\n",
    "    else:\n",
    "        temp_close_indicator[i] = np.nan\n",
    "    \n",
    "    if i <= rows - 11:\n",
    "        vol_10d_avg = sum(SPX_data_dump.iloc[(i+1):(i+11),int(volume_index[0])])/10\n",
    "        if vol_10d_avg > SPX_data_dump.iloc[i+1,int(volume_index[0])]:\n",
    "            temp_volume_indicator[i] = 1\n",
    "        else:\n",
    "            temp_volume_indicator[i] = 0\n",
    "    else:\n",
    "        temp_volume_indicator[i] = np.nan\n",
    "        \n",
    "SPX_data_modified['Is_MA5_above_MA20'] = temp_close_indicator\n",
    "SPX_data_modified['Is_Trailing_1d_Vol_above_MA10_Vol'] = temp_volume_indicator\n",
    "\n",
    "# Create label\n",
    "open_index = np.where(SPX_data_dump.columns.values == 'Open_SPX')\n",
    "a = np.zeros(rows)\n",
    "for i in np.arange(rows):\n",
    "    if SPX_data_dump.iloc[i,int(close_index[0])] > SPX_data_dump.iloc[i,int(open_index[0])]:\n",
    "        a[i] = 1\n",
    "    elif SPX_data_dump.iloc[i,int(close_index[0])] < SPX_data_dump.iloc[i,int(open_index[0])]:\n",
    "        a[i] = 0    \n",
    "    else:\n",
    "        a[i] = np.nan\n",
    "\n",
    "if any(np.isnan(a)):\n",
    "    a_check_ind = [i for i,x in enumerate(np.isnan(a)) if x]\n",
    "    for i in a_check_ind:\n",
    "#         The 5 labels corresponding to the oldest dates don't have enough data points to take an average\n",
    "        if i in np.arange(rows-5,rows,1):\n",
    "            a[i] = np.nan\n",
    "        else:\n",
    "            a[i] = round(sum(a[(i+1):(i+6)])/5)\n",
    "\n",
    "SPX_data_modified['Label'] = a\n",
    "\n",
    "# Drop rows if any of the engineered features is invalid\n",
    "if SPX_data_modified.isnull().values.any():\n",
    "    SPX_data_modified = SPX_data_modified.dropna()\n",
    "    print('{} rows have been dropped due to at least one engineered feature having invalid data.'.\\\n",
    "          format(rows - SPX_data_modified.shape[0]))\n",
    "\n",
    "display(SPX_data_modified.head(n=5))\n",
    "rows_new, cols_new = SPX_data_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 out of 1007 data points are labelled 1.\n"
     ]
    }
   ],
   "source": [
    "print('{} out of {} data points are labelled 1.'.format(sum(SPX_data_modified['Label'] == 1), len(SPX_data_modified['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>Trailing_1d_Return</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing_10d_Return</th>\n",
       "      <th>Trailing_22d_Return</th>\n",
       "      <th>Trailing_63d_Return</th>\n",
       "      <th>Trailing_252d_Return</th>\n",
       "      <th>Trailing_1d_Return_VIX</th>\n",
       "      <th>Trailing_1d_Max_Move_VIX</th>\n",
       "      <th>Trailing_5d_Return_VIX</th>\n",
       "      <th>Is_MA5_above_MA20</th>\n",
       "      <th>Is_Trailing_1d_Vol_above_MA10_Vol</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1.007000e+03</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.00000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2119.581836</td>\n",
       "      <td>2128.078819</td>\n",
       "      <td>2110.290448</td>\n",
       "      <td>2120.067844</td>\n",
       "      <td>3.578009e+09</td>\n",
       "      <td>14.591470</td>\n",
       "      <td>15.398352</td>\n",
       "      <td>13.88146</td>\n",
       "      <td>14.509851</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.025246</td>\n",
       "      <td>0.109754</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.104367</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.540218</td>\n",
       "      <td>0.528302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>203.564221</td>\n",
       "      <td>201.960388</td>\n",
       "      <td>205.402709</td>\n",
       "      <td>203.788063</td>\n",
       "      <td>7.006715e+08</td>\n",
       "      <td>3.818355</td>\n",
       "      <td>4.362816</td>\n",
       "      <td>3.45737</td>\n",
       "      <td>3.878825</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021532</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.040427</td>\n",
       "      <td>0.085228</td>\n",
       "      <td>0.079916</td>\n",
       "      <td>0.065659</td>\n",
       "      <td>0.182587</td>\n",
       "      <td>0.479795</td>\n",
       "      <td>0.498627</td>\n",
       "      <td>0.499446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1743.819946</td>\n",
       "      <td>1755.790039</td>\n",
       "      <td>1737.920044</td>\n",
       "      <td>1741.890015</td>\n",
       "      <td>1.281125e+09</td>\n",
       "      <td>9.230000</td>\n",
       "      <td>9.520000</td>\n",
       "      <td>8.56000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>-0.039414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103864</td>\n",
       "      <td>-0.103103</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.115759</td>\n",
       "      <td>-0.259057</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>-0.426630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1977.690002</td>\n",
       "      <td>1985.454956</td>\n",
       "      <td>1968.880005</td>\n",
       "      <td>1978.279968</td>\n",
       "      <td>3.164345e+09</td>\n",
       "      <td>12.030000</td>\n",
       "      <td>12.580000</td>\n",
       "      <td>11.55500</td>\n",
       "      <td>11.990000</td>\n",
       "      <td>-0.002768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006465</td>\n",
       "      <td>-0.006519</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>0.046023</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.064586</td>\n",
       "      <td>-0.089208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2083.100098</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2073.649902</td>\n",
       "      <td>2082.419922</td>\n",
       "      <td>3.486910e+09</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>14.350000</td>\n",
       "      <td>13.09000</td>\n",
       "      <td>13.630000</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>0.127883</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>-0.011140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2210.464966</td>\n",
       "      <td>2227.864990</td>\n",
       "      <td>2207.599976</td>\n",
       "      <td>2226.090088</td>\n",
       "      <td>3.896030e+09</td>\n",
       "      <td>15.905000</td>\n",
       "      <td>16.675000</td>\n",
       "      <td>15.12000</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.025598</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.173678</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.121102</td>\n",
       "      <td>0.078661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>7.597450e+09</td>\n",
       "      <td>31.910000</td>\n",
       "      <td>53.290001</td>\n",
       "      <td>29.91000</td>\n",
       "      <td>40.740002</td>\n",
       "      <td>0.039034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075779</td>\n",
       "      <td>0.110842</td>\n",
       "      <td>0.130730</td>\n",
       "      <td>0.312771</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.901177</td>\n",
       "      <td>2.129032</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open_SPX     High_SPX      Low_SPX    Close_SPX    Volume_SPX  \\\n",
       "count  1007.000000  1007.000000  1007.000000  1007.000000  1.007000e+03   \n",
       "mean   2119.581836  2128.078819  2110.290448  2120.067844  3.578009e+09   \n",
       "std     203.564221   201.960388   205.402709   203.788063  7.006715e+08   \n",
       "min    1743.819946  1755.790039  1737.920044  1741.890015  1.281125e+09   \n",
       "25%    1977.690002  1985.454956  1968.880005  1978.279968  3.164345e+09   \n",
       "50%    2083.100098  2093.000000  2073.649902  2082.419922  3.486910e+09   \n",
       "75%    2210.464966  2227.864990  2207.599976  2226.090088  3.896030e+09   \n",
       "max    2657.189941  2665.189941  2639.030029  2647.580078  7.597450e+09   \n",
       "\n",
       "          Open_VIX     High_VIX     Low_VIX    Close_VIX  Trailing_1d_Return  \\\n",
       "count  1007.000000  1007.000000  1007.00000  1007.000000         1007.000000   \n",
       "mean     14.591470    15.398352    13.88146    14.509851            0.000414   \n",
       "std       3.818355     4.362816     3.45737     3.878825            0.007647   \n",
       "min       9.230000     9.520000     8.56000     9.140000           -0.039414   \n",
       "25%      12.030000    12.580000    11.55500    11.990000           -0.002768   \n",
       "50%      13.700000    14.350000    13.09000    13.630000            0.000367   \n",
       "75%      15.905000    16.675000    15.12000    15.770000            0.004441   \n",
       "max      31.910000    53.290001    29.91000    40.740002            0.039034   \n",
       "\n",
       "          ...       Trailing_10d_Return  Trailing_22d_Return  \\\n",
       "count     ...               1007.000000          1007.000000   \n",
       "mean      ...                  0.003976             0.008668   \n",
       "std       ...                  0.021532             0.029552   \n",
       "min       ...                 -0.103864            -0.103103   \n",
       "25%       ...                 -0.006465            -0.006519   \n",
       "50%       ...                  0.004842             0.010867   \n",
       "75%       ...                  0.016340             0.025598   \n",
       "max       ...                  0.075779             0.110842   \n",
       "\n",
       "       Trailing_63d_Return  Trailing_252d_Return  Trailing_1d_Return_VIX  \\\n",
       "count          1007.000000           1007.000000             1007.000000   \n",
       "mean              0.025246              0.109754                0.002730   \n",
       "std               0.040427              0.085228                0.079916   \n",
       "min              -0.121361             -0.115759               -0.259057   \n",
       "25%               0.007828              0.046023               -0.040619   \n",
       "50%               0.030593              0.127883               -0.004528   \n",
       "75%               0.050070              0.173678                0.038286   \n",
       "max               0.130730              0.312771                0.493333   \n",
       "\n",
       "       Trailing_1d_Max_Move_VIX  Trailing_5d_Return_VIX  Is_MA5_above_MA20  \\\n",
       "count               1007.000000             1007.000000        1007.000000   \n",
       "mean                   0.104367                0.012358           0.641509   \n",
       "std                    0.065659                0.182587           0.479795   \n",
       "min                    0.027692               -0.426630           0.000000   \n",
       "25%                    0.064586               -0.089208           0.000000   \n",
       "50%                    0.088867               -0.011140           1.000000   \n",
       "75%                    0.121102                0.078661           1.000000   \n",
       "max                    0.901177                2.129032           1.000000   \n",
       "\n",
       "       Is_Trailing_1d_Vol_above_MA10_Vol        Label  \n",
       "count                        1007.000000  1007.000000  \n",
       "mean                            0.540218     0.528302  \n",
       "std                             0.498627     0.499446  \n",
       "min                             0.000000     0.000000  \n",
       "25%                             0.000000     0.000000  \n",
       "50%                             1.000000     1.000000  \n",
       "75%                             1.000000     1.000000  \n",
       "max                             1.000000     1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe the dataset that would be used for training of models\n",
    "display(SPX_data_modified.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data description shows that SPX closed above its open 52.8% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the dataset with the engineered features\n",
    "SPX_data_modified.to_csv('SPX_Data_New_Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataset with the engineered features needs to be separated clearly into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features are-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_SPX</th>\n",
       "      <th>High_SPX</th>\n",
       "      <th>Low_SPX</th>\n",
       "      <th>Close_SPX</th>\n",
       "      <th>Volume_SPX</th>\n",
       "      <th>Open_VIX</th>\n",
       "      <th>High_VIX</th>\n",
       "      <th>Low_VIX</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing_5d_Return</th>\n",
       "      <th>Trailing_10d_Return</th>\n",
       "      <th>Trailing_22d_Return</th>\n",
       "      <th>Trailing_63d_Return</th>\n",
       "      <th>Trailing_252d_Return</th>\n",
       "      <th>Trailing_1d_Return_VIX</th>\n",
       "      <th>Trailing_1d_Max_Move_VIX</th>\n",
       "      <th>Trailing_5d_Return_VIX</th>\n",
       "      <th>Is_MA5_above_MA20</th>\n",
       "      <th>Is_Trailing_1d_Vol_above_MA10_Vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>2626.239990</td>\n",
       "      <td>2634.409912</td>\n",
       "      <td>2624.750000</td>\n",
       "      <td>2633.459961</td>\n",
       "      <td>1281124551</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>-0.029966</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.129611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2639.780029</td>\n",
       "      <td>2648.719971</td>\n",
       "      <td>2627.729980</td>\n",
       "      <td>2629.570068</td>\n",
       "      <td>3539040000</td>\n",
       "      <td>11.38</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.023495</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.073882</td>\n",
       "      <td>0.204152</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.183384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>2657.189941</td>\n",
       "      <td>2665.189941</td>\n",
       "      <td>2639.030029</td>\n",
       "      <td>2639.439941</td>\n",
       "      <td>4023150000</td>\n",
       "      <td>11.05</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.26</td>\n",
       "      <td>11.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015293</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.026001</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>0.205898</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2645.100098</td>\n",
       "      <td>2650.620117</td>\n",
       "      <td>2605.520020</td>\n",
       "      <td>2642.219971</td>\n",
       "      <td>3942320000</td>\n",
       "      <td>11.19</td>\n",
       "      <td>14.58</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.032348</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>0.071179</td>\n",
       "      <td>0.204097</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.175610</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2657.739990</td>\n",
       "      <td>2633.929932</td>\n",
       "      <td>2647.580078</td>\n",
       "      <td>4938490000</td>\n",
       "      <td>10.49</td>\n",
       "      <td>12.05</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.068555</td>\n",
       "      <td>0.191145</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.114169</td>\n",
       "      <td>0.099692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open_SPX     High_SPX      Low_SPX    Close_SPX  Volume_SPX  \\\n",
       "0 2017-12-06  2626.239990  2634.409912  2624.750000  2633.459961  1281124551   \n",
       "1 2017-12-05  2639.780029  2648.719971  2627.729980  2629.570068  3539040000   \n",
       "2 2017-12-04  2657.189941  2665.189941  2639.030029  2639.439941  4023150000   \n",
       "3 2017-12-01  2645.100098  2650.620117  2605.520020  2642.219971  3942320000   \n",
       "4 2017-11-30  2633.929932  2657.739990  2633.929932  2647.580078  4938490000   \n",
       "\n",
       "   Open_VIX  High_VIX  Low_VIX  Close_VIX                ...                  \\\n",
       "0     11.63     11.68    10.86      11.08                ...                   \n",
       "1     11.38     11.67    10.65      11.33                ...                   \n",
       "2     11.05     11.86    10.26      11.68                ...                   \n",
       "3     11.19     14.58    10.54      11.43                ...                   \n",
       "4     10.49     12.05    10.25      11.28                ...                   \n",
       "\n",
       "   Trailing_5d_Return  Trailing_10d_Return  Trailing_22d_Return  \\\n",
       "0            0.000963             0.018369             0.019272   \n",
       "1            0.014615             0.023495             0.023293   \n",
       "2            0.015293             0.021882             0.026001   \n",
       "3            0.019445             0.032348             0.029054   \n",
       "4            0.010404             0.018303             0.017435   \n",
       "\n",
       "   Trailing_63d_Return  Trailing_252d_Return  Trailing_1d_Return_VIX  \\\n",
       "0             0.066529              0.192706               -0.029966   \n",
       "1             0.073882              0.204152                0.021872   \n",
       "2             0.066895              0.205898                0.013298   \n",
       "3             0.071179              0.204097                0.054206   \n",
       "4             0.068555              0.191145                0.066800   \n",
       "\n",
       "   Trailing_1d_Max_Move_VIX  Trailing_5d_Return_VIX  Is_MA5_above_MA20  \\\n",
       "0                  0.095775                0.129611                1.0   \n",
       "1                  0.155945                0.183384                1.0   \n",
       "2                  0.383302                0.182006                1.0   \n",
       "3                  0.175610                0.141700                1.0   \n",
       "4                  0.114169                0.099692                1.0   \n",
       "\n",
       "   Is_Trailing_1d_Vol_above_MA10_Vol  \n",
       "0                                0.0  \n",
       "1                                0.0  \n",
       "2                                0.0  \n",
       "3                                0.0  \n",
       "4                                0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected labels are-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    1.0\n",
       "Name: Label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = SPX_data_modified.iloc[:,0:(cols_new-1)]\n",
    "Y = SPX_data_modified.iloc[:,(cols_new-1)]\n",
    "print('The features are-')\n",
    "display(X.head(n=5))\n",
    "print('The expected labels are-')\n",
    "display(Y.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The splitting of the labels has been done correctly.\n"
     ]
    }
   ],
   "source": [
    "# Check if the splitting is done correctly\n",
    "a = pd.concat([X,Y], axis=1)\n",
    "if pd.DataFrame.equals(a,SPX_data_modified):\n",
    "    print('The splitting of the labels has been done correctly.')\n",
    "else:\n",
    "    print('The splitting of the labels has NOT been done correctly. Please check again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset needs to be split into training, cross-validation and test sets. Care needs to be taken that look-ahead bias does not creep in while doing this. To ensure this, sequential splitting will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 202 604\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, cross-validation and testing sets\n",
    "\n",
    "# The split is supposed to be done sequentially into training, cross-validation and test sets in the ratio 60%-20%-20%\n",
    "n = round(0.2*rows_new)\n",
    "X_test = X.iloc[0:n,:]\n",
    "Y_test = Y[0:n]\n",
    "\n",
    "X_cv = X.iloc[n:(2*n+1),:]\n",
    "Y_cv = Y[n:(2*n+1)]\n",
    "\n",
    "X_train = X.iloc[(2*n+1):,:]\n",
    "Y_train = Y[(2*n+1):]\n",
    "\n",
    "print(X_test.shape[0], X_cv.shape[0], X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The splitting of the labels has been done correctly.\n",
      "The splitting of the features has been done correctly.\n"
     ]
    }
   ],
   "source": [
    "# Check if the splitting has been done correctly\n",
    "a = pd.concat([Y_test,Y_cv,Y_train])\n",
    "if pd.DataFrame.equals(a,Y):\n",
    "    print('The splitting of the labels has been done correctly.')\n",
    "else:\n",
    "    print('The splitting of the labels has NOT been done correctly. Please check again!')\n",
    "\n",
    "b = pd.concat([X_test,X_cv,X_train])\n",
    "if pd.DataFrame.equals(b,X):\n",
    "    print('The splitting of the features has been done correctly.')\n",
    "else:\n",
    "    print('The splitting of the features has NOT been done correctly. Please check again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark chosen is the buy and hold strategy. That translates to predicting 1 on all the days. The below snippet will copute the accuracy & F1 scores, and the PnL associated with the benchmark. The PnL would be calculated as buying at the open of the oldest date, and selling at the close of the newest date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An XIRR function should be implemented to allow for the comparison of benchmark PnLs against that of the models. The following was taken from https://github.com/peliot/XIRR-and-XNPV/blob/master/financial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000002\n"
     ]
    }
   ],
   "source": [
    "# XIRR function in python\n",
    "import datetime\n",
    "from scipy import optimize\n",
    "\n",
    "def xnpv(rate,cashflows):\n",
    "    chron_order = sorted(cashflows, key = lambda x: x[0])\n",
    "    t0 = pd.to_datetime(chron_order[0][0], format='%Y/%m/%d') #t0 is the date of the first cash flow\n",
    "    return sum([cf/(1+rate)**((pd.to_datetime(t, format='%Y/%m/%d')-t0).days/365.0) for (t,cf) in chron_order])\n",
    "\n",
    "def xirr(cashflows,guess=0.1):\n",
    "    return optimize.newton(lambda r: xnpv(r,cashflows),guess)\n",
    "\n",
    "# Dummy calculations to test the function\n",
    "cashflows = ('2016/12/26',-5),('2016/12/26',-5),('2017/12/26',16)\n",
    "print(xirr(cashflows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all the benchmark calculations below to allow for ease of comparison later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the benchmark on the training set is 0.5265\n",
      "The F1 score of the benchmark on the training set is 0.6898\n",
      "The XIRR of the benchmark on the training set is 5.70%\n",
      "\n",
      "The accuracy score of the benchmark on the cross-validation set is 0.5347\n",
      "The F1 score of the benchmark on the cross-validation set is 0.6968\n",
      "The XIRR of the benchmark on the cross-validation set is 18.77%\n",
      "\n",
      "The accuracy score of the benchmark on the test set is 0.5274\n",
      "The F1 score of the benchmark on the test set is 0.6906\n",
      "The XIRR of the benchmark on the test set is 14.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX')\n",
    "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX')\n",
    "# Show benchmark scores on the different sets against which the models would be compared\n",
    "benchmark_Y_pred_train = np.empty(Y_train.shape[0])\n",
    "benchmark_Y_pred_train.fill(1)\n",
    "cashflows_train = (X_train.iloc[0,0],X_train.iloc[0,int(close_index[0])]),\\\n",
    "                  (X_train.iloc[X_train.shape[0]-1,0],-X_train.iloc[X_train.shape[0]-1,int(open_index[0])])\n",
    "print('The accuracy score of the benchmark on the training set is {:,.4f}'.\\\n",
    "      format(accuracy_score(Y_train, benchmark_Y_pred_train)))\n",
    "print('The F1 score of the benchmark on the training set is {:,.4f}'.format(f1_score(Y_train, benchmark_Y_pred_train)))\n",
    "print('The XIRR of the benchmark on the training set is {:,.2f}%'.format(100*xirr(cashflows_train)))\n",
    "\n",
    "benchmark_Y_pred_cv = np.empty(Y_cv.shape[0])\n",
    "benchmark_Y_pred_cv.fill(1)\n",
    "cashflows_cv = (X_cv.iloc[0,0],X_cv.iloc[0,int(close_index[0])]),\\\n",
    "               (X_cv.iloc[X_cv.shape[0]-1,0],-X_cv.iloc[X_cv.shape[0]-1,int(open_index[0])])\n",
    "print('\\nThe accuracy score of the benchmark on the cross-validation set is {:,.4f}'.\\\n",
    "      format(accuracy_score(Y_cv, benchmark_Y_pred_cv)))\n",
    "print('The F1 score of the benchmark on the cross-validation set is {:,.4f}'.format(f1_score(Y_cv, benchmark_Y_pred_cv)))\n",
    "print('The XIRR of the benchmark on the cross-validation set is {:,.2f}%'.format(100*xirr(cashflows_cv)))\n",
    "\n",
    "benchmark_Y_pred_test = np.empty(Y_test.shape[0])\n",
    "benchmark_Y_pred_test.fill(1)\n",
    "cashflows_test = (X_test.iloc[0,0],X_test.iloc[0,int(close_index[0])]),\\\n",
    "                 (X_test.iloc[X_test.shape[0]-1,0],-X_test.iloc[X_test.shape[0]-1,int(open_index[0])])\n",
    "print('\\nThe accuracy score of the benchmark on the test set is {:,.4f}'.format(accuracy_score(Y_test, benchmark_Y_pred_test)))\n",
    "print('The F1 score of the benchmark on the test set is {:,.4f}'.format(f1_score(Y_test, benchmark_Y_pred_test)))\n",
    "print('The XIRR of the benchmark on the test set is {:,.2f}%'.format(100*xirr(cashflows_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_classifier(clf, X_train, Y_train):\n",
    "    start = time.clock()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    end = time.clock()\n",
    "    print('The classifier took {:,.4f} sec to train'.format(end - start))\n",
    "\n",
    "def classifier_predict(clf, X_train):\n",
    "    start = time.clock()\n",
    "    Y_train_pred = clf.predict(X_train)\n",
    "    end = time.clock()\n",
    "    print('The classifier took {:,.4f} sec to predict'.format(end - start))\n",
    "    \n",
    "    return(Y_train_pred)\n",
    "\n",
    "def score_classifier(clf, X_train, Y_train):\n",
    "    Y_pred = classifier_predict(clf, X_train)\n",
    "    print('The accuracy score is {:,.4f}'.format(accuracy_score(Y_train, Y_pred)))\n",
    "    print('The F1 score is {:,.4f}'.format(f1_score(Y_train, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing to note is that only the engineered features should be used for predicting the labels because the raw features haven't been balanced, and exhibit look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "engineered_features_start_index = int(np.where(SPX_data_modified.columns.values == 'Trailing_1d_Return')[0])\n",
    "print(engineered_features_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amrit Prasad\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "The classifier took 0.0109 sec to train\n",
      "The classifier took 0.0006 sec to predict\n",
      "The accuracy score is 1.0000\n",
      "The F1 score is 1.0000\n",
      "\n",
      "SVM:\n",
      "The classifier took 0.0257 sec to train\n",
      "The classifier took 0.0198 sec to predict\n",
      "The accuracy score is 0.5265\n",
      "The F1 score is 0.6898\n",
      "\n",
      "Random Forest:\n",
      "The classifier took 0.0381 sec to train\n",
      "The classifier took 0.0032 sec to predict\n",
      "The accuracy score is 0.9785\n",
      "The F1 score is 0.9793\n",
      "\n",
      "Extreme Gradient Boosting:\n",
      "The classifier took 0.0973 sec to train\n",
      "The classifier took 0.0017 sec to predict\n",
      "The accuracy score is 0.9255\n",
      "The F1 score is 0.9296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as x_gb\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "xgb = x_gb.XGBClassifier()\n",
    "\n",
    "print('Decision Tree:')\n",
    "train_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "print('\\nSVM:')\n",
    "train_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "train_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "print('\\nExtreme Gradient Boosting:')\n",
    "train_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning of the learners has been handled below one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Depth: 2, Min_Samples_Leaf: 1, Criterion: gini, Splitter: best, F1: 0.6178861788617886\n",
      "Max_Depth: 4, Min_Samples_Leaf: 9, Criterion: gini, Splitter: best, F1: 0.6428571428571429\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=9, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Tune Decision Tree\n",
    "max_depth_list = np.arange(1,11)\n",
    "min_samples_leaf_list = np.arange(1,11)\n",
    "criterion_list = ('gini','entropy')\n",
    "splitter_list = ('best','random')\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "best_dt = dt\n",
    "best_dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_dt_Y_cv_pred = best_dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_dt_F1 = f1_score(Y_cv, best_dt_Y_cv_pred)\n",
    "for max_depth in max_depth_list:\n",
    "    for min_samples_leaf in min_samples_leaf_list:\n",
    "        for criterion in criterion_list:\n",
    "            for splitter in splitter_list:\n",
    "                dt = DecisionTreeClassifier(random_state=42, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "                dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                dt_Y_cv_pred = dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                dt_F1 = f1_score(Y_cv, dt_Y_cv_pred)\n",
    "                if dt_F1 > best_dt_F1:\n",
    "                    best_dt = dt\n",
    "                    best_dt_F1 = dt_F1\n",
    "                    print('Max_Depth: {}, Min_Samples_Leaf: {}, Criterion: {}, Splitter: {}, F1: {}'.\\\n",
    "                          format(max_depth, min_samples_leaf, criterion, splitter, dt_F1))\n",
    "\n",
    "print(best_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the importance of the features used in the best Decision Tree is shown. The most important feature is the Trailing 1d Gap Return, while the least are 7 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trailing_1d_Max_Move_VIX             0.262671\n",
      "Trailing_1d_Gap_Return               0.159327\n",
      "Trailing_3d_Return                   0.126040\n",
      "Trailing_1d_Return_VIX               0.118499\n",
      "Trailing_1d_Max_Move                 0.110672\n",
      "Trailing_1d_Return                   0.073412\n",
      "Trailing_10d_Return                  0.058735\n",
      "Trailing_63d_Return                  0.056813\n",
      "Trailing_5d_Return_VIX               0.033832\n",
      "Is_Trailing_1d_Vol_above_MA10_Vol    0.000000\n",
      "Is_MA5_above_MA20                    0.000000\n",
      "Trailing_252d_Return                 0.000000\n",
      "Trailing_22d_Return                  0.000000\n",
      "Trailing_5d_Return                   0.000000\n",
      "Trailing_4d_Return                   0.000000\n",
      "Trailing_2d_Return                   0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_dt_feature_importance = pd.Series(best_dt.feature_importances_, index=\\\n",
    "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
    "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
    "print(best_dt_feature_importance.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# params = {'max_depth':np.arange(1,11), 'min_samples_leaf':np.arange(1,5), 'criterion':('gini','entropy'), \\\n",
    "#           'splitter':('best','random')}\n",
    "\n",
    "# dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# grid = GridSearchCV(dt, params)\n",
    "# grid.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "\n",
    "# best_dt = grid.best_estimator_\n",
    "# print(best_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM has been tuned below. Care should be taken to not increase the gamma parameter too much, else it'll lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Tune SVM\n",
    "kernel_list = ('linear', 'rbf', 'sigmoid', 'poly')\n",
    "C_list = np.arange(1,16)\n",
    "gamma_list = [1e-7*10**i for i in np.arange(1,7)]\n",
    "degree_list = np.arange(2,6)\n",
    "\n",
    "svm = SVC()\n",
    "best_svm = svm\n",
    "best_svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_svm_Y_cv_pred = best_svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_svm_F1 = f1_score(Y_cv, best_svm_Y_cv_pred)\n",
    "for kernel in kernel_list:\n",
    "    for C in C_list:\n",
    "        for gamma in gamma_list:\n",
    "            for degree in degree_list:\n",
    "                svm = SVC(C=C, gamma=gamma, kernel=kernel, degree=degree)\n",
    "                svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                svm_Y_cv_pred = svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                svm_F1 = f1_score(Y_cv, svm_Y_cv_pred)\n",
    "                if svm_F1 > best_svm_F1:\n",
    "                    best_svm = svm\n",
    "                    best_svm_F1 = svm_F1\n",
    "                    print('Kernel: {}, C: {}, Gamma: {}, Degree: {}, F1: {}'.format(kernel, C, gamma, degree, svm_F1))\n",
    "\n",
    "print(best_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 1, Max_Features: sqrt, F1: 0.6285714285714286\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 20, Max_Features: sqrt, F1: 0.6302521008403361\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 40, Max_Features: sqrt, F1: 0.6475409836065574\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 50, Max_Features: sqrt, F1: 0.6502057613168724\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: gini, N_Estimators: 90, Max_Features: sqrt, F1: 0.6535433070866142\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: entropy, N_Estimators: 40, Max_Features: None, F1: 0.6639676113360323\n",
      "Max_Depth: 1, Min_Samples_Leaf: 1, Criterion: entropy, N_Estimators: 60, Max_Features: None, F1: 0.6666666666666666\n",
      "Max_Depth: 1, Min_Samples_Leaf: 7, Criterion: entropy, N_Estimators: 60, Max_Features: None, F1: 0.6720647773279352\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=7, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Tune Random Forest\n",
    "n_estimators_list = [1] + [i*10 for i in np.arange(1,11)]\n",
    "criterion_list = ('gini','entropy')\n",
    "max_depth_list = np.arange(1,7)\n",
    "min_samples_leaf_list = np.arange(1, 10, 3)\n",
    "max_features_list = ('sqrt','auto',None)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "best_rf = rf\n",
    "best_rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
    "for max_depth in max_depth_list:\n",
    "    for min_samples_leaf in min_samples_leaf_list:\n",
    "        for criterion in criterion_list:\n",
    "            for n_estimators in n_estimators_list:\n",
    "                for max_features in max_features_list:\n",
    "                    rf = RandomForestClassifier(random_state=42,max_depth=max_depth, min_samples_leaf=min_samples_leaf,\\\n",
    "                                                criterion=criterion, n_estimators=n_estimators, max_features=max_features)\n",
    "                    rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                    rf_Y_cv_pred = rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                    rf_F1 = f1_score(Y_cv, rf_Y_cv_pred)\n",
    "                    if rf_F1 > best_rf_F1:\n",
    "                        best_rf = rf\n",
    "                        best_rf_F1 = rf_F1\n",
    "                        print('Max_Depth: {}, Min_Samples_Leaf: {}, Criterion: {}, N_Estimators: {}, Max_Features: {}, F1: {}'.\\\n",
    "                              format(max_depth, min_samples_leaf, criterion, n_estimators, max_features, rf_F1))\n",
    "\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the importance of the features used in the best Random Forest is shown. The most important feature is the Trailing 1d Return, while the least are 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trailing_1d_Return                   0.183333\n",
      "Trailing_1d_Max_Move                 0.166667\n",
      "Trailing_10d_Return                  0.150000\n",
      "Trailing_63d_Return                  0.100000\n",
      "Trailing_22d_Return                  0.100000\n",
      "Trailing_252d_Return                 0.083333\n",
      "Trailing_1d_Gap_Return               0.083333\n",
      "Trailing_1d_Max_Move_VIX             0.050000\n",
      "Trailing_1d_Return_VIX               0.033333\n",
      "Trailing_2d_Return                   0.033333\n",
      "Trailing_5d_Return                   0.016667\n",
      "Is_Trailing_1d_Vol_above_MA10_Vol    0.000000\n",
      "Is_MA5_above_MA20                    0.000000\n",
      "Trailing_5d_Return_VIX               0.000000\n",
      "Trailing_4d_Return                   0.000000\n",
      "Trailing_3d_Return                   0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_rf_feature_importance = pd.Series(best_rf.feature_importances_, index=\\\n",
    "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
    "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
    "print(best_rf_feature_importance.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 1, N_Estimators: 1, L1 Factor: 0, Gamma: 0, F1: 0.6967741935483871\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=1, missing=None, n_estimators=1, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=0,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "n_estimators_list = [1, 10, 30, 50 ,100] + [i*250 for i in np.arange(1, 5)]\n",
    "max_depth_list = np.arange(1,11)\n",
    "reg_lambda_list = [0, 0.1, 0.5, 1, 2, 5]\n",
    "gamma_list = [0] + [10**i for i in np.arange(0,4)]\n",
    "\n",
    "xgb = x_gb.XGBClassifier()\n",
    "best_xgb = xgb\n",
    "best_xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_xgb_Y_cv_pred = best_xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_xgb_F1 = f1_score(Y_cv, best_xgb_Y_cv_pred)\n",
    "for max_depth in max_depth_list:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for reg_lambda in reg_lambda_list:\n",
    "            for gamma in gamma_list:\n",
    "                xgb = x_gb.XGBClassifier(max_depth=max_depth, n_estimators = n_estimators, reg_lambda=reg_lambda, gamma=gamma)\n",
    "                xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "                xgb_Y_cv_pred = xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "                xgb_F1 = f1_score(Y_cv, xgb_Y_cv_pred)\n",
    "                if xgb_F1 > best_xgb_F1:\n",
    "                    best_xgb = xgb\n",
    "                    best_xgb_F1 = xgb_F1\n",
    "                    print('Max Depth: {}, N_Estimators: {}, L1 Factor: {}, Gamma: {}, F1: {}'.\\\n",
    "                           format(max_depth, n_estimators, reg_lambda, gamma, xgb_F1))\n",
    "\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune Gradient Boosting\n",
    "# loss_list = ('deviance', 'exponential')\n",
    "# n_estimators_list = [1, 10, 30, 50 ,100] + [i*250 for i in np.arange(1, 5)]\n",
    "# criterion_list = ('friedman_mse','mae')\n",
    "# max_depth_list = np.arange(1,6)\n",
    "# min_samples_leaf_list = np.arange(1,4)\n",
    "\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "# best_gb = gb\n",
    "# best_gb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "# best_gb_Y_cv_pred = best_gb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "# best_gb_F1 = f1_score(Y_cv, best_gb_Y_cv_pred)\n",
    "# for max_depth in max_depth_list:\n",
    "#     for min_samples_leaf in min_samples_leaf_list:\n",
    "#         for criterion in criterion_list:\n",
    "#             for n_estimators in n_estimators_list:\n",
    "#                 for loss in loss_list:\n",
    "#                     gb = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion=criterion,\\\n",
    "#                                                     n_estimators=n_estimators, loss=loss)\n",
    "#                     gb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "#                     gb_Y_cv_pred = gb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "#                     gb_F1 = f1_score(Y_cv, gb_Y_cv_pred)\n",
    "#                     if gb_F1 > best_gb_F1:\n",
    "#                         best_gb = gb\n",
    "#                         best_gb_F1 = gb_F1\n",
    "#                         print('Max_Depth: {}, Min_Samples_Leaf: {}, Criterion: {}, N_Estimators: {}, Loss: {}, F1: {}'.\\\n",
    "#                               format(max_depth, min_samples_leaf, criterion, n_estimators, loss, gb_F1))\n",
    "\n",
    "# print(best_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the best out of the tuned models on the basis of the cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate XIRR of the best models\n",
    "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX')\n",
    "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX')\n",
    "date_index = np.where(SPX_data_modified.columns.values == 'Date')\n",
    "\n",
    "def convert_label_to_cash_sign(label):\n",
    "    if label == 1:\n",
    "        cash_sign = -1\n",
    "    else:\n",
    "        cash_sign = 1\n",
    "    \n",
    "    return(cash_sign)\n",
    "\n",
    "def get_cashflows(best_clf, X):\n",
    "    Y_pred = best_clf.predict(X.iloc[:,engineered_features_start_index:X.shape[1]])\n",
    "    # A prediction of 1 should signal buying that day, which can be done at the index open price. Similarly, 0 should signal\n",
    "    # selling at the open. The position should be kept until the prediction label changes. At the change, sell/buy accordingly.\n",
    "    # The net position should be either 1 unit long/short or squared off.\n",
    "    length = len(Y_pred)\n",
    "    prev_cash_sign = convert_label_to_cash_sign(Y_pred[length-1])\n",
    "    net_position = -prev_cash_sign\n",
    "    cashflows = ((X.iloc[length-1, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                  X.iloc[length-1, int(open_index[0])]*prev_cash_sign),)\n",
    "\n",
    "    for i in np.arange(length-2, 1, -1):\n",
    "        cur_cash_sign = convert_label_to_cash_sign(Y_pred[i])\n",
    "        if cur_cash_sign != prev_cash_sign or net_position == 0:\n",
    "            net_position = net_position - cur_cash_sign\n",
    "            cashflows = (cashflows) + ((X.iloc[i, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                         X.iloc[i, int(open_index[0])]*cur_cash_sign),)\n",
    "        prev_cash_sign = cur_cash_sign\n",
    "\n",
    "    cur_cash_sign = convert_label_to_cash_sign(Y_pred[0])\n",
    "    # On the last day, if net position is 0, execute at the open and square off on that day's close. If net position isn't 0,\n",
    "    # then check if the label is different from last day. If yes, then square off at open and stop, else square off at close and\n",
    "    # stop\n",
    "    if net_position == 0:\n",
    "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),) + \\\n",
    "                                ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 -X.iloc[0, int(close_index[0])]*cur_cash_sign),)\n",
    "    elif cur_cash_sign != prev_cash_sign:\n",
    "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),)\n",
    "        net_position = net_position - cur_cash_sign\n",
    "    else:\n",
    "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('%Y/%m/%d'),\\\n",
    "                                 -X.iloc[0, int(close_index[0])]*prev_cash_sign),)\n",
    "        net_position = net_position + prev_cash_sign\n",
    "    \n",
    "    return(cashflows)\n",
    "\n",
    "best_dt_cv_cashflows = get_cashflows(best_dt, X_cv)\n",
    "best_svm_cv_cashflows = get_cashflows(best_svm, X_cv)\n",
    "best_rf_cv_cashflows = get_cashflows(best_rf, X_cv)\n",
    "best_xgb_cv_cashflows = get_cashflows(best_xgb, X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree:\n",
      "The classifier took 0.0060 sec to train\n",
      "The classifier took 0.0008 sec to predict\n",
      "The accuracy score is 0.5545\n",
      "The F1 score is 0.6429\n",
      "The XIRR is 33.69%\n",
      "\n",
      "Best SVM:\n",
      "The classifier took 0.0254 sec to train\n",
      "The classifier took 0.0062 sec to predict\n",
      "The accuracy score is 0.5347\n",
      "The F1 score is 0.6968\n",
      "The XIRR is 18.77%\n",
      "\n",
      "Best Random Forest:\n",
      "The classifier took 0.1967 sec to train\n",
      "The classifier took 0.0073 sec to predict\n",
      "The accuracy score is 0.5990\n",
      "The F1 score is 0.6721\n",
      "The XIRR is 62.86%\n",
      "\n",
      "Best Extreme Gradient Boosting:\n",
      "The classifier took 0.0070 sec to train\n",
      "The classifier took 0.0017 sec to predict\n",
      "The accuracy score is 0.5347\n",
      "The F1 score is 0.6968\n",
      "The XIRR is 18.77%\n"
     ]
    }
   ],
   "source": [
    "print('Best Decision Tree:')\n",
    "train_classifier(best_dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_dt, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_dt_cv_cashflows)))\n",
    "\n",
    "print('\\nBest SVM:')\n",
    "train_classifier(best_svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_svm, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_svm_cv_cashflows)))\n",
    "\n",
    "print('\\nBest Random Forest:')\n",
    "train_classifier(best_rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_rf, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_rf_cv_cashflows)))\n",
    "\n",
    "print('\\nBest Extreme Gradient Boosting:')\n",
    "train_classifier(best_xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "score_classifier(best_xgb, X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_cv)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_xgb_cv_cashflows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the tuned models perform better than the benchmark on the F1 metric in the cross-validation set. On accuracy and XIRR, Decision Tree and Random Forest are better. Random Forest is the best out of the two. A Voting Classifier was implemented to see if the combination of the best tuned Random Forest, Extreme Gradient Boosting and Decision Tree would beat the benchmark. SVM was not included because it and Extreme Gradient Boosting predict exactly the same as the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best SVM is the same as the benchmark.\n",
      "The best Extreme Gradient Booster is the same as the benchmark.\n"
     ]
    }
   ],
   "source": [
    "if np.unique(best_svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])) == 1:\n",
    "    print('The best SVM is the same as the benchmark.')\n",
    "if np.unique(best_xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])) == 1:\n",
    "    print('The best Extreme Gradient Booster is the same as the benchmark.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=9, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0....stimators=60, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))],\n",
      "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_list = ('hard','soft')\n",
    "weights_list = [[2,1,3],[2,3,1],[1,2,3],[1,3,2],[3,1,2],[3,2,1],None]\n",
    "vot = VotingClassifier(estimators=[('dt', best_dt), ('xgb', best_xgb), ('rf', best_rf)])\n",
    "best_vot = vot\n",
    "best_vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_vot_Y_cv_pred = best_vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_vot_F1 = f1_score(Y_cv, best_vot_Y_cv_pred)\n",
    "for voting in voting_list:\n",
    "    for weights in weights_list:        \n",
    "        vot = VotingClassifier(estimators=[('dt', best_dt), ('xgb', best_xgb), ('rf', best_rf)], voting=voting, weights=weights)\n",
    "        vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "        vot_Y_cv_pred = vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "        vot_F1 = f1_score(Y_cv, vot_Y_cv_pred)        \n",
    "        if vot_F1 > best_vot_F1:\n",
    "            best_vot = vot\n",
    "            best_vot_F1 = vot_F1\n",
    "            print('Voting: {}, Weights: {}, F1: {}'.format(voting, weights, vot_F1))\n",
    "\n",
    "print(best_vot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Voting Classifier is a different model from the best tuned model. Investigate if it is superior.\n"
     ]
    }
   ],
   "source": [
    "if np.array_equal(best_vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]]), \\\n",
    "                  best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])):\n",
    "    print('The best Voting Classifier is the same as the best Random Forest.')\n",
    "else:\n",
    "    print('The Voting Classifier is a different model from the best tuned model. Investigate if it is superior.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Voting Classifier has accuracy 0.5396, F1 0.6782, and XIRR: 26.90%\n"
     ]
    }
   ],
   "source": [
    "best_vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "best_vot_Y_cv_pred = best_vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "best_vot_accuracy = accuracy_score(Y_cv, best_vot_Y_cv_pred)\n",
    "best_vot_F1 = f1_score(Y_cv, best_vot_Y_cv_pred)\n",
    "best_vot_cashflows = get_cashflows(best_vot, X_cv)\n",
    "best_vot_XIRR = xirr(best_vot_cashflows)\n",
    "print('The best Voting Classifier has accuracy {:,.4f}, F1 {:,.4f}, and XIRR: {:,.2f}%'.format(best_vot_accuracy, \\\n",
    "                                                                                               best_vot_F1, 100*best_vot_XIRR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Random Forest is the best classifier, it would be good to see if the random state had any impact on its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best RF is RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=7, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "100 Random States were chosen and the best RF model run using them.\n",
      "\n",
      "F1 scores:\n",
      "Their mean is 0.6196, and std. deviation is 0.0217\n",
      "The maximum is 0.6721 for random_state = 42\n",
      "The minimum is 0.5674 for random_state = 90\n",
      "\n",
      "XIRR:\n",
      "Their mean is 75.19%, and std. deviation is 88.72%\n",
      "The maximum is 745.09% for random_state = 89\n",
      "The minimum is 19.90% for random_state = 62\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd8XGeV+P/PmZFGvVrFVnOVXOO4\nKIlTbJyENBKSLJCQUDaBhbC7dFj2F9hl4UvZBmQpG2CzJBsWQhI2hBAC6XG67bh3Se5W773PzPP7\n484dj6QZaVTGGkvn/Xr5Zc1oytXo6p77nOc854oxBqWUUiraOKZ7A5RSSqlgNEAppZSKShqglFJK\nRSUNUEoppaKSBiillFJRSQOUUkqpqBTRACUi14tIuYgcE5F7g3z/P0Rkr+9fhYi0BXzvLhE56vt3\nVyS3UymlVPSRSK2DEhEnUAFcA1QBO4A7jTGHQzz+s8BaY8zHRSQT2AmUAgbYBaw3xrRGZGOVUkpF\nnUiOoC4GjhljThhjBoDHgFtGefydwKO+r68DXjTGtPiC0ovA9RHcVqWUUlEmJoKvnQ9UBtyuAi4J\n9kARmQ8sBF4Z5bn5QZ53D3APQFJS0vply5ZNfquVUkpNiV27djUZY7In+vxIBigJcl+ofOIdwBPG\nGM94nmuMeQB4AKC0tNTs3LlzItuplFIqAkTk9GSeH8kUXxVQGHC7AKgJ8dg7OJveG+9zlVJKzUCR\nDFA7gGIRWSgiLqwg9PTwB4nIUiAD2Bpw9/PAtSKSISIZwLW++5RSSs0SEUvxGWPcIvIZrMDiBB4y\nxhwSkW8BO40xdrC6E3jMBJQTGmNaROTbWEEO4FvGmJZIbatSSqnoE7Ey83NN56CUUiq6iMguY0zp\nRJ+vnSSUUkpFJQ1QSimlopIGKKWUUlFJA5RSSqmopAFKKaVUVNIApZRSKippgFJKKRWVNEApFaC+\no4+K+s7p3gylFBqglBrim08f4hO/1AXfSkWDSHYzV+q8Yoxhx6lWmrv7GXB7ccXo+ZtS00n/AtWE\ntPUM8K0/HqZv0DP2g88TVa29NHX1YwzUtPVO9+YoNetpgFIT8vKRBh566yQ7Ts2cHr67z7T6v65s\n7ZnGLVFKgQYoNUEnmroAON08cw7ke8604fBdKrOyZeQIyus19Ay4J/0+gx4vXf2Tfx2lZjoNUGpC\nTjR2A3CmZSYFqFZK52cS65SgI6hH3jnD5f/6Ct2TDC7fe76c63/4Oh7vzLiSgFKRogFqgn63q4pf\nvHFiujdj2pxssgLU6ebuad6SqdE36OFQTQfrF2SQl55AZZDAu/t0K609g2w93jyp99p1upWq1l7e\nOTlz0qMqurX3DvKpX+2krr1vujdlXDRATUDfoIfv/OkwP3nlGDPlelrj4fWagAA1M0ZQB6vbcXsN\n64oyKMxIpLJ1ZIrvWIOV1txS3jDh9zHGUFFnrbP684HaCb+Oin6vVTRS3xEdAWHr8WaeP1TP1hNN\n070p4xLRACUi14tIuYgcE5F7QzzmdhE5LCKHROQ3Afd7RGSv79+IS8VPpz8fqKW1Z5D23kGqZ2G1\nV017L/1uLylxMVS29MyIIG0XSKwtSqcwM4GqYSMoYwzHG60A9Wp544R/5pr2Pjr73bhiHDx7sHbW\npvmau/qnexMiqm/Qw8cf3sF9L1RM96YAUO47Kappi46AGa6IBSgRcQL3AzcAK4A7RWTFsMcUA18F\nLjfGrAS+EPDtXmPMGt+/myO1nRPxyPYzJMQ6AThY3THNW3Pu2fNPVxRn0T3gobl7YJq3aPL2nGmj\nKDORrOQ4CjISae4eGDLXVNveR8+Ah5V5qVS39fpHU+Nlj54+csl8mroG2H5ycunC81FZXQel332J\nbSdm7s9+rKELj9fwxtGJn8xMpbI66zh1vp1QR3IEdTFwzBhzwhgzADwG3DLsMZ8E7jfGtAIYYyae\nOzlHjtR2sOt0K5++cjEOgcM17dO9SefcCd9IYvPSbOD8T/MZY9h9ppW1RekAFGYmAta6KJsdkD6x\ncSEw8TRfmS9Afepdi0iIdfKn/bMvzbfjVCvGwPYTM3cOzm6XVdPe5x95T6ezIygNULZ8oDLgdpXv\nvkAlQImIvCUi20Tk+oDvxYvITt/9twZ7AxG5x/eYnY2NjVO79SH8ettp4mIcfGTDfJbkJHOwZvaN\noE42dZMcF8P6+RkAnGk5vwslatv7qO/oZ12R9fMUZiQADCmUsA8yVyzJZmluClvKJra/VdR3Mi8t\nntzUeK5ansPzh+pwe7yT/AnOLwerrJO6/VVt07wlFrfHO+UH7vL6Tv+Shdcrpnfep3fAwylfMZMG\nqLMkyH3Dx7oxQDGwGbgT+IWIpPu+V2SMKQU+BPxQRBaPeDFjHjDGlBpjSrOzs6duy0Po6nfz1J5q\nblqdR3qii5V5aRyajSOopm4WZSdRkJGIyLkbQfUNeqZkHdJwgfNPcHYEFVhqfqyhi7SEWLKSXWxe\nls3O0y109g2O+73K6jpZOjcFgJsumEdT18Csq+Y76Pub2V/dHhXpr0d3VHLFv73Ci4frp+w1j9Z3\nUZKbwqKsJN44em5OnkNuS0MnXgPz0uKp1TkovyqgMOB2AVAT5DF/MMYMGmNOAuVYAQtjTI3v/xPA\nq8DaCG5rWJ7aU033gIcPbygCYGVeKvUd/TR2zuwJ3+FONHazKCuJ+Fgnc1PjOXOOAtTXnjzAXz74\nzpS/7p4zbcTHOlg+LxWAOUkuEmKdQxbrHmvoYklOMiLC5pIcBj2Gt46Nbw7F7fFyvKGLpblWgNq8\nNIdEl5NnQlTzdfW7ueuhd3jwzZMT/MmiT7/bQ0V9J+mJsTR29lPfMf1/O/sr2/Aa+Oyju9lbOTWj\nuor6TopzU9hUks22Ey30u6evJZidVr5yWQ6d/W46JnBiNV3GDFAi8u8ikioisSLysog0ichHwnjt\nHUCxiCwUERdwBzC8Gu8p4Erf+2RhpfxOiEiGiMQF3H85cDj8H2vqGWP49bbTrJiXytpC60x7ZV4a\nwJijqGg4S5wqfYMeatp7WZiVDEBRZuI5WaxrjOG1ikb2VbUxOMUpsd1nWlmdn06s0/pzEBEKMxOG\njKCON3axODsJgNIFGSTHxfBaxfjmoU419zDg8fpHUAkuJ1cvz+W5gyPTfH2DHu753528VtHIM/uH\nn9edvyrquhj0GD6wrgCIjjRfRX0nF+SnkZMSz189vINTTeGnrPdXtY343XX3u6lq7WVpbjIbi7Po\nHfSw61RriFeIvLLaThJinWxYNAc4v9J84YygrjXGdAA3YY14SoCvjPUkY4wb+AzwPHAE+K0x5pCI\nfEtE7Kq854FmETkMbAG+YoxpBpYDO0Vkn+/+fzXGTGuAOlTTQVldJx/eUISIlb1ckZfq/14oHX2D\n3PCjN/jm04dmRKA62dSNMbDId7CePyeR0+cgQJ1q7qG5e4BBz9k1WFOh3+3hUHWHP71nK8xI9M9B\ntfUM0NQ1wJIcKyjHOh1sLM5iS9n4KrTsieoS3wgK4MYL5tHSPcAf9tb4X8vt8fL5x/bw9vFmluam\nUFbbOaly9EGPly8+vndS67emip3eu620EKdD2F8VuRR5axjVpV6voaK+i4sWZPLwxy7Cawx3/887\nYZXBH67p4Ob/fIvf7qwacv9RX0FNcW4KGxbNIdYpvDaONN+W8gZ+u6Ny7AeGqby+g5LcZAp8c6sz\nLUDF+v5/D/CoMSbshLkx5s/GmBJjzGJjzHd99/2TMeZp39fGGPMlY8wKY8wFxpjHfPe/7bt9oe//\nB8f5c005++Byqe8sBCAtIZaizMRRR1D/+mwZZXWdPPz2Kb7/QnnEtzPS7OCwMMsOUEk0dvZHZG4o\nUGBTWvt3MRX2VbYz4PGy1lcgYSvMTKSqtXfI+ic7QIFVwVjX0edPn4TDnjgf/joFGQl8+f/28e77\nXuMXb5zg3icP8Pyhev7pphV8ctMiegc9nGyaeCXYA6+f4Pd7qvmPF6d/Tc6B6nZS4mMoyU2mJDeF\n/dWRCVCnm7sp/e5LY84rVbX20jvoYencZBZlJ/Pg3RdR297H98NYv/TC4ToA3jw2NPhUBJyIJPmK\nid4Is1DC4zX84+8P8vU/HJyyVFxZrTXvmZ9uB6jzZx4qnAD1RxEpA0qBl0UkGzh/fsIpYqd78n1n\nIbZV+akh10JtPd7Mb7af4RNXLOTOi4u4f8vxc9Ieqba9l68/dZD2nqnPNdsl5vYIyi4oiHSab9ep\nVlLiY3A6ZEqvePvswVpcMQ6uKM4acn9BRgJd/W7aegb9JeaLswMDSw7AuCbWK+o6WeCbu7PFxzp5\n8Yvv4nsfWE1aQizf+dMRnthVxeeuLubjVyxkZRijdIDOvkHe99O3eO5g3ZD7jzd28aOXjzInycX+\nqnYOTmFA+OO+Gu57oXxco8hD1e2syktDRFidn8aBqraIZBZ2nW7F4zU8vW/09Ki9LxX7RrXrijJY\nU5jO0TD2Mft3v/V4M96AEW5FfSdxMQ6KfH8bG4uzOVzbEdZc9esVjVS3WQvhh/8uJ6Kxs5/m7gGW\nzU0lOzmOWKfMrBGUMeZe4FKg1BgzCPQwcj3TjFfZ0ktuahxxMc4h96/MS+NMSw/tvUODQe+Ah3uf\n3E9RZiJfvnYp37l1FTesmst3/nSE3+0amhKYar/fU82vtp3mC4/vGfKHMxVONHYzLy2eRJd1rcv5\nvj/C0Sr57t9yjM3f28Kj75yZ8PzRztMtlM7PYGFW0rhGLaPxeg3PHazjXSXZJMcNvXZnYCXfsYYu\nXDEOCjIS/d/PTY1nY3EWP3v1OMcawtue8vpOf4FEoASXk9tKC3nyby/n2c9v5KcfXscX310MWKMt\nV4xjzAC1t7KN3Wfa+MxvdvsPnF6v4atPHiA+xsHjn7qUuBgHj+04E9a2jqar382XHt/LZx/dw49f\nOTbmttkGPV6O1HVyQYE1d7u6MI3WnsEh682min3S+GpZAwPu0PtcuR2gAka1RZmJY15upaatl0M1\nHSybm0Jrz6D/dQAqfAU1Tl+d+btKrArj4SOtYB7ZfoasZBfz5yTy1J7qMR8/FnuB7rK5KTgcQm5q\n/MwKUCKSCHwa+Jnvrjys0dSsUtXaQ2HAAcpmn+EeHvZHet+L5Zxu7uFf338BCS4nTofwwzvWcPmS\nOfz97/ZHdCd552QLcTEOtpQ38uNXjo7ruQeq2ukdCF1xdKKp25/eA2sOCghZyXesoYsfvlRBa88g\nX33yAO++7zV+v6dqXHMqrd0DHG/spnRBJktzU6ZsBLWnso3a9j7ec8HcEd+zf9eVLb0c91Ut2gcc\n2/dvu5BEl5NPP7JnzAs39g1aa1HsAolQls9L5T0XzPPPc8Y6HSzNTRmzEMfe/0pyU/jbR3axpayB\nR3ec4Z2TLfzjjStYkpPMjavn8dSemkmlY/dWtnHjj9/gqb3V/PW7FuOKcfBEmCdcR+u7GHB7/X8z\nq/Oteb9IzEMdqmnH5XTQ2e8etWNFRX0n+ekJpMTH+u8rzEykvqN/1N/py0esk4CvvWc5AG8HNBCu\nqOscMs+4Yl4qc5JcY6b5att7eaWsnttKC7l1TT5bTzRT2z6544SdDrf3u7z0hBmX4vsfYAC4zHe7\nCvhOxLYoSlW19vrPqgMFq+Tbc6aVB988yZ0XF3HZ4rOpo7gYJ99870o8XqsiLRLcHi87T7XygfUF\nvH9dAT986SivlIWXhuroG+QvfvoWX//DwaDfN8ZworHLn94DSE90kRofEzTFZ4zhm08fIj7WyUtf\nehcP3lVKoiuGLz6+j4ffPhX2z7TrtFUBVTo/g6VzUzjT0jPiINvQ0cfr4/xM/3ygFpfTwdXLc0d8\nrzDTt1jXN4JaHHCGbctNjee+D66hvL6T//fHQ6O+19H6Lowh6AhqLCvzUjlU0zFqKuxQTQd5afE8\nes8Gls5N4VO/3sU//+kIly2ew22lVsXchy4uoqvfzTP7Jta9Ys+ZVm7/+VbcHsPjn7qUe29YxjXL\nc3l6X82ooxSbXSBxQb71N1MyNxmX08H+6tCVfIMe77gbrnq9xipgWJNHQqxz1DRsecC6NFuRv5NI\n6FHUC4frWZSVxKaSbObPSfR3uG/vHaSuo29IgHI4hCuKs3j9aNOoGY3f7qjCa+DOi4q4dW0+xsDT\neydXwXmktpPslDjmJMcBkJ+ecF61OwonQC02xvw7MAhgjOkl+CLcGWvQ46W2vdffYSBQdkocualx\n/jRHQ0cff/vIbuamxvPV9ywb8fglOcnMTY2P2OK9w7UddPW7uWTRHL77F6tYmZfKFx7bG1bpbEVd\nJ26v4cndVUFHKS3dA3T0uf0l5rb5c5KCVvI9e7CON4818eVrSshOiePq5bn86bNXsCgraVx92Hae\nbiXWKVxYmE5JbgrGWAf8QD94oYKPPbwj7Gs1GWN49kAtG4uzSA04e7alxMeSnhjLsYYuKlt7WJI9\nMkCBlb75m82LefSdSv6wN3RKxk4BlYwxggpmZV4qbT2jNyY+XNvBirw00hJi+fVfXcLi7GQ8xvAv\n77vAPxpbPz+D4pxkfvPO+NN8bT0DfOY3e8hOieOZz17BRQsyAXjfunxaugfCOuE6WN1OclwMC+ZY\nJzhxMU6WzUvhwCgjqIffOsVV3391XKO+ytYeOvvdrJ+fwaaSLF48XB80uA96vJxo7B4STCAgvRvk\nopVgzfdtO9HMu1dYJzaXLprD9pPNeLzGn+4tyR26v2wszqapqz9ketrjNTy+4wwbi7MompPIwqwk\n1hSm89QkA1R5vZWGtOWlx1Pf0XfeNCkOJ0ANiEgCvi4Qvo4O07+67hyqaevFa6AgyAgK8HeU6Bv0\n8Mlf7aKtZ5AH/rI06IFPRNhUksWbR5vGbHHT0Nk37qakdn+zDQsziY918vOPrMfhEL74271jTkbb\nB9EYp4N/f25kxeEJX5ALHEGBby3UsOtCdfe7+fYzh1k+L5WPbJjvv9/hEFblp41IiY5m1+kWVual\nER/r9J/tBub87TVSHq8Juwhgb2UbNe19vOeCeSEfU5iRyOsVjRgztPJuuC9fU0Lp/Ay+9uQBmkKU\nJ1fUd+KKcfgPzuOxMt8epQf/zHoHPJxo7PIve0hPdPHk31zGS196F/MD3k9EuPPiIvZWto3r8zfG\n8Hf/t5+Gzj7u//A6MpJc/u9tKskmK9kV1rzqwep2VuSl4ghIlV6Qn8aB6vaQI4udp1voHvBwpDb8\n7bU/p1V5aVy7Yi51HX0cCLJfnG7u9q1LG/q7tUfPoQp/XqtoZNBjuMYOUIvn0Nnn5lBNO+V11t/r\n8KB3+RKr+vft48HTfK+WN1DT3seHLi7y3/cXa/M5Utvhn0cCqyDjV9tOh/7hA7g9Xo7Wdw0LUAm4\nvea8aS4QToD6BvAcUCgijwAvA38f0a2KMvaZVLA5KIBVeakca+ji84/tYX9VGz+8Yw2rfAeVYDYW\nZ9PR5x61xNbt8XLXQzu466HxdU7YfrKZhVlJ5KTGW9ucmci91y9jz5k2Xhij4qy8rpPkuBg+e+US\nXjpSz85TQ1cU2BV8i4eNoIrmWCXZgQH3J68co7a9j2/fspIY59DdbIWvI3hbz9jrVPrdHvZVtVPq\n6/tXlJlIfKxjSKn50YYu6nxpoGAHomCePVhHrFP8Z8HBFGYm0OD7Q14cYgQFVkD/5s0r6R7w8Gp5\n8JFEWV0nxQET5+OxfG4qDgkdoMrrrVY2K3ydMMAqvCgIsr++b10+rnEWSzz45kleOlLPvTcsZ03h\n0PVisU4Ht6zJ5+Wy+lHXHbk9Xg7XdrAqb+jfxYUF6XT2uf294oazf+bxXDXgYHU7MQ6hZG4yVy3L\nwekQXjg0ct8PFUyyk+OIj3UEvWglwEuH68lMcvl7N9pLT7Yeb6aivpNEl9Nf0m2bl5bA4uwk3jwW\nPED9ZvsZslPihuyPN62eh9MhPLXHWiP38Fsn+ciD2/n6UwfDGlGeau6h3+1l2dyz+0VemrVd50ua\nb9QAJVZuoAx4H3A38ChWNd+rEd+yKdLV7+YTv9wxrjPG4eyKnoIgKT6AFXlpeA08f6ie/+/6ZVy3\ncuSke6ArlmQhwqiTpr/edpojtR1Ut/XSEGYO3uM1vHOyhUsWZg65/wPrC1iUlcT3ny8fdWhfXtdJ\nSW4yf7VxIdkpcfzbc2VDRl0nmrpxOR0jSu3nZybi9hpqfVfrPFjdzoNvnuD96wooXTB0W+DsgfRw\nGGfFB6s7GHB7KV1gHQycDqE4Z2ihhD33lBIXw74g6aIzzT18+bf7/EHNGMOf9tdyxZIs0hJGjnJt\n9gmJyMhRY7CfKSs5LmTqtqIueAVfOBJcThZlJ4fsnG/Pf9rFB6NJT3TxnlVz+f2e6jELO8Cad/rX\nZ8u4dkUuH798QdDHvG9dPoMeM2rHixNN3fQNermgYOg22hV9wU4s2gMq/MZTHn+opoPi3BTiYpxk\nJLm4aEGGf81SIHtd2vCTDxGhMCN4h5RBj5dXyhr8gQ8gJzWeJTnJvO0LUMU5yUNGibYrlmSx/UTL\niPm6mrZetpQ3cHtpgb+bCcCc5DjeVZLNH/ZW87XfH+SbfzzMXN+JZzhFVsMLJMAaQYX7/GgwaoAy\n1tHpKWNMszHmT8aYZ4wx59UlGbeUNfDSkQa+9czEOzlUtfbgdAjz0uKDfv/CwjQcAretL+BTmxaN\n+XoZSS5W56fxeoiDWWNnPz94scJ/FhZulVNZXQcdfW4uWTQ0KMQ4HfzddUs52tDF70OUrhpjrDLo\nuSkkumL43NXF7DjVyitlZ7sPnGjsZv6cxBGjgKI5Z0vN23oG+Otf7yI7OY5/uHF50PdaEaLyMZhd\np61R3Pr5Z3+mktyUISOo1yoaWZydxOVLsoK2znlsxxl+t7uK9/7kTe7fcozdZ9qobuvlhlHSe3A2\npVuYkThk7VIwDoewsdhK3Q5PV7X3WBPnY1XwjcYulAjmcE0HKfExIU+ghrtpdR6dfe6w9qtvPXOY\nnJQ4vveBC/1zWSO3LY1lc1N4YnfoOTg7wAwfQRXnJBMX4wi6LYdqrfuSXM6gVw3YX9XG5u9tGTIa\nMMZwqKZ9SLC+dsVcKuq7RszDBluXZrNKzUcexHecaqGjz827hxXWXLpoDjtOtfhO8oL/ni9fYrU9\n2nNmaNuj3++pxmvgg6VFI55zy5o8atv7ePSdM3z6ysXcd/uFAGGV5pfVdeB0yJD0dF66dQybbHXg\nuRJOim+biFwU8S2JELu9y7YTLbxxdGKxtbKll7z0+BGpKtu8tARe+fJm/u39q0P+EQ+3qSSbvZVt\nI9ZPgdV9om/Qw399dD0OCb9fmT3/dMnCOSO+d8OquVyQn8Z/vFgRtHFlY2c/bT2D/rP8Oy4qZMGc\nRL72+wPc/vOtXPWDV3m1vGFIibnNnuc41dzNl367j/oOa64iM2CuIlBWslVYEk6A2nGqlflzEslO\nifPft2xuCg2d/bR2D9A36OGdky1sKslmdWGaP0gGeutYEyvzUnn3ihy+93w5H31wOzEO4dpR0ntw\n9rIbi8cYPdmuWJJFc/cAR+qG/lx2WsceLUzEyrxUatv7aAmSRjtc28GKealh73vrfOlSuzoylEM1\n7ew508ZfbVxEWmLokSbA+9cVsK+yLeSc6YHqdhJirZFgoBing5V5qUH3cXv/eO+FeRyt7xwx4vvT\ngVpONffweEBboIbOfpq6BlgVEKDsuaLh1XwV9Z2U5AQPJoWZiUGvFv3S4QZcMQ42lQxd2H3Z4jn0\n+C7eGSpAbVg8B4dY+6PNGKso6aIFGf4TvUDXrpjLey6Yy4/uWMNXrlvmL+AIJ0VXVtfJwmEBOCU+\nlpT4mFFLzf+0v5b3/uTNsEbYkRZOgLoS2Coix0Vkv4gcEJH9kd6wqeD1Gl4rb+SGVXMpyEjg358v\nm9DC1coQa6ACLchKCjqsD2VjcTYer2HrsEnTnada+N3uKj65cRGr8tMoyU0JmrYK5p2TLRRkJPiH\n8YFEhL+/finVbb38ZvvI+YcyfzrA+sOOdTr4xs0ryUh04XBY8yB3XFTEZ68qHvHcuanxuJwOfvLK\nUV4pa+DrN60Y0TpouBXzUsdM8Rlj2H26ldL5Q0eEJQGFEttPttDv9rKpJJsLC6z5kcB0UXvvIAeq\n27l6eS4//fB6/vNDa4mLcXD18hzSE4MHUJt9MBitQCLQRl83iuEnQr/fU8Xc1PigJw7hCtWY2OM1\nlNV2+kel4chMcrEoK2nMAPWb7WeIi3Hw/nXDL+M20i1r83zzJcFHUQeqrAKJYHNw64oy2FfVPuKA\neLimg9xUK83l9poRlaXbfKXdT+ys9Keu/enOgDngwsxEls9L5blDZ9N89rq0UFWVhZmJ/k4igbae\naOaShZn+heq2SwJaoBXnBt9fUuNjubAwfcg81P6qdo43dvM+X/Pc4RJcTn764fXcssb6HeSmxhPj\nEKrHGEF5vIadp1pYHWQuPC9t9FLzB988wYHqdraUjd278ZtPH+Lvn9g35uMmKpwAdQOwGLgKeC9W\n09j3RmyLptCB6naauwe4buVcvnRNCQerO3h2Au1DKlt6xwxQ47W2KJ3kuBheDziYDXq8fP0Ph8hL\ni+czVy0BYHVBGvvDaAdjjOGdUy2jHgSvWJLFZYvn8J+vHKNrWDm2/ccfmIa6cmkOz31hE4/dcyn3\nf3gd3751VdBRgNMhFGQkUN/Rzy1r8vhoQNVeKCt8hSWjnaWdbOqmuXvAP/9ks6uSKuo7eb2iEVeM\ngw0L5/gLUwLTRdtONOM1cPli63O5aXUeW796NT+6Y+yrtxRlJnLVshyuHWNO0ZaTGs+yuSlD5qGa\nuvp5tbzRfwCfqFAtj042ddM76PEHsHCtm5/B7jOtIfer4dc+G0tOSjzr52cMSQnbOvoG2VPZxoZF\nI+cjAS4vzmLA7R1xXaxDNR2szEvz/14DCyU6+qwTj6W5KdS09/mr4w5WdyCC/9Iptg+sL2DX6Vb+\n6Gt9dLyxC+8o69KKgrTw6hlwU17X4b+aQaDMJJf/PUdL5V6xJIt9Ve3+PntP7q7CFeMYtZo0kNMh\nzE2LH3MEdbC6ndaeQd61dOR18vLSQ3eTONPcw+4z1mj2qVGWTYB1zHlmfy2/3VnlL6CaauG0OjoN\npGMFpfcC6b77ot6W8gZErHSCNXYVAAAgAElEQVTaLWvyKclN5gcvlI/rCqa9Ax6auvr9padTJdbp\n4NLFc3xlzIbeAQ9//atdHKnt4J/eu8J/hra6ID2sdjBHG7po6R4YMf8USET48rVLae4e8P+h2srq\nrAV9odJyY7mgwJqHCFx3M5oV89Jwe82oZfT2SGh45VhOShxpCbGU1VkB6uIFmSS4nKQlxLIwK4l9\nAdf0eftYEwmxziEjuvhY55hzSmD9jh66+yL/mp9wbCzOYsfJVn83jj/uq8HtNbxvbfAz5HClJ7rI\nT08YUSxgj0JXzAt/BAXWmqiW7gFOhegA8oe9Q699Fo7NS62ec8OLet462oTHa/z9C4e7ZGEmLqdj\nyMiib9DDscYuVsxLpSAjgdT4GP9CX4B3TrTgNfC1G5eTnhjr7yh+qKadhXOSRrSuuuvS+VxYmM7X\n/3CQhs6+gBOy4KOdwIXatgNV7XgNXBgkQAFcszyH/PQEfyFDMJctzsLjNf5iiaf31XDNitxRi3WG\ny09PGHME9VpFIyJWQBwuLz3BX9A0nL2W77qVuWwpaxy1n2d9R79/WcUvx7HwfjzCaXX0eeARIMf3\n79ci8tmIbM0U21LeyJrCdDKTXDgdwleuW8aJpu6wW7MAVLdZO2iwLhKTtak4i6rWXvZVtfPRB7fz\nSnkD3751FdevOns2Zaethk8il9d18i9/PuJfH7Ldt/B1wxhppHVF6RRmJgTNx0+0ygzgvtvX8PRn\nrhiR+gjl7IggdPrySG0nsU4JWmW1NNcaqRxt6BoyH2CNOM++5lvHm7l4YSaumEhem/OsjcXZDHi8\nbD9p/T6e3F3NqvzUSRVI2FbmpY6Ytztc00GsU8JOQ9rW++ahdgdJ81nXPjvD8oBrn4XD7jn36rBF\nu6+WN5ISHxPytRJdVsfvwE4g5XXWJUZW5llza6vy0zgUEJy3nmjGFePgkoWZ3Lomn+cP1dHWM8Ch\nmo6g6c4Yp4Mf3LaangEPX3vyIGV1nbicjiHrxALZGZPAEdQ+3zzZ8BMm2+euLubFL20a9QRt3fx0\n4mMdvHWsidcqGmntGQwrhRooPyNhzCq81yoaWZ2f5u8gESgvPYGW7oERLc2MMTy1t5qLF2byt5uX\nMODx8uzB0F1H7HnDktxk/m9XVdD59MkK56/2r4BLfJfJ+CdgA/DJKd+SKdbU1c/+qjauDDhre/fy\nHNYVpfOTV46FPRdlr4EKt0JqPDb5/qDveGAr+6vauf9D60akx5bOTbHawQybRP7BC+X81+snuOFH\nb3D7z7fyxK4q5qXFjznSExHevTyXN481+bsueHz5/ckcRJ0OGVcQKMpMJMnlHLVQoqyugyU5KUFf\nd+ncFP/vxv4cwRpx1nX00dDRR127tdDZXiR5LtjB8M2jTVTUd3Kgun3Soyfbyrw0TjZ3D+mWcaim\nnZLc4J/RaJZkJ5MSH8OuMyMD1N7KNo7UdvDhS4rCLrwAaxSXkxLHawFrwexF1BuLs0IWGQFsLMmi\nrK6Thk7rzN5OZdqpy1X5aRyp6/Q3G956vJn1RRnExzq5rbSAAbeXX759mqrW3pBrEJfkpPCVa5fy\n0pF6Ht9RyaLspCFl3YGS4mKYk+QashZqb2UbhZkJQQ/6YAXBsU7Q4mKcXLxwDm8da+LJ3VVkJbvY\nWDwyDTeagvQE6jr6QjZebu8ZZM+ZVv8Jw3B2JV/NsEq+QzUdHG/s5tY1+awuSGNhVtKoab4D1e04\nBP7lfRfQM+CZ0mtY2cLZqwUIDLUezoNWR3YHgMAAJSJ8+JL5VLf1DkkXjMYe4k/1HBRY1W8Ls5KI\ncTh4+GMXBc1Du2IcLJ+X4j97A6vl0JbyBu68uIh/eM9yajusUdili+aEdUC5ZkUuA26vf66ksqWH\nvkHvpEZQ4+VwCMvHKJQ4UtvB8hBB057czk2NG7LdFxacnYey5yUC+yFGWnysk4sXZPLG0Sae3F2N\n0yHcvCZvSl57TVE6xsAvt54CrIP/4ZqOcaf3wPr81xZlBB1BPbL9DEkuJ7euHd+ZvYjwrpJs3jja\n6E+jl9V1UtfRx+aS4Ok928Yl1sHUrnA7VGNdN8o+4VqZl8qA28uxhi7aeqxKyUt984or89JYmZfK\nz1877n9sKB+/YiGl8zOsitUxTsisSr6zB/G9Z9r8GY3JuGLJHI42dPHi4XpuvjA/ZJAMJT8jAa+B\nuhBpujePNeE1BJ1/grOLdYePwp7aU02sU3jPBXMREW5dk8/2ky0hS9IPVFsnR+vnZ3LxwkwefvvU\nuKZPwhFus9jtIvJNEfkmsA0I6wKCInK9iJSLyDERuTfEY24XkcMickhEfhNw/10ictT3765w3i/Q\nlvJGspLjRuysVy7LwSHw0pHwri5a2dJDXIxjSJnzVHro7ot49vMbuSxIrti2uiCdg9Ud/lHf03ur\nGfQY7rpsPp/ctIhX/+5KHv3kBr76nuDrjoa7eEEmaQmx/s4SZUEW9J0LK/JSOVLbGXQ029I9QH1H\nP8vmBd8mu1BiY3H2kKC8Mi/NX5r/1rFmMhJjJ3QAn4yNxVmU13fy+I4zvKskm6wQZ9zjtak4ixtX\nz+N7z5ezpazBf62f8VTwBVpflEF5feeQC+O19wzyx3013LI2f8Q8Tjg2L82ho8/NHt88oN1ZI9TB\n0rYyL5WMxFh/BeTw0vmzhRLtbDvRgjFWabft9tJCen0FN6MVjDgdwvduu5DkuBh/J4hQCjPPLtZt\n6Oijpr0vZHpvPC73/a27vYb3jTO9B5CfPnqp+WsVDaTGx4QMpsEW69rXztq89Gx16y1r8kI2rDXG\ncKCq3f97+fjlC6lu6x3X9dHCEU6RxH3Ax4AWoBX4mDHmh2M9T0ScwP1YVYArgDtFZMWwxxQDXwUu\nN8asBL7guz8Tq8XSJcDFwDdEZPS9KYDb4+X1ikY2L80eUfpttygJ1uG7pq2XB14/PuSAWdnSS0FG\nwrhSHeOxMCtpzPmtCwrS6Op3+3vh/W53NSvzUv0tTJwO4dLFc8IOojFOB1cty+GVsgbcHi8V9Z2I\nhC6PjZSVeal09buDrtgv842shldj2VbMS2X5vFTeP6w8N8Hl9Jfmv328iUsXzxlX+f9UsFM2rT2D\nEzoAhSIifO8Dq1k+N5XPPbbHfzG+iQbg9fMzMMYaGdgeeOM4/W5vWJWYwVxRnIXTIbzqW3/4ankD\ny+elkjtK4QBYI7rLl5ztUVlW2zkk0Cyck0SSy8mhmg62HrcKX1YHHIBvWZOHy+kgLy1+zEKfhVlJ\nbP/a1fzlpaP/jEWZ1lyP2+Nlry/gri2afIBaPjeVzCQXS3NTwur+MZzdySVYocTZlGp2yJTq3LR4\nRIZeWXfr8WYaOvu5dc3Z/XWBr2FtsMX9te19NHcPsNqXsbhmRS4FGQn8z1unxv3zjCacIokNwFFj\nzI+NMT8CjonIJWG89sXAMWPMCWPMAPAYIy90+EngfmNMK4Axxh7WXAe8aIxp8X3vReD68H4k/Atg\nrwxRNXT18lwOVneMGCLf92IF//znsiGdmavaeiJSIDEeZwsl2s7Oa4RYNxGua1bk0tYzyM7TrZTX\ndVKUmRh2gcNUWTHP2rmDpfmO+EZ1gX3EAiXFxfDs5zf60zyBVheksfV4M7Xtff6z1XNp2dwUspLj\nSImPGdFxYLISXTH8912luJwOvvOnIwAsn+AIyu6Asts3D1XX3seDb57k5gvzQp4YjCUtIZZ1Rem8\nVtFIZ98gu063snmM0ZNtY3EWDZ39vHC4nt5Bz5CRocMhrMhL5WB1O1tPNFO6IGPIvFt6oou/3ryY\nOy4Or+owKS5mzJPOooAWXnsr24hxyLjL+YNxOIQf37GW790W/sL+QHZHm2AjqPL6Tuo7+kPOP4FV\nnZqbMrTU/Km91STHxXD18qHHzFvX5FFW1zmkcwucLdqyL53idAh3X7aAd061jNqdfrzCSfH9DAis\nBe7m7MULR5MPBM6aVfnuC1QClIjIWyKyTUSuH8dzEZF7RGSniOxsbDwbVLaUN+D0XYMlGPuX8HLA\nKKqle8B/Rmrn+CEya6DGa0lOMokuJ/ur2vnd7ipiHMItk5zX2FSSjcvp4MXD9ZTXh27PEknFuVbz\n1GCFEkdqO8hKjptQanV1QToDvlz45edw/snmcAhfua6Ef7xxeVjl7OOVn57Azz6ynlinUJiZELRr\nfjhS4mNZOjfVv2D3P16swOM1fOW6pZPavs1LczhY3cEf9lol9ptHOVgGusI38gw1l7QyL4391e1U\n1HcFnVf80jUlfO7qkQvJJ8p/0crWHvZVtbFsXsqU/T6vKM4aMgIcj/hYJ1nJcUFHUHaByqYxPvN5\n6fFU1Hfy2Dtn+NLje/njvhpuWDV3xM9304XW+r3ho6gD1W04ffPIttsvKmRNYTqd/VNXzRdWkYQJ\nWM1njPEC4ZxqBzs1GD7ZEAMUA5uBO4FfiEh6mM/FGPOAMabUGFOanX32F/JaRSPrizJCri0ozkmm\nMDOBlwPmoR7fUcmA28uNq+fxankjp5q66egbpL13MCIVfOPhdAir8tLYU9nGU3uq2bx08vMayXEx\nXLZkDs8drONkU/eQlvznSnyskyXZyUFHUGV1HSwPMf80FnvEmZ+e4L/i77n2wYuK+OBF4a8hGq+L\nF2by84+s5x9vXDH2g0exriidvWfaKKvr4P92VfLRDQsmnTGwz97/48UKUuJi/K2VxpKfnsCi7CT2\nV7XjinGMKJ1flZ/mb7QabOQ81ezP4XRzD/sr26ekQGKq5GcE7wbxWkUjy+amMDdE31BbQUYi+6ra\nuffJA7x+tJF3L8/lC9eUjHhcVnIcVy7N4YldlUNapB2o7qAkd2jATo2P5alPXz6lRUnhBKgTIvI5\nEYn1/fs8cCKM51UBhQG3C4Dhs21VwB+MMYPGmJNAOVbACue5QbV0W2shQo2ewMrlX70sl7eONdE7\n4MHjNfx622kuXTSHb9y0ghiH8Kttp/0lptOd4gNrHmpfZRv1Hf0j5l0m6poVuVS39eLxmmkZQYHd\nBHVoSsCaF+uacJpp6dwU4mMdvq7xUV9wOmFXL88ds3P+WNbPz6Cz383nHt1DkivG38FkMlbmpZKd\nEkdz9wBXFGeNq0ptoy8luzQ3ZcTzVuVb+0NyXMyQXnuRMi8t3j+f1tnvnpICialSEOTKuN39bnac\nahk1vWf74ruL+bf3X8BLX9rEjn94N/d/eN2IS4TYPrKhiKauAZ7zdeGxCiTauCA/8r+DcPacv8a6\n3Hu1798lwD1hPG8HUCwiC0XEBdwBPD3sMU9h9fpDRLKwUn4ngOeBa0Ukw1ccca3vvjG9fbwJYxg1\nQIGV5ut3e3nrWBMvH6mnuq2Xuy6bT05qPDdcMI/f7qz0rzaf7hQf4J+MTEuI5arlo5fshitwfmQ6\nRlBgVfLVd/QP+WM72dTNgNs74W1yxTh49JMb+Mr1k0tVzQb2gt2K+i7+5srFE+4kEkhE2ORL14Vz\nsAxkF5gEKx5Ykm11Pr94Yeaoa6qmSozTQX56Alt8abOpKJCYKvYIKrCga+vxZgY9JqzPfFF2Mh+8\nqIglOSljnsRtKs6mKDORR7ZZPTyr23pp7RnkgnMwogyniq/BGHOHMSbH9+9DAcUMoz3PDXwGK7Ac\nAX5rjDkkIt8SkZt9D3seaBaRw8AW4Cu+S3u0AN/GCnI7gG/57hvTW8eaSImLCdokMdAlC+eQHBfD\ny2X1/O/W0+SlxfsP2HdfNp/OPjc/3WLlwqe6zdFE2OmF9144j7iYqcmD56bGc2FBGrFOYUGQLuXn\nwrUrrBFA4BVZD49RwReOtUUZU1bePZMVZSaSlexibmo8H7984ZS97q1r88hMco37ZGrD4jnkpcUH\nnUOJcTq47/Y1/N215+7EozAzgQG3l5S4GBZlndsq19Hkp1vb1dR99sq4r1U0khDrZP2CsAuew+Jw\nCB/ZUMQ7p1ooq+vwF0GMdYydCiHnkkTkk8CrxpijvgsXPgi8HzgN3G2M2T3Wixtj/gz8edh9/xTw\ntQG+5Ps3/LkPAQ+F+XP4vXmsiQ2L54x5hmW3zH9mXy2d/W6+ct1S/3PWFWX4r7+TEhczrj5ZkbIg\nK4l//8DqkJWJE/WFa0o4Utsx7sWCU6VoTiKXL5nD4zsq+cyVS3A4hLK64C2O1NQTEe67fQ2pCbFT\nWtCxsTib3V+/ZtzPS46L4e2vXh3y+zeuDq+p6lQpykzkLZpZXZh2zpcrjMZOx1W39pKTYs03vX60\nkUsXz5myE9hAt60v5PsvVPDItjMkx8cQ45Bzsm5ytKPS54FTvq/vBC4EFmEFkx9FdrMm5nRzN5Ut\nvf7LHozlqmW5dPa7cTkd3HHR2SkvEeGuSxcA1kXromUe4/bSwilfMHzl0hz+dvPk5x0m44MXFVHd\n1utvFnqktoPF2cnnrH/ebLepJDuq5leiiT3/HG2fj38tlC81frq5m9PNPWwK89g3XhlJLm5aPY8n\nd1ex7USzb5536gPhcKMdAdzGGLte8Cbgf33pt5eA6ckHjcE+wIW79uXKpdk4HcJNF84b0V/r5jV5\npCfGsjBr+uefZrrrVuaSnhjrv/BcWW3npNJ7Sk0V+7Ib0VTBByMX69qNdscqL5+Mj26YT/eAhz1n\n2vzrnyJttHJxr4jMw+oecTXw3YDvTf+kTBBvHWtiXlo8i8KcT5mTHMfj92ygOMhVNeNjnTx+z6Uk\nx5/bxauzUVyMk/etLeBX205xrKGLuo6+CZeYKzWVrlyaw5evKRmzVdO5luq7Mq49gnqtoomCjISg\nV7yeKmsK0/1TH5O5OvR4jDaC+idgJ1aa72ljzCEAEXkX4ZWZn3NvHWsed2lx6YLMkJezXjo3JWTp\npZpaH7yokEGP4V/+bHVHCNVBQqlzKSkuhs9eXRyReZ3Jsq8LNeD2svV4E5tKsiM6HSEi3HXZAuBs\n9WekhRweGGOeEZH5QIrdishnJ/DBiG/ZOPUOeOjuHRyzvFxFp6VzU1hblM7LviuyaopPqdEVZCRQ\n1drL7jOtdA94/KX9kXTb+gLWFqZTfI7WTY46C22McQ8LThhjuo0xkbm+7yTYlzA/l5dWUFPrTl/n\nhaxkV8S6xys1U9gjqNcrGnE6hMvOwXXPROScBScIb6HueaGr382yuSl6YDuP3bh6Hkkup46elApD\nfkYCnf1u/nyglnVF6RPuyRjNZkwFQHe/myumoXO1mjpJcTH810dLp6SbgVIznX1dqFPNPVPW/iza\nTChAicgyY0zZVG/MZBjGbm+kop/+DpUKT35AE+toqzKcKhNN8b0wpVsxBXJS4rh4YeZ0b4ZSSp0T\ndoVxZpKLVVNwnapoNFqrox+H+hYQXavWsPrKnesL7iml1HTJSnaR6HJyxZKsqGrDNJVGO6J/DPgy\n0B/ke3dGZnOUUkqFQ0T4xV+WTluj53NhtAC1AzhojHl7+DdE5JsR2yKllFJhuWyGF4aNFqA+APQF\n+4YxZur68iullFJBjFYkkWyM6TlnW6KUUkoFGC1APWV/ISK/OwfbopRSSvmNFqACy0IWTeTFReR6\nESkXkWMicm+Q798tIo0istf37xMB3/ME3D/8UvFKKaVmuNHmoEyIr8MiIk7gfuAaoArYISJPG2MO\nD3vo48aYzwR5iV5jzJrxvq9SSqmZYbQAdaGIdGCNpBJ8X+O7bYwxYzVMuxg4Zow5ASAijwG3AMMD\nlFJKKTVCyBSfMcZpjEk1xqQYY2J8X9u3w+nmmQ9UBtyu8t033PtFZL+IPCEihQH3x4vIThHZJiK3\nBnsDEbnH95idjY2NYWySUkqp80Uku5kHW9o8PFX4R2CBMWY18BLwy4DvFRljSoEPAT8UkcUjXsyY\nB4wxpcaY0uzsmdmLSimlZqtIBqgqIHBEVADUBD7AGNNsjLE7Vfw3sD7gezW+/08ArwJrI7itSiml\nokwkA9QOoFhEFoqIC7gDGFKNJyLzAm7eDBzx3Z8hInG+r7OAy9G5K6WUmlUi1l3VGOMWkc8AzwNO\n4CFjzCER+Raw0xjzNPA5EbkZcAMtwN2+py8H/ktEvFhB9F+DVP8ppZSawcSYcVeQR6XS0lKzc+fO\n6d4MpZRSPiKyy1dLMCEz5pLvSimlZhYNUEoppaKSBiillFJRSQOUUkqpqKQBSimlVFTSAKWUUioq\naYBSSikVlTRAKaWUikoaoJRSSkUlDVBKKaWikgYopZRSUUkDlFJKqaikAUoppVRU0gCllFIqKmmA\nUkopFZU0QCmllIpKEQ1QInK9iJSLyDERuTfI9+8WkUYR2ev794mA790lIkd9/+6K5HYqpZSKPhG7\n5LuIOIH7gWuAKmCHiDwd5NLtjxtjPjPsuZnAN4BSwAC7fM9tjdT2KqWUii6RHEFdDBwzxpwwxgwA\njwG3hPnc64AXjTEtvqD0InB9hLZTKaVUFIrYCArIByoDblcBlwR53PtFZBNQAXzRGFMZ4rn5w58o\nIvcA9/hu9ovIwanY8BkqC2ia7o2IYvr5hKafzej08wlt6WSeHMkAJUHuM8Nu/xF41BjTLyJ/DfwS\nuCrM52KMeQB4AEBEdhpjSie3yTOXfj6j088nNP1sRqefT2gisnMyz49kiq8KKAy4XQDUBD7AGNNs\njOn33fxvYH24z1VKKTWzRTJA7QCKRWShiLiAO4CnAx8gIvMCbt4MHPF9/TxwrYhkiEgGcK3vPqWU\nUrNExFJ8xhi3iHwGK7A4gYeMMYdE5FvATmPM08DnRORmwA20AHf7ntsiIt/GCnIA3zLGtIzxlg9E\n4ueYQfTzGZ1+PqHpZzM6/XxCm9RnI8aMmNpRSimlpp12klBKKRWVNEAppZSKSjMiQI3VUmk2EZFC\nEdkiIkdE5JCIfN53f6aIvOhrHfWir/hk1hIRp4jsEZFnfLcXish23+fzuK+wZ1YSkXQReUJEynz7\n0aW6/1hE5Iu+v6uDIvKoiMTP5n1HRB4SkYbANaih9hWx/Nh3nN4vIuvGev3zPkAFtFS6AVgB3Cki\nK6Z3q6aVG/iyMWY5sAH4tO/zuBd42RhTDLzsuz2bfZ6zVaMA/wb8h+/zaQX+alq2Kjr8CHjOGLMM\nuBDrc5r1+4+I5AOfA0qNMauwir/uYHbvOw8zsstPqH3lBqDY9+8e4Gdjvfh5H6CYXEulGccYU2uM\n2e37uhPr4JKP9Zn80vewXwK3Ts8WTj8RKQBuBH7huy1YC8Sf8D1k1n4+IpIKbAIeBDDGDBhj2tD9\nxxYDJIhIDJAI1DKL9x1jzOtYFdiBQu0rtwD/ayzbgPRhS41GmAkBKqy2SLORiCwA1gLbgVxjTC1Y\nQQzImb4tm3Y/BP4e8PpuzwHajDFu3+3ZvA8tAhqB//GlQH8hIkno/oMxphr4PnAGKzC1A7vQfWe4\nUPvKuI/VMyFAhdUWabYRkWTgd8AXjDEd07090UJEbgIajDG7Au8O8tDZug/FAOuAnxlj1gLdzMJ0\nXjC+uZRbgIVAHpCElbYabrbuO2MZ99/ZTAhQ2hZpGBGJxQpOjxhjnvTdXW8Pp33/N0zX9k2zy4Gb\nReQUVjr4KqwRVbovbQOzex+qAqqMMdt9t5/ACli6/8C7gZPGmEZjzCDwJHAZuu8MF2pfGfexeiYE\nqDFbKs0mvvmUB4Ejxpj7Ar71NGBf+PEu4A/netuigTHmq8aYAmPMAqx95RVjzIeBLcAHfA+bzZ9P\nHVApInYX6quBw+j+A1Zqb4OIJPr+zuzPRvedoULtK08Df+mr5tsAtNupwFBmRCcJEXkP1lmw3VLp\nu9O8SdNGRK4A3gAOcHaO5WtY81C/BYqw/tBuC6N91IwmIpuBvzPG3CQii7BGVJnAHuAjAY2MZxUR\nWYNVQOICTgAfwzqZnfX7j4j8P+CDWNWye4BPYM2jzMp9R0QeBTZjXXKkHutCs08RZF/xBfX/xKr6\n6wE+ZowZtdv5jAhQSimlZp6ZkOJTSik1A2mAUkopFZU0QCmllIpKGqCUUkpFJQ1QSimlopIGKKWU\nUlFJA5RSSqmopAFKKaVUVNIApZRSKippgFJKKRWVNEAppZSKShqglFJKRSUNUEoppaKSBiillFJR\nSQOUUkqpqBQz9kPOD1lZWWbBggXTvRlKKRUxDZ39NHb2szIvdbo3JSy7du1qMsZkT/T5MyZALViw\ngJ07R704o1JKnde++fQhHn77FG9++3riY53TvTljEpHTk3m+pviUUuo80TvgAaBv0DPNW3JuaIBS\nSqnzRI8vMPVqgFJKKRVNevrdwNmR1EynAUoppc4TPb7A1KMBSimlVDTpGbBGUDoHpZRSKqrYIyed\ng1JKKRVV/AFKU3xKKaWiiZ3i0xGUUkqpqKIjKKWUUlHH4zX0u72AjqCUUkpFETu9BxqglFJKRZHA\ntF6fpviUUkpFi+6AoKQjKKWUUlEjMMWnnSSUUkpFjR4dQUUXEUkXkSdEpExEjojIpdO9TUopNR0C\nA9RsaXUU7Rcs/BHwnDHmAyLiAhKne4OUUmo62J3MXU7HrFkHFbUBSkRSgU3A3QDGmAFgYDq3SSml\npos9gspMcmmKLwosAhqB/xGRPSLyCxFJCnyAiNwjIjtFZGdjY+P0bKVSSp0D9sUKrQDlneatOTei\nOUDFAOuAnxlj1gLdwL2BDzDGPGCMKTXGlGZnZ0/HNiql1Dlhp/jmJLvoDajom8miOUBVAVXGmO2+\n209gBSyllJp1NMUXRYwxdUCliCz13XU1cHgaN0kppaZNz4CbhFgnia4YegdmR4ovaoskfD4LPOKr\n4DsBfGyat0cppaZFz4CHRJeTRJdTy8yjgTFmL1A63duhlFLTrXfAQ4LLSUKsk95BD8YYRGS6Nyui\nojbFp5RS6qzuATdJrhgSXE48XsOAZ+an+TRAKaXUeaDHN4KKj3UC0DcL5qE0QCml1HmgZ8BDUpyV\n4oPZ0Y9PA5RSSp0HegY8JMTGkOCyDtsaoJRSSkWFngE3iS4nCbFWbdts6MenAUoppc4D/hSfy07x\nzfxuEhqglFLqPNBrp3iH3SkAACAASURBVPjsOSgtklBKKTXdjDFWmbkWSSillIom/W4vxmAt1NUi\nCaWUUtGi29fJ3FqoaxVJ9GmRhFJKqelmdzK3Wx1Z92mRhFJKqWlmp/MSXYFzUFokoZRSapoFpvji\nYnQOSimlVJToDUjxORxCfKxjVlxyY1oClIgkTcf7KqXU+ajbF6CSfAUS1kULNUBNiojki0ip74KD\niEiOiPwzcDSS76uUUjOJXRBhd5Gwrwk100UsQInIF4C9wE+AbSJyF3AESADWR+p9lVJqprGr+BJ9\nASo+1jErRlCRvKLuPcBSY0yLiBQBx4BNxphtEXxPpZSacXqGpfgSXDqCmqw+Y0wLgDHmDFAxkeAk\nIk4R2SMiz0z5Fiql1HmgN1iKT0dQk1IgIj8OuJ0TeNsY87kwX+fzWKnB1KncOKWUOl90D3iIdQou\nX4l5giuGjt7Bad6qyItkgPrKsNu7xvsCIlIA3Ah8F/jSVGyUUkqdb6xO5k7/7YRYBw0dOoKaMGPM\nL0N9T0Tmh/kyPwT+HkiZko1SSqnzUHe/m6S4s4frhFinf15qJot0mfmlIvIBEcnx3V4tIr8B3gzj\nuTcBDcaYkCMvEblHRHaKyM7Gxsap23CllIoiPYMe//wTaJHEpInI94CHgPcDfxKRbwAvAtuB4jBe\n4nLgZhE5BTwGXCUivw58gDHmAWNMqTGmNDs7e0q3XymlokXvgMdfYg4QH+ucFd3MIzkHdSOw1hjT\nJyIZQA2w2hgT1iJdY8xXga8CiMhm4O+MMR+J1MYqpVS06u53k+g6e7hO1BHUpPUaY/oAjDGtQHm4\nwUkppdRZvYNDR1AJsU7cXsOgZ2Z3NI/kCGqxiDwdcHtB4G1jzM3hvpAx5lXg1anbNKWUOn9097sp\nzEj03473XxPKQ1rCzO35HckAdcuw2z+I4HsppdSM1TswskgCoG/QQ1pC7HRtVsRFssz8tUi9tlJK\nzSbdAyNTfMCM7yYRsQAlIgcAE+xbgDHGrI7Ueyul1ExiVfENLZKAmX/Rwkim+D4NnA7xvXAX6iql\n1Kw26PEy4PGOKDOHmR+gIjm79jDwQaDaGHPaGHMa6MNqW6TzUUopFYbhl9qAsym+mb4WKpIBaj2w\nGNgjIleJyOeBd4CtwCURfF81Q718pJ7f7qic7s1Q6pzq9QeogFZHrrNVfDNZJIskWoFP+QLTS1gL\ndTcYY6oi9Z7q/GOMod/t9acsRvPw26c42dTN7RcVnoMtUyo6dPsutRG0SEJTfBMjIuki8l/Ax4Dr\ngSeAZ0Xkqki9pzr/PH+ojou+8xIdfWNfOqCxs5+Gjn6MCVZ7o9TM1BssxTdLiiQimeLbDRwFSo0x\nLxhjvgB8FPiOiDwawfdV55HDtZ109rs509wz5mObuvoZ8HhpnwXXwVHK1hMsxRd7dh3UTBbJALXJ\nGPN9Y4zbvsMYs9cYcxnwSgTfV51HGjv7AKhr7xv1cW6Pl+buAQDqO/ojvl1KRQt/ii8uyAhqhs9B\nRSxAjTbXZIz570i9rzq/NPiCTW3H6AGqpWcAO7PX0Dn6Y5WaSYKl+OJjZkeRxMxt4qTOCw2dVoCq\na+8d9XGNnWdHTTqCUrNJd781gkoKSPE5HEJcjENTfEpFkh14asdI8TV1Dfi/rh9jtKXUTGIXQgT2\n4rNva5GEUhHi9RqauuwR1OhBJ3AEFfi1UjNdsIW6AImxTp2DUipSWnoGcHutiaWxR1BWUJqXFq8j\nKDWr9PS7ETk772SL1xGUUpFjF0jkpydQ29476vqmxs5+El1OFsxJ0gClZpWeAQ8JsU4cDhlyf8I5\nGkFVt/WGtU4xEmZ9gPr358rYW9k23ZsxK9nVeBcWptE3OPr6psbOfrJT4shNjfMXVkQTj9ew41TL\ndG+GmoG6h3UytyXEnpsR1Af/ayvff7484u8TzKwOUK3dA/z01eP8frd2X5oOdqBZXZAOjJ7ma+rq\nJys5jtzU+KjsJvHswVpu+/lWKuo7p3tT1AzTO+AeMf8E56ZIonfAQ1VrL0fruyL6PqHM6gB1oqkb\ngKrW0UucVWQ0+gNUGjB6oURjZz/ZyXFkp8RFZTeJijorMJ307VNKTZXhFyu0nYsUX3WbdWysbB27\n00skRG2AEpFCEdkiIkdE5JCv6eyUOqkBalo1dPSREh/DwqwkIIwRVIqL3NR4IPrWQh3XfUlFSG+o\nAOVyRnwdVJUvMNW29+H2eCP6XsFEbYAC3MCXjTHLgQ3Ap0VkxVS+wSn/QaUn6lJGs0FDZz85/397\n5x3e1HU34PfI8ra88MQD29hMAzY7BDLIXiWjbWbTrLZJ04ym6Uy/dKRJs9qMZjeELEIW2WmABBLC\nNAGMwXjgCd57yJYtW9L5/rhXwrIlDzDYse/7PDxY914dHV2de37nN4/Bm/AAb3TCfbJut9VGk6mb\n8ACfHgJqdAVKlNQdHUsaGsOJqcvi1gd1oitJ2BdcVpscMNL2RDBqBZSUskpKuVf92wjkAjHD+Rl2\nDaq9y0qzaXSZjMYDioDyQe+hI8LgQ6WbB6BBTdINM3gRYfB2vHe0YLNJx1gqa9Q0KI3hxeRGg/I5\nCUESPS0CZY0nf/E1agVUT4QQCUA6kNHr+M+FELuFELvr6uqG3G5xfTueHkropmaaOfnUGjuJCFQE\nTnSwj1sflN1XFR7g7bh+NGlQ1a2djolC06A0hht3Aupkmfi89Tr175M/R456ASWECADWAndLKVt7\nnpNSviSlnC+lnB8eHj6kdm02SWl9O/MmhQDaxHKykVJS22p2aETRQT5UuTHx2ZN0wwze+HnpMXjr\nqR1FAqpYNe9NjTRQ0dR/PpeGxlAxdVnw8+5r4vPz9KDbKuk+gb6hiuYO0uKC8dCJEQmUGNUCSgjh\niSKcVkspPxjOtmuMyqp3WYoi2DQN6uTS2mnBbLERYVB8SlGBvlS1dLqc3HtqUAARoywXqqReCcFd\nlhKG0WyhtcMywDs0NAaPqcuKn4sdp+21+U6kFlXe1EHCBH+iAn00E19PhBACWAnkSin/Pdzt253a\naXHBGLz1mgZ1krHvA+Uw8QX5YOqyYjT3ndzrVA0qXNW2IgNHV7mjorp2/L08HNr4SIXkaow9bDZJ\nR7d7HxScuF11O7ut1BnNxIT4EhfqS5lm4nPiVJQdeJcLIfap/y4crsbtOVCJYf7EhPiOWw3qs/2V\nPPh5zkn/XHuZI7vQiQpSNClXfqg6oxmDt97xQEYYRpcGVVzfTmK4P3GhfoCmjWsMH50WK1Li0sRn\n31X3ROVC2XOgYkN8iQvx0zSonkgpt0ophZRytpQyTf33v+Fqv7S+HR9PHVGBPsSG+I3bSWXl1hJW\nbi056VWR7QLGbuKLVgVUZXPf36GuzewQZMCoqyZRUt9GYlgAsSG+gObP1Bg+sisUt7vBx4WA8jqx\nGlRFk11A+REX6ket0XzS958atQLqRFNS307CBH90OkFsiO/3LhequqWTz/dXHVcbrZ3dZJU1Y5OQ\nV9068BsGiZSSd3eX0ebCXGentpeJrz8Nqt6olDmyExHoQ5fVNipSAzq7lVIwSWH+BPl6qubi8bnY\n0Rhe6oxm7lyTSVyoLxfNiu5z/kRv+17e1EODClUWXxUuFpAnknEtoJLClQoGcaF+37tcqL98ks3t\nb+11RLgdCzuLGlB3uyC7omWYega7Dzfxu/f3897uMrfX1Laa8fHUYVBNF5GBPgjhuppEbw1qNOVC\nHW4wISUkhfsjhFDNxZoGpXF8dFtt3L56L80dXbx43XyC/bz6XON7gn1Q5U0m9DpBZKAPcSGK+fpk\nm/nGpYDqtto40mhylNg5appxXh28vKWYT7MqT3r/BqKoro0NOTUA7Dty7JXYtxc14OvpQbCfp8OU\nMBxsL2wA6LdKvD1JV4mFAU8PHeEB3v1oUEcf0NFUTcIewZcUFgAwrs3F4w0pJf87UIWpa/ijNh/8\nPJddpY08csVsZkwMdHmNXUCdKLNbeVMHE4N98dAJYu0C6iSP7XEpoMqbOrDYJImOSaWv76DLYuPx\nDfn84/OcEalB1R8vbynG00OHXifILGs65na2FtazMDGUWTFBZFcOnwa1vageGEhAdTppRaDmQvUS\nOp3dVlo7LaNWgypSo0ETw48udsqHMRfq8fX5fDIKF0kAWWXN/PvLQ8NqGi+sNXLBU1v4Or922NoE\nxee8pWDoyfz9sfdIM79cvZc3dhwe1nY/yark1e2l3HRqIivS3BfPsZv4TlS5o/ImEzHBytwYYfDG\nS6+jXNOgTjz2Ve9RDapv9NWBimY6u23UtJrZfGh4B/bxUGvsZO2eCn40L5bp0YHsPXxsGlR1SyeF\ntW0sTQ4jNSaIQzVGzJbjH+gdXVYyjzQT4K3ncIOJpvYul9fZ6/D1JCrIp089PkeSrpMPavRUkyiu\naycy0JsA1VQZG+JLm9kyLNXW64xmnv2mkBc3Fx13WyeClVtLeHpjARtzh0eYlNa3c81/M8itauXd\n79ybh4+Ff3yew82v7R5WbWdjrmLFGG5hunJLMdOiDPzxwmn9Xncyovjsi3edThAb7HvSUyjGpYCy\nZ/4nqQIqyNcTg49zLtTOYmXzuWA/T94e5ofleFi1rRSLzcbPliUxNz6YrPJmrLahr2C3FSpazqnJ\nYcyKCaLbKjlUffx7vuw+3EiX1ca1i+MB2FfuWoDWtfYVUNFBvn18UPVqHb6eGpSflx6Dz+ioJqFE\n8Pk7Xrta7BwrX+bUICUcrGwdlK8xt6r1mMbCsSClJKNEMeU+viEf23F+bnmTiWtfzsBikyyZPIGt\nhfXDZrno6LKypaCeLovNYX4eDuyCeXdp07DtONts6mJ/RQvnp0bh6dH/9HwiE3XNFis1rWbHeAaI\nDfU76bUmx6WAKqlvJ8jXkxD/o36N3r6DXSWNpEQEcOWCODbl1TqizkYSY2c3b+48zAWp0SSE+ZMe\nH4Kpy3pMm+RtK6xngr8X06IMpE5U9mMaDjPf9qIG9DrBzUsTEcK1j6xDTciNUH1JdqKCfDB2Wpyi\n/xxVJHoJs9GSC1Vc305SeIDj9XCGmq87WO1YJdsXFO7YXdrIBU9t4e3vjhz35w6G0gYTNa1mTkma\nQF61kc8PHHtEaU1rJ9e+nIGxs5vXb1rIdYsnYey0uDQR/+bdLF7fUTqk9rcW1mO2KMJuY97waDtl\njSbya4ycMyMSi02yraD/32ew7ChqQEpYmhw24LUnMkiislmZ7+zjGSAuRNOgTgqlDe1Oq1446jsA\nsFht7C5tZFFSKFfOj8Nqk6zdUzESXXXi7V1lGDst/Py0JADS45WdaPceGZofSkrJtqJ6liSHodMJ\n4kJ9CfTRc2AYIvm2F9aTHh9MhMGHKREGl5OMXdi78kGB87Ybrkx8MDqqSTS2d9Fs6nZo4sCwJeu2\ndHSzvbCeaxfFE+LnybeH+p8AX1DNgJ9lHV/qwWDJKFY0kb+vmMnUSANPfHnomDWee97dR73RzGs3\nLSQ1JohTJ4ehE/QxredVt7J2bzl/+zRnSGN+Y24NBm89Z0+PZFNezbD4zDapgu53503F4KMfNjPf\nlsJ6Arz1zIkLHvBaRyWJruH3kdsXWD0FVGyIH82mbozDpC0OhnEpoErq2p0mFcApF+pgZSvtXVYW\nJU4gKTyAhYmhvPPdkRHNk+qy2Fi5tYRTkiY4Bm98qB8T/L3IHGIkX1FdGzWtZk6dPAEAIQSpMUEc\nPE4B1dLRzYGKFk6ZrKz+0uIUE2Tv+1bnSNLt5YNSNaqeZj77tRMCnMNsFQE1shqU3Zc5uYcGZTcX\nH2847qa8Giw2yYWzozk1OYwtBXVux19BjZGvcmsJC/Aio6SBhuNIPRgsGSWNhAV4kxwRwD3nTqG4\nvp0P9g59Ebe1oJ5thQ3cc+5U0uOVUlFBfp6kx4fwbS8B9cHeCiXs2eDN3W/vG9REabNJvsqt5fSp\n4ZyfGkVNq5mDlYOPWD3SYOIPa/f3MbFuzKslMcyflEgDp00J5+t897/PUNhaUM/ipAkDmvcAPHQC\nL70OU/fwRxE6cqBCj5r47LlQJzNKddwJqI4uK5UtnS40qKO5UHbb+qLEUACuWhBHaYOJjJLGk95f\nOx/sLae6tZNbz5jsOCaEID0+eMga1NaCo/4nO6kxQeRWG50qIzebuvjiQNWgH7xdJY3YJCxRBV9a\nfDDNpm4ONzhP1r2rSNiZqEYM9RRQ9W1mgnw98dY71yKLMHhTZxzZahKOCD4XY+l4H+J12dVEBnqT\nFhvMaSnh1BrNHKpx7SN88dtifDx1PHllOjap+K5OJFJKdhY3sCgxFCEE586IZE5sEE9tLBhSoI2U\nkkfX5xET7Mu1i+Kdzp2WEs7+ihYa1SAbi9XGh5kVnDktgqevTqe8ycRfPj444Gfsr2ihvs3MOTMi\nOWNqOEIc1X4GwmK1cefbmbz9XRmPrct3HG8zW9hZ1MBZ0yIAOHNqBHXGoQk+VxxpMHGk0cTS5AmD\nfk9UoM+QF6iDoaKpAw91MWBnJHKhxp2AKm1wDgu20zMXaldJI4lh/g4fyQWp0Rh89LwzQsESFquN\n5zcXMSsmiNNSnG3T6fEhFNe102xyHS3niq2FDUya4OcwR4EioLosNgp6TIL3fZTNbav38qcPswfl\nBN9eVI+3XucwPc6JVf7vbeazBzfYo/Hs2F9X99KgepsClWtHvppEcZ2yn1hPMwg4m4uPBVOXhc2H\n6jh/ZhQ6nWCp+pu7CpOuaung430VXDk/jlOTJxAf6sf/sqsH/VldFhuvbisZUgh2WWMHVS2dLE5S\nFnBCCO49byoVzR28tLl40O2sy65mf3kLd52d4jBX2Tl9ajhSHv3OWwvrqTOauWJuDPMTQrljeQof\nZFbw8b7+tbavcmrw0AnOmBJBWIA3aXHBg/ZDPft1EfvKmkmLC+bdPWWOZPatBfV0WW0sn64IqNOn\nKDsiDCXaV0rZZ3G1VfUzLk0Z/NZB158yiV0ljew5fOzpJq4obzIRHaRsJmrHPl+czFyoMSegbDbJ\nHz/Yzz3v7nM5qZbUu1v1KpPM4cZ2dpU0OrQnUKJlLk2L4X8HqtyGTZ9IPttfxeEGE79anuxIbLVj\nFwb95Rz1xGK1kVHc4KQ9AaSqyYD2QInsihY+31/F1EgDa3Yd4TfvZQ3oY9he2MCChFCHtjMlMgBf\nT4++AspoRq8ThPbKjvfWexAW4OVUj6+uV5KuncjA/nOhuq02Mo808cLmIm5ctYsVz2xlv5uIwmPd\nT6ekvo1JE/ydHmLguEtnbc6vo7PbxnmpUYCiWSZHBPCtC0f8qm2l2CTcsiwJIQQXpEaxvbCelkEI\n7l0ljVz09Bb++mkOt76xZ9Ar4512C0PS0ZX+0uQwLpodzb++PDSoIAaLVckzTI4I4PL0vrk+s2KC\nCPHzdEz6a/dWEOznyZmq1nLH8mTmTQrhzx9m99vvr3JrWJAQQpCfJwBnTYsgq6zZYTp2x76yZp7e\nVMClaRN57aaFhPh58cBnOUgpFZ+Wj54FCcocEW7wZlZMEF8PUvBVNHdw/pNbuOvtfU7HtxbWER3k\nw+Rei+f+uHphPMF+njz/zfCmIpQ3dfRZeIX4eeLv5aFpUMfDI+vyWLOrjA/2VvC8i/yRwlpFQ0iY\n0NcsA0roaGunhUVJoU7nr1s8CZuU3P7WXrosQ5vQsitahqTh9MRmkzz7dSFTIgM4Z3pkn/OzY4PR\nCSVpcCCa2ru49c29GM0Wzpwa4XQuYYI/Ad56xyrxXxvyCfL15N1bT+Hec6fwYWYFd6zJdPvd64xm\n8muMLOlhntB76JgVG+RSQIUFeKPTid7NkB4fwsf7KilQIxPr28yE9zIFwlHzYO9AiYY2M098eYhF\nD23ksue28/AXeRxpVCLOrnppp9Mqt7Pbyj8+y2HG/etYs2tw0W+1rZ1sPlTHC5uL2HO4qc9CB5zN\nxcfCuoPVhPh5sjDh6BhclhJGRnGDU0hxS0c3b2Uc4aJZ0Y7V7QWzorHYJF/lujfzNZu6+P37+/nx\nizswdVl59IrZihb0XtagNOWM4kZC/b1IiTjqexNC8MSP0zhnRiT3f3yQN3f2n7z6wd4Kiurauffc\nKX0EPCj+laUp4Xx7qJ6Wjm42HKzmktkTHYsfvYeOJ69MAwF3vp3pcpFR1mgir9rI2T2em+XTlL/7\nC2owdVn49Tv7iDR487cVqQT5enLPOVPIKGnki+xqvs6v5fQp4U5+ojOnhrP3SNOAz3lRXRs/en47\nBbVGPsmqZMNBRdu12iTbi5SFY+9FaH/4e+u5YUkCX+XWkF899GhedygCys/pmBCCuFC/k1rKa0wJ\nqNUZh3nx22KuWxzPxbOj+deGfHYWH8172Hyojue+KWRObBD+vcrX253b61TzyKJEZzvw1CgDD18+\nm+1FDfzxgwODXh1/vK+CS57ZyuXPbx9w1eaKDTk1FNS2cfuZyS4n9ABvPVMiDWQO4IfaVdLIhU9v\n4dtDdfzlkhmcPd1ZQOl0ghkTA8muaOG70ka+zq/j1tMnE+Trya+Wp/B/F8/gi+xqfv3uPpff3X6f\nl0zuZYKMCyanstXJN1FrNPcx79l58NJU/L09+OXqvZi6LANqUH/68AC3vbmHf23I574PD7Dk4U08\ntbGAufHBPHNNOrvuO4uNvzmDT351KgkT/Ln51e9Yu6ecPYcbufCpLby8tYSJwb7c9+EBx2ThCrPF\nym/fy2LhQxv56Su7ePiLPDw9dFwyZ2Kfa92VzhoMZouVTbm1nDMj0mniPi0lHLPFxu7So7/zK1tL\naDMfjeoEmBMbxMQgH77I7hvNJ6Xk430VnPWvzby/t5xfnJ7El/ecxo8XxPGXS2aQUdLIK9tKBuxj\nT/9TT7z0Op69Zi5nTYvgzx9l81aGa6Fv7Ozmya8OMSc2iPNmRrn9nNOnhFPfZubfG/IxW2xcMS/W\n6XxcqB8PXTaLzCPNPPVVQZ/32xNpz+ohoKZHG4gO8mGTm+TipvYufr/2AKUN7Tz+4zkE+Sqa11UL\n4pgaaeD3a/dT39blJPQAzpgWgU3iUsu1k13Rwo9f2IHZYuPDX57K1EgDf/3kIO1mCwcrW2g2dbMs\nZeDw8t789JQE/Lw8HJGcx0uXxUaNsbOPBgXHb74eKn1ruH9PMXZauP/jg5wxNZy/XjKTTouNnMpW\n7lyTyed3LuO70kbuejuTlAgDK29Y4LKN2BA/cqtaiQ3xdTjse3LFvFjKmkw8+VUBcaG+3H32FIyd\n3XxxoJpvC+pYPi2CS9NiHILkiwNV3PNuFrNjgjhU08ZPVmaw5meLHflXnd1W3t51hHCDDxfN7lut\nWEpFe0qY4OeymrGduZNC+DSrEptN9hFiNpvkuW8K+feXh4gP9eODXy4hNSbIZTupE4N4a9dhHl2X\nR7jBm58umeQ4d/PSRLosNh5Zl8eM6EBuPzPZqZ8bcpRQ3tRedcPmxAXTZbWRW2UkTY0+rG11PfhB\n8S09dVU6163M4Lfv76e9y+rSBxUf6sdvz5tK5pFm8qqNrD9YjV6n47L0GH52WiLJEYY+7b7zi8Xc\n+uYefvNeFkLAxCBfVt+yiPT4YK7+bwZ3rMlk9S2LmJ/grD03tJn5xRt72H24iVuWJnLW9EimRxtc\nFvAE51yoWbGu77UrbDappBKYLZyf6jxxL0oKxctDx5aCOhYnhfLY+nxe/LaY82ZGOv2eQgjOT43m\nzYzDtJktjgoX5U0m/vxRNt/k1zEnNog3bl7kVOPth/NiWX+whkfX53P6lHBSIp3vn53yJhMVzR38\nbFmiy/Neeh3PXTeXW9/Yw58+PEB2ZQt/unC6ox951a388s29VLd28u8r0/rVFuz+1td3HiYp3J85\nLu7lJXMmsqWgjme/KWRJ8gSnBdLGvFomh/s7ablCCJZPi+CjzArMFqtDI2toM/PfLSW8saOU9i4r\nd5+d4tSW3kPHny+ezk9W7kInjvqd7MyJDSbEz5PPsirx8tCRU9VKXlUrpi4rQiifm3m4CYOPnjdv\nWURSeAAPXpbKD1/YwdMbCwhUBWHvBd5gCPH34uqF8by6vZR7zpni0Ka7rTYqmxV/YU1rJ60d3Zw7\nM8pRy9IdVS0dSImjzFFPYkP81Fwt6fjtXM07w8WYEVBHGk2cHhHAM9fMRe+hI8BDx7PXzuXSZ7dx\n1Us7KKlvZ258CCtvWOBYFfUmNsSX3KrWPtpTT+46K4UjjYqQ2nO4iV0ljZgtNgw+ej7bX8WbOw/z\n1x/MpLbVzB1rMkmLC+b1mxayr6yZG1/9jp+8ksGbNy/im/w6Hluf7yhfv70onvsvmeEUrbYxt5YD\nFS08csUsl2YQO+lxwbyVcYSiujaniaWxvYu739nHt4fq+MGciTx0+SzHROGKWbGBdG6z8V1pE39f\nMRM/L+drbz09idyqVh7fkM/0aAPLp0XSbbXx5w+z+TSrkptOTezTT7tQylKdzZ3dVmpaOx0hxa44\nNTmMO5en8NRGZVXcOwcKlAe+p5A0W6xYrLKPZtwTg48nq25YyD8+z0GnOvbt92PVDQv44fPbuenV\n73j1poXEhfjh6SEob+rg1jf3UGc088w16Vw8u6/G1JujhTWdTSFVLR1kFDeys7gBY6eFeZNCWJgY\nytQoA+sPVvOfjYXk1xiZER3Yx0fo56VnfkIIX+bWcLCyla2F9Vy3OJ77L57Z5/MvmBXFK9tK2JRX\ny+KkUF74ppjVGYfx0Anuv3gGP12SgEevCUUIwT8vn8V5T37LPe9m8d/r5zu2QOlJhlphpaf/qTfe\neg9e+Mk8Hl+fz8tblQCMx344h7JGE//3cTYGH0/e+tliFvfTBiiLiunRgeRWtXLF3Fi3wuyvP5jJ\n7sNN/Pqdfbx/6xLKGk1klbews7iBm5b2FaRnTY9gdcYR/rj2ABabpKK5g5zKVjotVi6ePZFfnZnM\n1Ki+AnpZSjiXzJlIl8XqlOQPikny9CnhfLSvkg05NegEJIUHEOijRwI2CfMTQvjHZbMcE//8hFCu\nWhDHyq0lxIX6Ap2XXgAADP1JREFUMS3K4HIxNhhuWZbI6ztKee6bQpZPi+SLA1V8mVPTZ4fqBz7P\n5eoFcdx2RrLL3xd6brPh1+ecfeeH+z7Kprypg5L6NpLDA1h148Jj6vdAjBkB5a3XserGBU4T8PTo\nQB5Ykcrv1u5nWUoYL/5kXp9Jtyf2lW9v/1NPhBA8fPls6oxmRWWfH8dlc2NIiw1m7d5yHlmXx4pn\nt6FXTWarblyAv7eeU5PDePG6efz8jd0seXgTpi4rqTGBPHzFLLYVNvDC5iIOVLTwzNVzKag18vqO\nw2w+VEdcqC+Xpce67Q/gmOw35NQQ4KMn1N+L7IoWbl+dSaOpi4cum8XVC+MGtG3bK0rEhvhy1YL4\nPueFEDxyxWyK6tq4a80+3rhlEY+tz2NbYQN3LE/mnnOm9HlPdJAPEQZvthXW095lYdW2UppM3aTF\n9a9Z3HlWCrtKGtlR3EC4CwHVG2+9B/3IJgdeeh1/X5Ha53iovxev3bSQy5/fzuXPbXc6F2Hw5t1f\nnDKo5ElQzMWBPnryq9vYcFDRrrcU1DvC7QN99Bh8PB3VFzw9BN1WyeRwf568Mo2LZ0e7XJAsSwnn\nkXV5lDd18OgPZ/Pj+XEuP39efAjhBm8eXZdHfZuZbqvksvQY7j47xeWkYyfc4M1Dl83ittV7WPzP\njcyND+b81CjOmxnFJNVnu7O4gWA/T6a60bDseOs9uO+iGZw7M4p738viqpd2AnBK0gSeujqtT4qB\nO5ZPC+dQjZHLXARS2PHz0vP0Velc/tx2lj36teN4SkQAV7sYx0smhxEW4MUnWZVEB/sQE+zLFfNi\nuGFJIsk9/Gqu+M/V6W7P/e78aSxJDmNqpIGpUYY+kYmu+P3509iQU0NJfbtbrXQwRAf5cnl6LGt2\nlbFmVxmBPnrOS41iYWIo0UE+jhzDl7eUsDrjCGt2lXH2jAjiQv2YGORLZKA3Vhu0mbsdZd5cWTlm\nqdr6x5kVJIb7kxYXwrz4wT0Xx4L4Pm3S1x/z58+Xu3fv7nPcnng7JdKAl75/l9sbO0r5v48PsuV3\nZzqFYLvC7kzurdq2dnbzzKZCCmqMPHFlWh8z0IaD1Ty/uYgbliRwyeyJjvevP1jNve9m0dZlQUpl\nUrxmUTzXLpo04KrKZpMsfOgrR906O5Mm+PHsNXPdmvR6Y7VJbnntO65bPMnJbt+biuYOfvCfrTS0\nd6HXKSvvH7mZLAF+/vpux/Ygy1LCuO30yZwyecKAArPOaObFzUXcfc6UfjW/4aSiuYPN+XVYbDYs\nVuU3vmh29IBmkd5c+NQWcqqUvBg/Lw+WTJ7AKZPDWJQYyvToQDx0gspmJaVhX1kz8yaFcOGs6D6a\nTU+qWjr42yc53HbG5AGF5QOf5bBqWwmXpsdw5/IUElwEc7ijqK6NLw5U8UV2tSO3JzHMn9OnhLMu\nu5rZsUG8dP38Qbdn6rLw1MYCDN56bjsjud/v2Jt2s4WS+vZBjeEvc2rIrWplTlwws2OC+mg5Pem2\n2tAJMaS+nCje31POve9l8dYti1gyiBJH7qhp7eSVbUoy/5LJYW7nu7JGE899U8TWwjqqWzrptvaV\nATHBvmz+7RkuF0rtZgt+Xh6DCuYQQuyRUg5+sPR+/1gXUEOhs9vKwcpW5k1yb346kZTWt/Pq9lIW\nJIRy7szIQWWT2znc0E5ulZEmUxeN7V1IKbl+SQKBPq7NmcfLrpJGHvw8h9+rq8b+2F5Uz2f7q7hm\nYfygheX3nfUHq9lf3szS5HDmTQoZcHE03JgtVoydFpfm0aFwpMHEprwavjlUx46iBswWG39fMZPr\nT0kYno5qAMqzP5RFxHBhs0ka2ruoae3E00NHgI9SiDnASz8sfiVNQKkIIYxA/oAXjl/CgOGpaDk2\n0e6Pe7R70z/a/XHPVCll//bgfhgzPigg/3gk9VhHCLFbuz/u0e6Pe7R70z/a/XGPEOK4zFpjKg9K\nQ0NDQ2PsoAkoDQ0NDY1RyVgSUC+NdAdGOdr96R/t/rhHuzf9o90f9xzXvRkzQRIaGhoaGmOLsaRB\naWhoaGiMITQBpaGhoaExKhkTAkoIcb4QIl8IUSiE+MNI92ckEULECSG+FkLkCiEOCiHuUo+HCiG+\nFEIUqP+PTDbyKEEI4SGEyBRCfKa+ThRCZKj35x0hhPsyBGMcIUSwEOJ9IUSeOo5O0caPghDi1+pz\nlS2EWCOE8BnPY0cI8YoQolYIkd3jmMuxIhSeVufp/UKIuQO1/70XUEIID+BZ4AJgBnC1EGLGyPZq\nRLEAv5FSTgcWA7er9+MPwEYpZQqwUX09nrkLyO3x+hHgCfX+NAE3j0ivRgdPAeuklNOAOSj3adyP\nHyFEDHAnMF9KmQp4AFcxvsfOq8D5vY65GysXACnqv58Dzw/U+PdeQAELgUIpZbGUsgt4G1gxwn0a\nMaSUVVLKverfRpTJJQblnrymXvYacOnI9HDkEULEAhcBL6uvBbAceF+9ZNzeHyFEIHAasBJAStkl\npWxGGz929ICvEEIP+AFVjOOxI6X8FmjsddjdWFkBvC4VdgLBQgj3+wgxNgRUDFDW43W5emzcI4RI\nANKBDCBSSlkFihADIty/c8zzJPA7wL4N6wSgWUpp35tgPI+hJKAOWKWaQF8WQvijjR+klBXA48AR\nFMHUAuxBGzu9cTdWhjxXjwUB5aqi4biPnRdCBABrgbullK0j3Z/RghDiYqBWSrmn52EXl47XMaQH\n5gLPSynTgXbGoTnPFaovZQWQCEwE/FHMVr0Zr2NnIIb8nI0FAVUO9NzrIRaoHKG+jAqEEJ4owmm1\nlPID9XCNXZ1W/3e95/XY51TgB0KIUhRz8HIUjSpYNdvA+B5D5UC5lDJDff0+isDSxg+cDZRIKeuk\nlN3AB8AStLHTG3djZchz9VgQUN8BKWokjReK0/KTEe7TiKH6U1YCuVLKf/c49QnwU/XvnwIfn+y+\njQaklH+UUsZKKRNQxsomKeW1wNfAD9XLxvP9qQbKhBBT1UNnATlo4wcU095iIYSf+pzZ7402dpxx\nN1Y+Aa5Xo/kWAy12U6A7xkQlCSHEhSirYA/gFSnlgyPcpRFDCLEU2AIc4KiP5U8ofqh3gXiUB+1H\nUsrezs1xhRDiDOBeKeXFQogkFI0qFMgErpNSmkeyfyOFECINJYDECygGbkRZzI778SOE+BtwJUq0\nbCZwC4ofZVyOHSHEGuAMlC1HaoC/AB/hYqyoQv0ZlKg/E3CjlLLfaudjQkBpaGhoaIw9xoKJT0ND\nQ0NjDKIJKA0NDQ2NUYkmoDQ0NDQ0RiWagNLQ0NDQGJVoAkpDQ0NDY1SiCSgNDRUhhFUIsU+tVP2p\nECJ4mNpN6FntebgQQkwVQnyj9jlXCPGSejxNTb0Y6P2Duk5DY6TQBJSGxlE6pJRpaqXqRuD2ke7Q\nADyNUkU7Ta1e/x/1eBowGMEz2Os0NEYETUBpaLhmB2ohSyFEgBBioxBirxDigBBihXo8QdVc/qvu\nEbRBCOGrnpsnhMgSQuygh6BT9w9apbaTKYQ4Uz1+gxDiI1VzKxFC/EoIcY96zU4hRKiLPkajlI8B\nQEp5QK2m8nfgSlWzulIIsVAIsV1ta7uqebm6zl/d3+c79dpxuyuAxuhAE1AaGr1Q9xg7i6MlszqB\ny6SUc4EzgX+pWfGg7G3zrJRyJtAMXKEeXwXcKaU8pVfztwNIKWcBVwOvCSF81HOpwDUoW8g8CJjU\ngq07gOtddPUJYJMQ4guhbKQXrG45cz/wjqpZvQPkAaepbd0PPOTmuvtQSj8tUL/nY2olcw2NEUET\nUBoaR/EVQuwDGlDK1nypHhfAQ0KI/cBXKJpVpHquREq5T/17D5AghAgCgqWUm9Xjb/T4jKX211LK\nPOAwMEU997WU0iilrEPZyuFT9fgBIKF3Z6WUq4DpwHso5WZ2CiG8XXyvIOA91Q/2BDDTzfc/F/iD\neg++AXxQytVoaIwImoDS0DhKh5QyDZiEUofObpq7FggH5qnna1Amb4CeNdesKNtVCNxvI+BqywE7\nPduy9XhtU9vtg5SyUkr5ipRyBUp9uFQXlz2AIvxSgUt69N1V365QNao0KWW8lDLXzbUaGiccTUBp\naPRCStmCsrX3verWJUEoe0h1qz6jSQO8vxloUQv3giLg7Hxrfy2EmIKioeQfSz+FEOer/UMIEYWy\n8WIFYAQMPS4NUo8D3NDjeO/r1gN32M2XQoj0Y+mXhsZwoQkoDQ0XSCkzgSyULTlWA/OFELtRhEve\nIJq4EXhWDZLo6HH8OcBDCHEAeAe44TgqX58LZAshslCEy2/V7TK+BmbYgx+AR4F/CiG2oVT8t9P7\nugcAT2C/ag584Bj7paExLGjVzDU0NDQ0RiWaBqWhoaGhMSrRBJSGhoaGxqhEE1AaGhoaGqMSTUBp\naGhoaIxKNAGloaGhoTEq0QSUhoaGhsaoRBNQGhoaGhqjkv8H/6l1sxDcJf4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ddf6ca4dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('The best RF is {}'.format(best_rf))\n",
    "print(('-')*100)\n",
    "\n",
    "random_state_list = np.arange(1,101)\n",
    "F1_list = []\n",
    "XIRR_list = []\n",
    "for random_state in random_state_list:\n",
    "    rf_modified_random_state = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
    "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                            min_samples_leaf=7, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
    "                            oob_score=False, random_state=random_state, verbose=0, warm_start=False)\n",
    "    rf_modified_random_state.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
    "    rfm_Y_cv_pred = rf_modified_random_state.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
    "    rfm_F1 = f1_score(Y_cv, rfm_Y_cv_pred)\n",
    "    F1_list = F1_list + [rfm_F1]\n",
    "    rfm_cv_cashflows = get_cashflows(rf_modified_random_state, X_cv)\n",
    "    try:\n",
    "        rfm_XIRR = xirr(rfm_cv_cashflows)\n",
    "    except:\n",
    "        rfm_XIRR = np.nan\n",
    "    XIRR_list = XIRR_list + [rfm_XIRR]\n",
    "#     print('Random State: {}, F1: {}, XIRR: {}'.format(random_state, rfm_F1, rfm_XIRR))\n",
    "\n",
    "# Remove NaNs from XIRR_list\n",
    "from itertools import compress\n",
    "XIRR_list = list(compress(XIRR_list, np.logical_not(np.isnan(XIRR_list))))\n",
    "\n",
    "f, axes = plt.subplots(2,1)\n",
    "print('100 Random States were chosen and the best RF model run using them.')\n",
    "print('\\nF1 scores:')\n",
    "print('Their mean is {:,.4f}, and std. deviation is {:,.4f}'.format(np.mean(F1_list), np.std(F1_list)))\n",
    "print('The maximum is {:,.4f} for random_state = {}'.\\\n",
    "      format(np.max(F1_list), int(np.where(F1_list == np.max(F1_list))[0]+1)))\n",
    "print('The minimum is {:,.4f} for random_state = {}'.format(np.min(F1_list), int(np.where(F1_list == np.min(F1_list))[0]+1)))\n",
    "print('\\nXIRR:')\n",
    "print('Their mean is {:,.2f}%, and std. deviation is {:,.2f}%'.format(100*np.mean(XIRR_list), 100*np.std(XIRR_list)))\n",
    "print('The maximum is {:,.2f}% for random_state = {}'.\\\n",
    "      format(100*np.max(XIRR_list), int(np.where(XIRR_list == np.max(XIRR_list))[0]+1)))\n",
    "print('The minimum is {:,.2f}% for random_state = {}'.\\\n",
    "      format(100*np.min(XIRR_list), int(np.where(XIRR_list == np.min(XIRR_list))[0]+1)))\n",
    "\n",
    "axes[0].axis([0, 100, 0.5, 0.7])\n",
    "axes[0].plot(F1_list)\n",
    "axes[0].set_ylabel('F1 Scores')\n",
    "\n",
    "axes[1].axis([0, 100, 0.18, 7.5])\n",
    "axes[1].plot(XIRR_list)\n",
    "axes[1].set_ylabel('XIRR')\n",
    "\n",
    "axes[1].set_xlabel('Random State')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution plots of the F1 scores and XIRR are showcased below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~amrit_prasad/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='amrit_prasad', api_key='fHe56VYYHWTjQxmy0tRN')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF\n",
    "import scipy\n",
    "\n",
    "x = F1_list\n",
    "trace = go.Histogram(x=x,\n",
    "                     \n",
    "                     xbins=dict(start=np.min(x),\n",
    "                                size=0.005,\n",
    "                                end=np.max(x)),\n",
    "                     marker=dict(color='rgb(25, 25, 100)'))\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"F1 Scores Frequency Count\"\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
    "py.iplot(fig, filename='F1 Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~amrit_prasad/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = XIRR_list\n",
    "trace = go.Histogram(x=x,\n",
    "                     xbins=dict(start=np.min(x),\n",
    "                                size=0.05,\n",
    "                                end=np.max(x)),\n",
    "                     marker=dict(color='rgb(25, 25, 100)'))\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"XIRR Frequency Count\"\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
    "py.iplot(fig, filename='XIRR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above clearly shows that the Random Forest F1 score and XIRR is quite susceptible to the random_state, which indicates a lack of robustness of the model. A curious thing to note is that the lowest XIRR beats the benchmark, while the entire F1 score range is lesser than the benchmark's. The voting classifier is a marginal improvement on the F1 score over the Random Forest, but still can't beat the benchmark.\n",
    "\n",
    "Due to the XIRR result, I would be curious to see how these two models perform on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "The classifier took 0.2094 sec to train\n",
      "The classifier took 0.0057 sec to predict\n",
      "The accuracy score is 0.5323\n",
      "The F1 score is 0.5648\n",
      "The XIRR is 90.99%\n",
      "\n",
      "Voting Classifier:\n",
      "The classifier took 0.2410 sec to train\n",
      "The classifier took 0.0115 sec to predict\n",
      "The accuracy score is 0.4975\n",
      "The F1 score is 0.4712\n",
      "The XIRR is -8.93%\n"
     ]
    }
   ],
   "source": [
    "X_train_new = pd.concat([X_cv, X_train])\n",
    "Y_train_new = pd.concat([Y_cv, Y_train])\n",
    "\n",
    "print('Random Forest:')\n",
    "train_classifier(best_rf, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
    "score_classifier(best_rf, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
    "best_rf_test_cashflows = get_cashflows(best_rf, X_test)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_rf_test_cashflows)))\n",
    "\n",
    "print('\\nVoting Classifier:')\n",
    "train_classifier(best_vot, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
    "score_classifier(best_vot, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
    "best_vot_test_cashflows = get_cashflows(best_vot, X_test)\n",
    "print('The XIRR is {:,.2f}%'.format(100*xirr(best_vot_test_cashflows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest again falls short on the F1 score! It's XIRR remains spectacular though. I would be very cautious while using this model to trade. The voting classifier it seems just isn't cut out for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
